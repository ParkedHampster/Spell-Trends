{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continued Development\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook is intended to provide a continuation on\n",
    "the project thus far. Rather than deleting or\n",
    "overwriting sections in the first two notebooks, this\n",
    "notebook is to act as a way to cover \"Next Steps\" as\n",
    "outlined at the end of the second notebook.\n",
    "\n",
    "Because this notebook exists outside of the scope of\n",
    "the original project, this notebook may be messier and\n",
    "won't provide as much extensive detail.\n",
    "\n",
    "Once this discovery reaches a satisfactory point, the\n",
    "project will undergo the same restructuring that was\n",
    "taken during its creation, including a recreation of\n",
    "the README and presentation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Because work has been done to clean data that exists as\n",
    "a jumping-off point, this notebook won't recreate the\n",
    "initial data - though it is important to note that\n",
    "newer and more accurate data will become available over\n",
    "time and this may not immediately be taken into account\n",
    "during the processes outlined within."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, \\\n",
    "            GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, \\\n",
    "            cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from joblib import parallel_backend, Parallel, \\\n",
    "            dump, load\n",
    "\n",
    "from _code.cleaner import preprocess\n",
    "\n",
    "from IPython.display import Image, Markdown\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreating Processes\n",
    "\n",
    "Many of the processes used in 2_Modeling.ipynb are\n",
    "still valid that lead up to the actual model process,\n",
    "so these steps will be combined here without explicit\n",
    "explanation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import initial card data set\n",
    "cards = pd.read_parquet('./data/simplified_cards.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform pre-processing of abilities\n",
    "processed_abilities = preprocess(cards['oracle_text'])\n",
    "cards['abilities_list'] = [\n",
    "    abilities.split('\\n') \n",
    "    for abilities in processed_abilities\n",
    "    ]\n",
    "\n",
    "# create an ability count feature\n",
    "cards['n_abilities'] = cards['abilities_list'].map(len)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test sets that are divided based on\n",
    "# the set that a card is a part of \n",
    "X = cards.drop(columns=['prices_normal','prices_foil'])\n",
    "y = cards['set']\n",
    "train, test, _, _ = \\\n",
    "    train_test_split(\n",
    "        X,y,stratify=y,\n",
    "        random_state=13,\n",
    "        test_size=0.2\n",
    "    )\n",
    "\n",
    "# we'll reset the indices of both sets to more easily\n",
    "# translate between polars and pandas in an upcoming\n",
    "# step\n",
    "\n",
    "train.reset_index(drop=True,inplace=True)\n",
    "test.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization Process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize Abilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our token pattern needs to be able to account for\n",
    "# several non-standard things for it to be effective\n",
    "# for our needs. As normal, it needs to be able to\n",
    "# match words that are contiguous letters of an\n",
    "# arbitrary length. However, we also need to be able to\n",
    "# account for numbers in a few formats. These can be\n",
    "# wrapped in curly brackets, e.g. {2}, representing 2\n",
    "# colorless mana.\n",
    "# It also needs to be able to account for letters\n",
    "# inside of curly brackets and return them as such.\n",
    "# It must also match something like +1/+1, -1/-1, or\n",
    "# several other variations thereof.\n",
    "# Lastly, it should also ignore any text that is inside\n",
    "# of parentheses, as this text is reminder text - which\n",
    "# is text that explains what an ability does but this\n",
    "# text doesn't actually contribute to explaining what\n",
    "# a card does in a meaningful way. \n",
    "\n",
    "token_pattern = \\\n",
    "    r\"([a-zA-Z]+(?:’[a-z]+)?|[+-]?\\d\\/[+-]?\\d|\\{\\d\\d?\\}|\\{.\\s?.?\\}|\\n)|\\(.+?\\)\"\n",
    "\n",
    "cvec = CountVectorizer(\n",
    "        token_pattern=token_pattern,\n",
    "        # min_df=0.0005, # <= this will mean that the\n",
    "                    # minimum number of cards that it\n",
    "                    # takes for an ability to show up\n",
    "                    # on this list will be 46 after the\n",
    "                    # explode is run, since it will be\n",
    "                    # 83,000 entries long. We'll just\n",
    "                    # limit our overall features since\n",
    "                    # this is such a small percentage. \n",
    "        max_df=0.4,\n",
    "        ngram_range=(1,5),\n",
    "        max_features=1500\n",
    "    )\n",
    "\n",
    "# Exploding abilities to create a vectorized set\n",
    "explode_train = train.explode('abilities_list')\n",
    "\n",
    "explode_vec = cvec.fit_transform(\n",
    "        explode_train['abilities_list']\n",
    "    )\n",
    "explode_vec = pd.DataFrame.sparse.from_spmatrix(\n",
    "    explode_vec\n",
    ")\n",
    "\n",
    "# we save the vocab here to export for later. This is\n",
    "# so we can bring in new data and make sure it's only\n",
    "# being segmented into vocab we can \"understand\"\n",
    "explode_vec.columns = sorted((vocab := cvec.vocabulary_))\n",
    "explode_vec['id'] = explode_train['id'].values\n",
    "explode_vec.head()\n",
    "\n",
    "# convert pandas vectorized dataframe to polars\n",
    "pl_vec = pl.from_pandas(explode_vec.astype(np.int32,errors='ignore'))\n",
    "# perform group by and sum aggregation and convert back\n",
    "# to pandas \n",
    "agged_vec = pl_vec.groupby('id').sum().to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_type_cvec = CountVectorizer()\n",
    "\n",
    "type_frame = pl.from_pandas(train['type_line']).apply(lambda x: x.split('—')[0])\n",
    "type_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "    card_type_cvec.fit_transform(type_frame)\n",
    ")\n",
    "# we save the type vocab for later to export\n",
    "type_df.columns = sorted((type_vocab:=card_type_cvec.vocabulary_))\n",
    "type_df['id'] = train['id']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing Color Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['str_color_identity'] = \\\n",
    "    train['color_identity'].map(\n",
    "        lambda x: ' '.join(x)\n",
    "        )\n",
    "\n",
    "color_match = CountVectorizer(\n",
    "    token_pattern=r\"[wubrg]\"\n",
    ")\n",
    "color_id_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "    color_match.fit_transform(train['str_color_identity'])\n",
    ")\n",
    "color_id_df.columns = sorted(\n",
    "        (color_vocab:=color_match.vocabulary_)\n",
    "    )\n",
    "color_id_df['c'] = color_id_df.T.apply(lambda x: 1 if sum(x)==0 else 0)\n",
    "color_id_df['id'] = train['id']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing Pseudo-Numbers et al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_vectorizer = CountVectorizer(\n",
    "    token_pattern = r\".*\",\n",
    "    stop_words=[''],\n",
    "    lowercase=False\n",
    "    )\n",
    "dummy_dict = {}\n",
    "dummy_vocab = {}\n",
    "dummy_columns = ['rarity','power','toughness','loyalty']\n",
    "for _col in dummy_columns:\n",
    "    dummy_column = train[_col].T.apply(\n",
    "        lambda x: '' if x == None else f'{_col}_{x}'\n",
    "        )\n",
    "    dummy_dict[_col] = pd.DataFrame.sparse.from_spmatrix(\n",
    "        dummy_vectorizer.fit_transform(dummy_column)\n",
    "    )\n",
    "    dummy_vocab[_col] = dummy_vectorizer.vocabulary_\n",
    "    dummy_dict[_col].columns = sorted(dummy_vocab[_col])\n",
    "    dummy_dict[_col]['id'] = train['id']\n",
    "\n",
    "dummies = train[['id']]\n",
    "for _col in dummy_columns:\n",
    "    dummies = dummies.merge(\n",
    "        dummy_dict[_col],\n",
    "        on='id'\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date-to-Age Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_now = pd.Timestamp.today().floor('D')\n",
    "train['card_age'] = train['released_at'].apply(lambda x: (_now - x).days)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# purpose for each column is explained in notebook 2\n",
    "# for simplicity, this won't be repeated here \n",
    "used_columns = [\n",
    "    'id','cmc','promo','reprint','full_art','textless',\n",
    "    'n_abilities','median_normal','median_foil','card_age'\n",
    "]\n",
    "train_reduced = train[used_columns].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Merging and Subsetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined = train_reduced.merge(\n",
    "    agged_vec,\n",
    "    on='id'\n",
    ").merge(\n",
    "    type_df,\n",
    "    on='id'\n",
    ").merge(\n",
    "    color_id_df,\n",
    "    on='id'\n",
    ").merge(\n",
    "    dummies,\n",
    "    on='id'\n",
    ")\n",
    "\n",
    "train_combined = train_combined[\n",
    "    (train_combined[ 'stickers' ] == 0) &\n",
    "    (train_combined['conspiracy'] == 0)\n",
    "].drop(columns=['stickers','conspiracy']).copy()\n",
    "\n",
    "# creating a normal and foil subset\n",
    "train_norm = train_combined.dropna(\n",
    "    subset=['median_normal']\n",
    "    ).drop(columns=['median_foil']\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "train_foil = train_combined.dropna(\n",
    "    subset=['median_foil']\n",
    "    ).drop(columns=['median_normal']\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "norm_prices = train_norm['median_normal']\n",
    "foil_prices = train_foil['median_foil']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling (Again)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Model\n",
    "\n",
    "We'll recreate the dummy model here to have a baseline\n",
    "for comparison later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyRegressor(strategy='median')\n",
    "norm_guess = dummy.fit(\n",
    "    train_norm,norm_prices\n",
    "    ).predict(train_norm)\n",
    "foil_guess = dummy.fit(\n",
    "    train_foil,foil_prices\n",
    "    ).predict(train_foil)\n",
    "\n",
    "norm_base_rmse = mean_squared_error(\n",
    "        norm_prices,norm_guess,\n",
    "        squared=False\n",
    "    )\n",
    "foil_base_rmse = mean_squared_error(\n",
    "        foil_prices,foil_guess,\n",
    "        squared=False\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbr = {\n",
    "#     'norm':GradientBoostingRegressor(\n",
    "#         random_state=13,\n",
    "#         verbose=3,\n",
    "#         n_iter_no_change=5\n",
    "#         ),\n",
    "#     'foil':GradientBoostingRegressor(\n",
    "#         random_state=13,\n",
    "#         verbose=3,\n",
    "#         n_iter_no_change=5\n",
    "#         )\n",
    "# }\n",
    "# params = {\n",
    "#     'learning_rate':[\n",
    "#         1.0,\n",
    "#         0.5,\n",
    "#         # 0.25,\n",
    "#         0.1,\n",
    "#         # 0.01\n",
    "#         ],\n",
    "#     'n_estimators':[\n",
    "#         50,\n",
    "#         100,\n",
    "#         # 200\n",
    "#         ],\n",
    "#     # 'subsample':[1.0],\n",
    "#     'min_samples_split':[\n",
    "#         # 5,\n",
    "#         10,\n",
    "#         20\n",
    "#         ],\n",
    "#     # 'min_samples_leaf'=1,\n",
    "#     # 'min_weight_fraction_leaf'=0.0,\n",
    "#     'max_depth':[\n",
    "#         # 25,\n",
    "#         50,\n",
    "#         100,\n",
    "#         200\n",
    "#         ],\n",
    "#     # 'max_features'=None,\n",
    "#     # 'alpha'=0.9,\n",
    "#     # 'max_leaf_nodes'=None,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         4         237.3863           28.57m\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "[CV 1/5; 1/36] START learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "[CV 2/5; 1/36] START learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "[CV 3/5; 1/36] START learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "[CV 4/5; 1/36] START learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "[CV 5/5; 1/36] START learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "[CV 1/5; 2/36] START learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "[CV 2/5; 2/36] START learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "[CV 3/5; 2/36] START learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "[CV 4/5; 2/36] START learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "[CV 5/5; 2/36] START learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2        1305.1508           33.66m\n",
      "        17          47.6080          127.35m\n",
      "         3         481.7594           34.63m\n",
      "         1         808.7596           59.78m\n",
      "         1         565.9117           30.25m\n",
      "         1         804.3936           32.03m\n",
      "         1         682.2923           65.50m\n",
      "         1         682.2923           33.51m\n",
      "         1         804.3936           70.01m\n",
      "         7          51.8957           39.88m\n",
      "        10          53.8373          144.50m\n",
      "         4         274.6815           40.58m\n",
      "         1         808.7596           43.90m\n",
      "         1        1104.7775          108.95m\n",
      "         3         528.2261           55.79m\n",
      "         1        1104.7775           55.24m\n",
      "         1         565.9117          111.60m\n",
      "         5         145.9303           46.05m\n",
      "         9          51.5883          135.74m\n",
      "        23          45.3100           42.74m\n",
      "         2         167.7901           76.19m\n",
      "         2         135.9364           39.37m\n",
      "         8          39.2128           45.49m\n",
      "        21          47.8142           41.30m\n",
      "         2         259.2938           88.13m\n",
      "         2         284.1408           43.30m\n",
      "         2         259.2938           44.58m\n",
      "         2         284.1408           91.23m\n",
      "         5         182.1353           54.20m\n",
      "         4         231.4702           65.61m\n",
      "         6         110.4451           55.88m\n",
      "[CV 3/5; 14/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100;, score=-54.510 total time= 3.6min\n",
      "         2         167.7901           56.87m\n",
      "         2         135.9364          116.37m\n",
      "         2         251.9254          121.94m\n",
      "         3         120.3091           81.59m\n",
      "        11          55.4021          138.55m\n",
      "         2         251.9254           64.58m\n",
      "         3          76.6722           43.97m\n",
      "         5         141.8588           67.03m\n",
      "         3         181.5259           96.75m\n",
      "         3         109.0558           46.91m\n",
      "         6         102.4838           62.13m\n",
      "         9          34.1240           54.90m\n",
      "[CV 2/5; 14/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100;, score=-103.656 total time= 5.4min\n",
      "         3         109.0558          106.75m\n",
      "        18          47.5580          132.85m\n",
      "         3          76.6722          111.06m\n",
      "         3         181.5259           54.12m\n",
      "         4          81.2362           83.13m\n",
      "         3          81.8942          116.47m\n",
      "         3         120.3091           59.20m\n",
      "        24          45.1968           42.09m\n",
      "         6         111.6096           70.38m\n",
      "[CV 5/5; 14/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100;, score=-58.017 total time= 4.5min\n",
      "         7          76.4132           63.40m\n",
      "[CV 4/5; 14/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100;, score=-96.008 total time= 4.8min\n",
      "         4          68.2044           45.93m\n",
      "         4          81.3248           47.05m\n",
      "        11          53.1495          156.48m\n",
      "         4          84.7143           98.82m\n",
      "         3          81.8942           65.41m\n",
      "        10          50.6161          146.69m\n",
      "        22          47.7967           41.20m\n",
      "         4          68.2044          103.63m\n",
      "         5          55.4526           82.80m\n",
      "         4          59.7171          108.14m\n",
      "         4          81.3248          109.72m\n",
      "         4          84.7143           52.80m\n",
      "        12          53.8549          141.85m\n",
      "         4          81.2362           56.67m\n",
      "         5          58.7137           44.85m\n",
      "         4          59.7171           58.00m\n",
      "         5          50.0471           99.08m\n",
      "         5          58.7137           99.91m\n",
      "         5          70.2630           47.39m\n",
      "         6          52.7735           83.13m\n",
      "         5          53.4767          102.77m\n",
      "         5          70.2630          104.60m\n",
      "         5          50.0471           49.90m\n",
      "         6          54.6733           43.48m\n",
      "         5          53.4767           53.36m\n",
      "         5          55.4526           54.63m\n",
      "         6          54.6733           96.18m\n",
      "         7          50.5394           82.46m\n",
      "         6          42.3746           98.72m\n",
      "        25          45.1097           41.32m\n",
      "        19          47.5210          137.20m\n",
      "         6          50.2065           99.33m\n",
      "         6          56.5146           46.66m\n",
      "         6          56.5146          100.51m\n",
      "         6          42.3746           47.25m\n",
      "        23          47.7874           40.79m\n",
      "         6          50.2065           49.68m\n",
      "         7          52.8050           42.07m\n",
      "        12          52.6787          162.47m\n",
      "         7          52.8050           93.48m\n",
      "        11          49.6138          154.95m\n",
      "         8          49.5758           81.31m\n",
      "        13          52.9798          145.58m\n",
      "         7          48.3734           95.82m\n",
      "         6          52.7735           53.08m\n",
      "         7          33.0913           44.86m\n",
      "         7          33.0913           97.05m\n",
      "         7          54.7564           97.35m\n",
      "         7          54.7564           45.81m\n",
      "         7          48.3734           47.02m\n",
      "         8          51.5534           41.09m\n",
      "         9          49.2452           80.43m\n",
      "         8          51.5534           91.89m\n",
      "         8          47.0019           92.69m\n",
      "         8          29.6209           43.02m\n",
      "[CV 2/5; 1/36] END learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=50;, score=-111.957 total time= 8.2min\n",
      "[CV 1/5; 3/36] START learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         8          29.6209           94.83m\n",
      "[CV 2/5; 2/36] END learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=100;, score=-111.957 total time= 8.3min\n",
      "[CV 2/5; 3/36] START learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         8          53.3118           96.36m\n",
      "         7          50.5394           51.53m\n",
      "         8          47.0019           44.65m\n",
      "         8          53.3118           44.95m\n",
      "         1        1339.5554           23.89m\n",
      "         1         981.4839           25.83m\n",
      "         9          51.1326           40.29m\n",
      "        24          47.7801           40.16m\n",
      "        10          48.7232           80.22m\n",
      "        20          47.4838          139.08m\n",
      "         9          51.1326           90.34m\n",
      "         9          46.4440           90.62m\n",
      "         9          46.4440           42.87m\n",
      "         9          52.9387           95.15m\n",
      "        12          49.1234          158.45m\n",
      "         8          49.5758           49.86m\n",
      "        14          52.3478          148.43m\n",
      "         2         593.1167           32.98m\n",
      "        26          45.0443           41.17m\n",
      "         9          52.9387           43.83m\n",
      "         2         404.5360           34.43m\n",
      "        10          50.6951           38.91m\n",
      "        10          50.6951           87.83m\n",
      "        10          46.0304           88.37m\n",
      "        11          48.5634           80.19m\n",
      "        13          52.5242          168.39m\n",
      "        10          46.0304           41.06m\n",
      "         3         347.6206           35.42m\n",
      "        10          52.7489           94.44m\n",
      "        11          50.4143           85.25m\n",
      "        11          50.4143           37.47m\n",
      "         9          49.2452           48.41m\n",
      "         3         185.7630           37.28m\n",
      "        11          45.4915           86.64m\n",
      "[CV 1/5; 2/36] END learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=100;, score=-123.227 total time=10.8min\n",
      "[CV 3/5; 3/36] START learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        10          52.7489           42.94m\n",
      "        12          48.3828           78.78m\n",
      "        11          45.4915           39.67m\n",
      "[CV 1/5; 1/36] END learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=50;, score=-123.227 total time=11.2min\n",
      "[CV 4/5; 3/36] START learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        25          47.7760           39.39m\n",
      "         1         902.8817           26.87m\n",
      "         4         207.5660           36.55m\n",
      "        12          50.2596           84.30m\n",
      "        11          52.4833           93.00m\n",
      "        12          50.2596           36.47m\n",
      "         4         142.1061           37.81m\n",
      "        13          48.2238           77.61m\n",
      "[CV 5/5; 2/36] END learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=100;, score=-61.573 total time=11.6min\n",
      "[CV 5/5; 3/36] START learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        15          51.8028          148.75m\n",
      "         1         861.1392           24.57m\n",
      "        10          48.7232           46.86m\n",
      "        11          52.4833           41.70m\n",
      "        21          47.4609          141.75m\n",
      "         1         994.8235           24.53m\n",
      "         2         505.2703           33.94m\n",
      "         5         150.9008           37.19m\n",
      "        13          50.1537           83.04m\n",
      "        13          50.1537           35.36m\n",
      "         5         125.7480           37.79m\n",
      "         2         396.3710           33.77m\n",
      "        13          48.7491          165.46m\n",
      "        12          52.3447           92.48m\n",
      "        27          44.9940           40.62m\n",
      "        14          52.2286          172.66m\n",
      "        12          52.3447           40.90m\n",
      "        11          48.5634           45.80m\n",
      "         2         375.3274           32.56m\n",
      "         3         270.9243           35.97m\n",
      "         6         102.3639           36.91m\n",
      "         6          96.3881           37.24m\n",
      "        14          50.1011           82.05m\n",
      "        14          50.1011           34.57m\n",
      "         3         238.4367           35.28m\n",
      "        26          47.7744           38.51m\n",
      "        13          52.1051           91.50m\n",
      "         3         282.6161           34.70m\n",
      "        16          51.1908          149.48m\n",
      "         4         200.9093           36.32m\n",
      "        12          48.3828           44.27m\n",
      "        13          52.1051           39.86m\n",
      "         7          77.9369           36.64m\n",
      "         7          83.1484           36.78m\n",
      "         4         186.5794           36.06m\n",
      "        15          50.0796           81.29m\n",
      "        15          50.0796           33.67m\n",
      "         4         134.3370           35.75m\n",
      "        14          48.6247          165.08m\n",
      "        14          52.0363           90.73m\n",
      "         5         108.6261           36.80m\n",
      "        22          45.4532          144.60m\n",
      "        13          48.2238           42.77m\n",
      "[CV 5/5; 1/36] END learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=50;, score=-61.573 total time=15.1min\n",
      "[CV 1/5; 4/36] START learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "         8          66.9227           35.83m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        14          52.0363           38.91m\n",
      "        28          44.9549           39.42m\n",
      "        27          47.7707           36.93m\n",
      "         5         161.5630           35.90m\n",
      "        16          50.0434           80.44m\n",
      "         8          66.6841           37.12m\n",
      "        16          50.0434           32.69m\n",
      "         1        1339.5554           49.73m\n",
      "         5          91.6970           36.24m\n",
      "        15          52.0568          175.28m\n",
      "         6          84.6173           36.68m\n",
      "        15          52.0057           89.89m\n",
      "[CV 3/5; 2/36] END learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=100;, score=-61.229 total time=15.9min\n",
      "[CV 2/5; 4/36] START learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         9          61.0178           35.27m\n",
      "         6          86.9113           36.03m\n",
      "        17          49.9978           78.86m\n",
      "        15          52.0057           37.95m\n",
      "        17          49.9978           31.57m\n",
      "[CV 3/5; 1/36] END learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=50;, score=-61.229 total time=16.3min\n",
      "[CV 3/5; 4/36] START learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        17          51.0313          151.52m\n",
      "         2         593.1167           68.79m\n",
      "         9          42.9522           37.49m\n",
      "         6          77.6938           35.91m\n",
      "         1         981.4839           63.41m\n",
      "         7          64.7986           36.15m\n",
      "         1         902.8817           54.70m\n",
      "        18          49.9242           76.99m\n",
      "        10          57.2733           35.20m\n",
      "         7          68.4884           35.70m\n",
      "        18          49.9242           30.31m\n",
      "        28          47.7678           35.57m\n",
      "        15          48.4029          166.39m\n",
      "         3         347.6206           73.13m\n",
      "         7          67.9002           35.19m\n",
      "         8          60.7898           35.59m\n",
      "         2         404.5360           84.38m\n",
      "        10          37.9068           37.60m\n",
      "         2         505.2703           70.29m\n",
      "        19          49.9007           75.86m\n",
      "        23          45.3100          146.39m\n",
      "         8          63.3234           35.37m\n",
      "        29          44.9245           38.35m\n",
      "        19          49.9007           29.34m\n",
      "        11          56.3933           35.15m\n",
      "         4         207.5660           75.88m\n",
      "         8          65.0296           34.84m\n",
      "         9          58.6621           35.88m\n",
      "         3         270.9243           75.56m\n",
      "        20          49.8792           74.54m\n",
      "        11          34.8172           36.89m\n",
      "         3         185.7630           90.70m\n",
      "        16          51.9315          178.31m\n",
      "[CV 3/5; 12/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100;, score=-63.673 total time=34.0min\n",
      "        20          49.8792           28.26m\n",
      "         9          59.4253           34.79m\n",
      "        29          47.7646           34.04m\n",
      "        18          50.7296          152.38m\n",
      "[CV 4/5; 12/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100;, score=-90.352 total time=33.5min\n",
      "         5         150.9008           76.95m\n",
      "         9          62.6979           34.10m\n",
      "        12          54.6736           34.62m\n",
      "        16          48.2156          164.13m\n",
      "         4         200.9093           74.89m\n",
      "        10          57.0097           34.77m\n",
      "        12          32.2444           35.47m\n",
      "        21          49.8661           73.31m\n",
      "        10          56.8158           33.87m\n",
      "         4         142.1061           92.18m\n",
      "        10          57.7649           32.94m\n",
      "        21          49.8661           27.42m\n",
      "         6         102.3639           77.20m\n",
      "        24          45.1968          145.22m\n",
      "        13          53.8235           33.83m\n",
      "[CV 1/5; 3/36] END learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=50;, score=-114.700 total time=11.9min\n",
      "[CV 4/5; 4/36] START learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        11          54.9195           33.45m\n",
      "         5         108.6261           73.77m\n",
      "        13          30.9583           34.09m\n",
      "        22          49.8614           71.96m\n",
      "        30          47.7631           32.34m\n",
      "        11          54.5386           32.78m\n",
      "         1         861.1392           54.54m\n",
      "        11          54.0834           32.24m\n",
      "         5         125.7480           92.24m\n",
      "         7          77.9369           76.92m\n",
      "        30          44.8905           37.23m\n",
      "        22          49.8614           26.58m\n",
      "         6          84.6173           72.83m\n",
      "        12          54.4389           32.38m\n",
      "        23          49.8509           70.27m\n",
      "        14          30.2834           32.78m\n",
      "[CV 2/5; 3/36] END learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=50;, score=-111.362 total time=12.8min\n",
      "[CV 5/5; 4/36] START learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        12          53.4562           31.88m\n",
      "         1         994.8235           41.78m\n",
      "        12          53.1089           31.49m\n",
      "         8          66.9227           75.19m\n",
      "        24          49.8299           68.47m\n",
      "         2         396.3710           74.31m\n",
      "        17          48.1631          164.88m\n",
      "         7          64.7986           71.83m\n",
      "         6          96.3881           91.29m\n",
      "        25          45.1097          142.80m\n",
      "        23          49.8509           25.57m\n",
      "        13          53.9780           31.68m\n",
      "        13          52.1789           30.78m\n",
      "        31          47.7613           30.76m\n",
      "         2         375.3274           56.14m\n",
      "        13          52.3678           30.35m\n",
      "        25          49.8194           66.92m\n",
      "         9          61.0178           74.07m\n",
      "         8          60.7898           71.18m\n",
      "         3         238.4367           79.25m\n",
      "        24          49.8299           24.50m\n",
      "        14          53.5076           30.71m\n",
      "         7          83.1484           90.37m\n",
      "        14          51.7755           29.81m\n",
      "         3         282.6161           61.13m\n",
      "        14          51.8573           29.42m\n",
      "[CV 5/5; 3/36] END learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=50;, score=-61.381 total time=11.5min\n",
      "[CV 1/5; 5/36] START learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        26          49.8139           65.73m\n",
      "        10          57.2733           73.34m\n",
      "         9          58.6621           70.49m\n",
      "        15          53.2061           29.64m\n",
      "         1        1104.4991           21.30m\n",
      "        25          49.8194           23.52m\n",
      "         4         186.5794           82.19m\n",
      "        31          44.8724           35.91m\n",
      "        15          51.3772           28.91m\n",
      "         4         134.3370           63.91m\n",
      "         8          66.6841           90.08m\n",
      "        27          49.8089           64.37m\n",
      "        11          56.3933           72.39m\n",
      "        18          47.9360          164.81m\n",
      "        26          45.0443          142.09m\n",
      "        10          57.0097           70.65m\n",
      "        16          53.0210           28.61m\n",
      "        32          47.7606           29.46m\n",
      "        16          50.9410           28.02m\n",
      "         5          91.6970           65.34m\n",
      "         5         161.5630           82.74m\n",
      "        26          49.8139           22.62m\n",
      "        28          49.8052           63.14m\n",
      "         9          42.9522           89.42m\n",
      "        12          54.6736           71.19m\n",
      "         2         246.6016           41.48m\n",
      "        11          54.9195           69.52m\n",
      "        17          52.6585           27.54m\n",
      "         6          77.6938           65.06m\n",
      "        17          50.6566           27.15m\n",
      "        27          49.8089           21.61m\n",
      "         6          86.9113           82.97m\n",
      "        29          49.8034           62.25m\n",
      "        13          53.8235           70.01m\n",
      "[CV 1/5; 4/36] END learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=100;, score=-114.700 total time=10.5min\n",
      "[CV 2/5; 5/36] START learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        18          52.4598           26.54m\n",
      "        12          54.4389           68.83m\n",
      "        10          37.9068           88.77m\n",
      "        18          50.5586           26.09m\n",
      "        19          47.8727          162.11m\n",
      "         7          67.9002           65.09m\n",
      "         1         804.1346           21.48m\n",
      "        28          49.8052           20.47m\n",
      "        33          47.7600           27.94m\n",
      "        30          49.8007           60.87m\n",
      "         3          72.4075           47.84m\n",
      "         7          68.4884           80.35m\n",
      "        19          52.2033           25.54m\n",
      "        13          53.9780           67.93m\n",
      "        32          44.8583           34.61m\n",
      "[CV 1/5; 11/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=50;, score=-113.547 total time=61.6min\n",
      "        27          44.9940          141.71m\n",
      "        19          50.4679           25.08m\n",
      "        31          49.7962           59.32m\n",
      "        11          34.8172           87.33m\n",
      "         8          65.0296           65.69m\n",
      "        29          49.8034           19.45m\n",
      "         8          63.3234           78.23m\n",
      "        14          53.5076           66.43m\n",
      "        20          52.1471           24.66m\n",
      "         2         276.8179           42.62m\n",
      "        32          49.7947           58.11m\n",
      "        12          32.2444           84.45m\n",
      "        20          50.3731           24.43m\n",
      "        30          49.8007           18.33m\n",
      "         9          62.6979           65.60m\n",
      "         4          53.3750           51.44m\n",
      "         9          59.4253           75.84m\n",
      "        15          53.2061           64.85m\n",
      "        20          47.8461          159.58m\n",
      "        21          52.0841           23.68m\n",
      "        31          49.7962           17.19m\n",
      "        33          49.7940           57.07m\n",
      "[CV 4/5; 2/36] END learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=100;, score=-96.848 total time=28.2min\n",
      "[CV 3/5; 5/36] START learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "        10          57.7649           63.92m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        13          30.9583           82.19m\n",
      "        10          56.8158           74.15m\n",
      "        34          47.7594           26.60m\n",
      "        16          53.0210           63.71m\n",
      "        21          50.3474           23.87m\n",
      "         3         179.7433           47.04m\n",
      "        22          52.0277           22.65m\n",
      "         1         682.0311           23.02m\n",
      "        28          44.9549          140.15m\n",
      "        32          49.7947           16.15m\n",
      "         5          50.4370           51.45m\n",
      "        11          54.0834           62.88m\n",
      "        14          30.2834           80.19m\n",
      "[CV 2/5; 4/36] END learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=100;, score=-111.362 total time=13.1min\n",
      "[CV 4/5; 5/36] START learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        17          52.6585           62.38m\n",
      "        11          54.5386           72.54m\n",
      "        22          50.2970           22.95m\n",
      "        33          49.7940           15.17m\n",
      "[CV 4/5; 1/36] END learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=50;, score=-96.848 total time=29.5min\n",
      "[CV 5/5; 5/36] START learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "        21          47.8142          156.38m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        23          52.0102           22.00m\n",
      "         1         565.5416           27.59m\n",
      "        12          53.1089           62.55m\n",
      "         4          81.7645           48.05m\n",
      "        18          52.4598           61.22m\n",
      "        12          53.4562           71.25m\n",
      "         1         808.4116           21.57m\n",
      "         6          47.3736           50.15m\n",
      "        23          50.2546           21.95m\n",
      "[CV 1/5; 5/36] END learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=50;, score=-124.131 total time= 6.8min\n",
      "[CV 1/5; 6/36] START learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2         245.1299           43.25m\n",
      "        35          47.7590           25.01m\n",
      "        13          52.3678           61.55m\n",
      "         1        1104.4991           39.74m\n",
      "        24          51.9719           21.22m\n",
      "        19          52.2033           60.08m\n",
      "        13          52.1789           69.67m\n",
      "        29          44.9245          138.43m\n",
      "        24          50.2267           21.16m\n",
      "        14          51.8573           60.88m\n",
      "[CV 5/5; 4/36] END learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=100;, score=-61.381 total time= 9.9min\n",
      "[CV 2/5; 6/36] START learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2         129.9009           49.29m\n",
      "         5          45.0986           49.94m\n",
      "        20          52.1471           59.26m\n",
      "        25          51.9145           20.46m\n",
      "        14          51.7755           68.36m\n",
      "         3         106.9072           49.04m\n",
      "         2         164.9951           43.40m\n",
      "         1         804.1346           38.06m\n",
      "        25          50.1146           20.20m\n",
      "[CV 4/5; 3/36] END learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=50;, score=-90.230 total time=20.2min\n",
      "[CV 3/5; 6/36] START learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2         246.6016           78.10m\n",
      "        22          47.7967          154.89m\n",
      "         1         682.0311           43.12m\n",
      "        21          52.0841           58.67m\n",
      "        15          51.3772           67.24m\n",
      "        26          51.9049           19.77m\n",
      "         6          34.7129           49.59m\n",
      "        36          47.7588           23.52m\n",
      "         3          72.8048           53.49m\n",
      "         4          79.4637           49.89m\n",
      "        22          52.0277           57.76m\n",
      "         3         109.7666           48.94m\n",
      "        16          50.9410           65.96m\n",
      "         3          72.4075           91.60m\n",
      "        30          44.8905          136.77m\n",
      "        27          51.8669           18.92m\n",
      "[CV 3/5; 3/36] END learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=50;, score=-63.999 total time=22.2min\n",
      "[CV 4/5; 6/36] START learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2         276.8179          100.00m\n",
      "        17          50.6566           64.24m\n",
      "        23          52.0102           57.11m\n",
      "        23          47.7874          152.74m\n",
      "         2         245.1299          103.87m\n",
      "         1         565.5416           56.49m\n",
      "         7          32.1019           49.37m\n",
      "         5          68.6642           49.44m\n",
      "         4          61.4412           53.73m\n",
      "        18          50.5586           62.90m\n",
      "        24          51.9719           56.08m\n",
      "         4          73.0207           52.58m\n",
      "         4          53.3750           99.13m\n",
      "        19          50.4679           61.57m\n",
      "         3         179.7433          118.24m\n",
      "        25          51.9145           55.10m\n",
      "         8          29.0627           48.24m\n",
      "[CV 2/5; 5/36] END learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=50;, score=-109.730 total time= 9.2min\n",
      "[CV 5/5; 6/36] START learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        37          47.7587           22.10m\n",
      "        31          44.8724          135.12m\n",
      "         6          55.1579           49.42m\n",
      "         5          54.5157           54.28m\n",
      "         2         129.9009          101.23m\n",
      "         1         808.4116           45.34m\n",
      "        20          50.3731           60.56m\n",
      "         3         106.9072          124.99m\n",
      "         5          50.4370          102.29m\n",
      "        26          51.9049           54.37m\n",
      "         5          50.6466           53.44m\n",
      "        24          47.7801          150.52m\n",
      "         7          53.3459           48.00m\n",
      "        27          51.8669           53.28m\n",
      "[CV 3/5; 4/36] END learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=100;, score=-63.999 total time=19.7min\n",
      "[CV 1/5; 7/36] START learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        21          50.3474           60.25m\n",
      "         4          81.7645          125.17m\n",
      "         3          72.8048          109.71m\n",
      "         1        1339.3149           19.66m\n",
      "         6          47.3736          102.12m\n",
      "[CV 1/5; 6/36] END learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=100;, score=-124.131 total time= 6.5min\n",
      "[CV 2/5; 7/36] START learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2         164.9951           86.58m\n",
      "         6          52.0390           56.59m\n",
      "         6          49.4896           53.42m\n",
      "         1         981.2696           18.99m\n",
      "         4          79.4637          131.12m\n",
      "        22          50.2970           59.53m\n",
      "        32          44.8583          133.47m\n",
      "[CV 1/5; 12/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100;, score=-113.547 total time=62.8min\n",
      "         8          52.2124           47.46m\n",
      "        38          47.7585           20.64m\n",
      "        25          47.7760          148.13m\n",
      "         2         586.7766           35.34m\n",
      "        23          50.2546           58.36m\n",
      "         4          61.4412          111.02m\n",
      "         5          45.0986          127.48m\n",
      "         3         109.7666           97.47m\n",
      "         2         400.7687           35.63m\n",
      "         7          48.7324           52.17m\n",
      "         7          50.8318           55.65m\n",
      "        24          50.2267           57.40m\n",
      "         5          68.6642          130.59m\n",
      "         9          52.0114           47.86m\n",
      "         3         341.4746           41.29m\n",
      "        25          50.1146           56.09m\n",
      "[CV 4/5; 4/36] END learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=100;, score=-90.230 total time=18.7min\n",
      "[CV 3/5; 7/36] START learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "         6          34.7129          123.44m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         4          73.0207          100.57m\n",
      "        26          47.7744          145.24m\n",
      "         5          54.5157          113.53m\n",
      "         8          48.2629           50.69m\n",
      "         3         180.7264           42.01m\n",
      "         1         902.6604           17.99m\n",
      "        39          47.7584           19.01m\n",
      "         8          50.2581           55.92m\n",
      "         6          55.1579          131.09m\n",
      "         4         229.9118           43.80m\n",
      "        10          51.9274           46.89m\n",
      "         5          50.6466          100.79m\n",
      "        27          47.7707          141.11m\n",
      "         7          32.1019          121.99m\n",
      "         6          52.0390          112.98m\n",
      "         4         127.5156           43.45m\n",
      "         2         499.9213           34.22m\n",
      "         9          48.0379           49.20m\n",
      "         9          50.0175           54.56m\n",
      "         5         161.8482           44.92m\n",
      "         7          53.3459          128.44m\n",
      "         7          50.8318          109.91m\n",
      "        11          51.7907           46.54m\n",
      "         6          49.4896          102.66m\n",
      "         8          29.0627          118.88m\n",
      "[CV 2/5; 6/36] END learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=100;, score=-109.730 total time=10.3min\n",
      "[CV 4/5; 7/36] START learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        10          47.8815           47.62m\n",
      "         5         101.0026           44.75m\n",
      "         3         262.6215           40.67m\n",
      "        28          47.7678          137.88m\n",
      "         1         860.8262           18.72m\n",
      "        40          47.7583           17.48m\n",
      "         6          98.6615           45.76m\n",
      "        10          49.9156           53.71m\n",
      "        12          51.7640           45.25m\n",
      "         7          48.7324          102.89m\n",
      "         8          52.2124          128.48m\n",
      "         4         185.9962           43.32m\n",
      "         6          73.4614           45.47m\n",
      "        11          47.8350           46.80m\n",
      "         8          50.2581          111.76m\n",
      "         2         393.0237           34.26m\n",
      "        29          47.7646          134.30m\n",
      "         7          70.9480           44.73m\n",
      "        13          51.7468           44.09m\n",
      "         8          48.2629          102.57m\n",
      "         5         107.8490           43.51m\n",
      "        12          47.7992           45.08m\n",
      "         9          50.0175          109.16m\n",
      "         7          59.9434           45.10m\n",
      "        41          47.7581           15.80m\n",
      "        11          49.8602           52.79m\n",
      "        30          47.7631          130.87m\n",
      "         3         235.0322           42.31m\n",
      "         9          52.0114          129.31m\n",
      "         8          60.7745           43.88m\n",
      "        14          51.7335           42.47m\n",
      "         6          83.0781           43.35m\n",
      "         9          48.0379          101.66m\n",
      "        13          47.7851           43.99m\n",
      "        10          49.9156          107.87m\n",
      "         8          51.0734           44.95m\n",
      "         4         182.9162           44.06m\n",
      "        12          49.8311           51.76m\n",
      "[CV 4/5; 5/36] END learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=50;, score=-96.232 total time=16.4min\n",
      "[CV 5/5; 7/36] START learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "        31          47.7613          127.82m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         9          55.3355           42.82m\n",
      "        15          51.7216           40.80m\n",
      "[CV 3/5; 5/36] END learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=50;, score=-61.776 total time=17.5min\n",
      "[CV 1/5; 8/36] START learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        10          51.9274          128.47m\n",
      "         1         994.5380           23.62m\n",
      "        10          47.8815          100.09m\n",
      "         7          74.4889           43.65m\n",
      "[CV 3/5; 7/36] END learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=50;, score=-66.697 total time= 7.1min\n",
      "[CV 2/5; 8/36] START learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1339.3149           34.62m\n",
      "        42          47.7580           14.12m\n",
      "         9          32.5892           43.79m\n",
      "        14          47.7747           42.87m\n",
      "        11          49.8602          106.79m\n",
      "         5         158.9505           44.37m\n",
      "         1         981.2696           33.77m\n",
      "        10          52.5406           42.10m\n",
      "        32          47.7606          125.59m\n",
      "         2         586.7766           67.43m\n",
      "        11          47.8350           99.70m\n",
      "        10          30.3826           42.72m\n",
      "         2         372.4628           43.77m\n",
      "        11          51.7907          128.37m\n",
      "        15          47.7661           41.74m\n",
      "         2         400.7687           69.74m\n",
      "        12          49.8311          105.61m\n",
      "[CV 4/5; 6/36] END learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=100;, score=-96.232 total time=14.4min\n",
      "[CV 3/5; 8/36] START learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         6          84.8141           45.31m\n",
      "        11          51.1017           40.90m\n",
      "         1         902.6604           43.58m\n",
      "        12          47.7992           97.12m\n",
      "         3         341.4746           81.31m\n",
      "        43          47.7580           12.44m\n",
      "        11          29.7209           41.87m\n",
      "        33          47.7600          122.75m\n",
      "         7          68.4744           44.43m\n",
      "         3         180.7264           84.42m\n",
      "        12          50.0439           39.79m\n",
      "        16          47.7632           40.75m\n",
      "[CV 5/5; 5/36] END learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=50;, score=-61.402 total time=19.2min\n",
      "[CV 4/5; 8/36] START learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         3         262.4291           53.36m\n",
      "        12          51.7640          127.45m\n",
      "         2         499.9213           74.29m\n",
      "         1         860.8262           35.87m\n",
      "        13          47.7851           95.91m\n",
      "         4         229.9118           88.39m\n",
      "        12          29.0113           41.46m\n",
      "         8          61.1435           43.85m\n",
      "         4         127.5156           88.86m\n",
      "        13          49.2928           38.94m\n",
      "[CV 1/5; 7/36] END learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=50;, score=-115.438 total time=13.7min\n",
      "[CV 5/5; 8/36] START learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        34          47.7594          120.41m\n",
      "         1         994.5380           35.54m\n",
      "         3         262.6215           87.84m\n",
      "         2         393.0237           71.58m\n",
      "        14          47.7747           94.73m\n",
      "         4         149.7487           56.95m\n",
      "        13          51.7468          126.81m\n",
      "         5         161.8482           91.99m\n",
      "        44          47.7579           10.73m\n",
      "        13          28.4436           40.43m\n",
      "         9          56.6328           43.38m\n",
      "         5         101.0026           92.86m\n",
      "         2         372.4628           68.32m\n",
      "        35          47.7590          117.57m\n",
      "        15          47.7661           93.79m\n",
      "         3         235.0322           86.82m\n",
      "         4         185.9962           99.21m\n",
      "        14          28.0757           38.95m\n",
      "         6          98.6615           95.46m\n",
      "        14          51.7335          124.73m\n",
      "         5          79.5526           58.39m\n",
      "        10          54.2343           43.08m\n",
      "         6          73.4614           95.90m\n",
      "         3         262.4291           84.64m\n",
      "         4         182.9162           90.87m\n",
      "        16          47.7632           93.13m\n",
      "[CV 5/5; 6/36] END learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=100;, score=-61.402 total time=17.8min\n",
      "[CV 1/5; 9/36] START learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         5         107.8490           98.70m\n",
      "        15          27.5482           37.69m\n",
      "[CV 2/5; 7/36] END learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=50;, score=-111.233 total time=16.2min\n",
      "[CV 2/5; 9/36] START learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        36          47.7588          115.19m\n",
      "        45          47.7579            8.99m\n",
      "         1        1104.4979           17.37m\n",
      "         7          70.9480           96.83m\n",
      "         1         804.1341           16.70m\n",
      "        15          51.7216          122.27m\n",
      "[CV 3/5; 6/36] END learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=100;, score=-61.776 total time=21.6min\n",
      "[CV 3/5; 9/36] START learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         7          59.9434           96.12m\n",
      "         6          61.2869           57.60m\n",
      "        11          52.9946           42.18m\n",
      "         4         149.7487           90.71m\n",
      "         5         158.9505           92.34m\n",
      "         1         682.0306           25.33m\n",
      "         6          83.0781           97.97m\n",
      "         8          60.7745           97.46m\n",
      "         2         246.3224           43.65m\n",
      "         8          51.0734           97.20m\n",
      "        12          52.0782           41.59m\n",
      "        37          47.7587          113.43m\n",
      "         7          53.7813           57.14m\n",
      "         5          79.5526           94.11m\n",
      "         6          84.8141           94.73m\n",
      "         7          74.4889           99.23m\n",
      "[CV 3/5; 8/36] END learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=100;, score=-66.697 total time= 7.5min\n",
      "[CV 4/5; 9/36] START learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2         276.1451           54.41m\n",
      "        46          47.7578            7.22m\n",
      "         9          55.3355           96.40m\n",
      "         1         565.5388           18.33m\n",
      "        13          51.1247           40.32m\n",
      "         9          32.5892           96.31m\n",
      "         2         244.6607           63.04m\n",
      "         6          61.2869           94.36m\n",
      "         7          68.4744           94.53m\n",
      "         3          70.2009           53.22m\n",
      "         8          51.1530           55.76m\n",
      "        10          52.5406           97.44m\n",
      "         2         129.7127           38.75m\n",
      "        10          30.3826           95.30m\n",
      "        14          50.7635           39.24m\n",
      "        38          47.7585          112.05m\n",
      "         3         177.6948           64.23m\n",
      "         7          53.7813           94.72m\n",
      "         8          61.1435           94.54m\n",
      "         9          50.1095           54.20m\n",
      "        47          47.7578            5.46m\n",
      "        11          29.7209           94.43m\n",
      "        15          50.5079           38.22m\n",
      "        11          51.1017           98.37m\n",
      "         4          52.1298           63.09m\n",
      "         3         106.4057           77.45m\n",
      "         9          56.6328           94.82m\n",
      "         8          51.1530           95.88m\n",
      "         3          72.7952           50.44m\n",
      "        39          47.7584          110.03m\n",
      "        10          48.8449           52.03m\n",
      "         4          80.2620           68.21m\n",
      "        12          29.0113           94.01m\n",
      "        16          50.3540           37.22m\n",
      "        12          50.0439           99.03m\n",
      "         9          50.1095           96.61m\n",
      "        48          47.7576            3.64m\n",
      "        10          54.2343           95.64m\n",
      "        11          48.5666           50.66m\n",
      "         4          61.6273           56.75m\n",
      "         5          48.9361           66.20m\n",
      "        13          28.4436           93.58m\n",
      "        17          50.2474           36.30m\n",
      "        40          47.7583          108.39m\n",
      "         4          79.7638           82.91m\n",
      "        10          48.8449           95.55m\n",
      "         5          47.4131           69.46m\n",
      "        11          52.9946           95.24m\n",
      "        13          49.2928          100.01m\n",
      "[CV 1/5; 8/36] END learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=100;, score=-115.438 total time=15.0min\n",
      "[CV 5/5; 9/36] START learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        12          48.2521           48.64m\n",
      "        14          28.0757           92.03m\n",
      "         1         808.4092           18.08m\n",
      "        18          50.1423           35.07m\n",
      "         5          54.5207           60.00m\n",
      "        12          52.0782           95.41m\n",
      "        41          47.7581          106.22m\n",
      "        13          48.0661           46.63m\n",
      "        49          47.7575            1.84m\n",
      "        11          48.5666           98.23m\n",
      "         6          47.4290           69.23m\n",
      "        15          27.5482           90.56m\n",
      "[CV 2/5; 8/36] END learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=100;, score=-111.233 total time=16.0min\n",
      "[CV 1/5; 10/36] START learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        19          50.0025           33.86m\n",
      "[CV 4/5; 7/36] END learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=50;, score=-90.335 total time=20.8min\n",
      "[CV 2/5; 10/36] START learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1104.4979           35.93m\n",
      "         6          37.4918           71.69m\n",
      "         1         804.1341           34.53m\n",
      "         2         164.5624           45.84m\n",
      "        13          51.1247           93.88m\n",
      "        14          47.9995           45.09m\n",
      "         5          65.8171           88.69m\n",
      "        12          48.2521           99.24m\n",
      "        42          47.7580          104.19m\n",
      "         6          51.9729           62.19m\n",
      "        14          50.7635           92.89m\n",
      "         2         246.3224           93.13m\n",
      "        15          47.9416           43.50m\n",
      "         3         117.0060           55.22m\n",
      "         2         276.1451          102.27m\n",
      "         7          32.4047           70.88m\n",
      "[CV 2/5; 9/36] END learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=50;, score=-112.123 total time=11.5min\n",
      "[CV 3/5; 10/36] START learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         7          46.0629           71.97m\n",
      "        50          47.7575            0.00s\n",
      "         1         682.0306           36.67m\n",
      "[CV 5/5; 11/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=50;, score=-59.005 total time=92.7min\n",
      "        13          48.0661           99.44m\n",
      "        15          50.5079           91.55m\n",
      "        43          47.7580          102.21m\n",
      "         6          60.8799           88.10m\n",
      "         7          50.7279           62.83m\n",
      "        16          47.9086           42.14m\n",
      "         3          70.2009          113.40m\n",
      "         3         177.6948          119.07m\n",
      "         4          76.8599           60.07m\n",
      "        16          50.3540           90.33m\n",
      "        14          47.9995           99.22m\n",
      "         8          45.7252           70.51m\n",
      "         2         244.6607           92.53m\n",
      "        17          47.8767           40.32m\n",
      "        44          47.7579          100.17m\n",
      "        15          47.9416           97.37m\n",
      "        17          50.2474           90.02m\n",
      "        18          47.8465           38.75m\n",
      "         4          52.1298          127.43m\n",
      "         7          54.1090           87.51m\n",
      "         5          54.4622           60.70m\n",
      "         8          50.2037           65.70m\n",
      "         4          80.2620          128.54m\n",
      "         9          45.0433           69.34m\n",
      "         3         106.4057          117.54m\n",
      "        16          47.9086           96.31m\n",
      "        19          47.8257           37.15m\n",
      "        18          50.1423           88.67m\n",
      "        45          47.7579           98.11m\n",
      "         6          51.6058           58.30m\n",
      "         5          48.9361          130.08m\n",
      "        17          47.8767           94.21m\n",
      "        20          47.8083           35.57m\n",
      "         5          47.4131          132.99m\n",
      "        19          50.0025           87.67m\n",
      "[CV 4/5; 8/36] END learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=100;, score=-90.335 total time=20.6min\n",
      "[CV 4/5; 10/36] START learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        10          44.8111           67.74m\n",
      "         8          52.5096           86.14m\n",
      "         4          79.7638          127.42m\n",
      "         1         565.5388           40.00m\n",
      "        46          47.7578           95.98m\n",
      "         9          49.9632           67.77m\n",
      "         7          50.4547           57.37m\n",
      "        21          47.7939           34.04m\n",
      "        18          47.8465           92.44m\n",
      "         6          47.4290          136.00m\n",
      "         2         129.7127           79.20m\n",
      "         6          37.4918          137.70m\n",
      "        19          47.8257           90.61m\n",
      "        22          47.7842           32.70m\n",
      "         9          52.1608           82.55m\n",
      "        11          44.6044           66.38m\n",
      "        47          47.7578           94.09m\n",
      "         5          65.8171          136.66m\n",
      "         8          48.3742           57.63m\n",
      "        10          49.8710           67.70m\n",
      "        20          47.8083           88.72m\n",
      "        23          47.7777           31.38m\n",
      "         7          32.4047          136.40m\n",
      "[CV 2/5; 10/36] END learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=100;, score=-112.123 total time=10.3min\n",
      "[CV 5/5; 10/36] START learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         3          72.7952          101.45m\n",
      "         7          46.0629          139.41m\n",
      "         1         808.4092           32.26m\n",
      "        48          47.7576           91.87m\n",
      "        21          47.7939           86.94m\n",
      "        10          51.8592           79.57m\n",
      "[CV 3/5; 9/36] END learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=50;, score=-60.030 total time=19.9min\n",
      "[CV 1/5; 11/36] START learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        12          44.5405           64.98m\n",
      "         6          60.8799          138.97m\n",
      "         9          47.9929           56.81m\n",
      "        24          47.7733           30.11m\n",
      "         1        1339.3138           20.46m\n",
      "        11          49.8326           67.01m\n",
      "        22          47.7842           85.71m\n",
      "         8          45.7252          138.09m\n",
      "         4          61.6273          113.45m\n",
      "        25          47.7692           28.71m\n",
      "         2         164.5624           84.39m\n",
      "        49          47.7575           90.08m\n",
      "        13          44.4924           62.87m\n",
      "        10          47.8880           56.39m\n",
      "         2         586.2908           46.32m\n",
      "         7          54.1090          142.42m\n",
      "        26          47.7662           27.32m\n",
      "        23          47.7777           84.77m\n",
      "         3         117.0060          103.56m\n",
      "         5          54.5207          120.00m\n",
      "        27          47.7639           25.74m\n",
      "        12          49.8117           65.78m\n",
      "         9          45.0433          139.03m\n",
      "        11          47.8276           54.28m\n",
      "        14          44.4716           60.53m\n",
      "        24          47.7733           84.04m\n",
      "        50          47.7575           88.45m\n",
      "        28          47.7623           24.58m\n",
      "         8          52.5096          143.70m\n",
      "         3         340.7842           62.18m\n",
      "         4          76.8599          116.78m\n",
      "        12          47.7908           52.65m\n",
      "         6          51.9729          125.19m\n",
      "        25          47.7692           82.57m\n",
      "        15          44.4582           57.89m\n",
      "        29          47.7607           23.24m\n",
      "[CV 5/5; 7/36] END learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=50;, score=-56.933 total time=32.1min\n",
      "[CV 2/5; 11/36] START learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        10          44.8111          139.90m\n",
      "        13          49.8028           64.85m\n",
      "         1         981.2693           15.37m\n",
      "        51          47.7575           86.41m\n",
      "         9          52.1608          139.83m\n",
      "        26          47.7662           81.24m\n",
      "        13          47.7771           51.77m\n",
      "         5          54.4622          122.45m\n",
      "         7          50.7279          127.99m\n",
      "         4         229.8408           68.19m\n",
      "        27          47.7639           78.86m\n",
      "         2         400.4081           39.22m\n",
      "        16          44.4312           56.53m\n",
      "        11          44.6044          138.81m\n",
      "        52          47.7575           84.37m\n",
      "        10          51.8592          136.99m\n",
      "[CV 3/5; 10/36] END learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=100;, score=-60.030 total time=15.2min\n",
      "[CV 3/5; 11/36] START learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        14          49.7986           63.16m\n",
      "         1         902.6599           16.51m\n",
      "         6          51.6058          120.18m\n",
      "        28          47.7623           78.13m\n",
      "        17          44.3803           54.51m\n",
      "        14          47.7695           51.34m\n",
      "         5         161.3421           68.81m\n",
      "         8          50.2037          131.21m\n",
      "        12          44.5405          137.80m\n",
      "         3         179.3529           53.92m\n",
      "         2         499.6575           36.44m\n",
      "        29          47.7607           76.43m\n",
      "[CV 5/5; 8/36] END learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=100;, score=-56.933 total time=31.2min\n",
      "[CV 4/5; 11/36] START learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        15          49.7942           61.35m\n",
      "[CV 4/5; 9/36] END learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=50;, score=-95.496 total time=26.3min\n",
      "[CV 5/5; 11/36] START learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        53          47.7574           82.81m\n",
      "         1         860.8251           18.06m\n",
      "         7          50.4547          120.01m\n",
      "         1         994.5360           20.46m\n",
      "        18          44.3763           52.64m\n",
      "        13          44.4924          135.68m\n",
      "        15          47.7634           50.52m\n",
      "         4         129.4480           57.70m\n",
      "         2         392.4241           36.85m\n",
      "         9          49.9632          134.06m\n",
      "         3         262.6485           51.06m\n",
      "         6          98.3093           71.72m\n",
      "        54          47.7574           80.92m\n",
      "         8          48.3742          122.59m\n",
      "         2         372.1152           45.41m\n",
      "        14          44.4716          133.50m\n",
      "        19          44.3452           51.19m\n",
      "        16          47.7593           49.50m\n",
      "        10          49.8710          134.53m\n",
      "         3         234.1413           50.79m\n",
      "         5         103.5974           60.90m\n",
      "         4         177.3770           56.20m\n",
      "        55          47.7573           79.06m\n",
      "         9          47.9929          123.17m\n",
      "         7          69.1107           72.51m\n",
      "        15          44.4582          129.89m\n",
      "         3         282.3256           60.24m\n",
      "        20          44.3210           49.41m\n",
      "        17          47.7585           48.58m\n",
      "[CV 5/5; 9/36] END learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=50;, score=-58.693 total time=25.0min\n",
      "[CV 1/5; 12/36] START learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        11          49.8326          133.86m\n",
      "         4         174.9865           55.55m\n",
      "         6          58.3266           61.44m\n",
      "         5         105.8279           58.23m\n",
      "         1        1339.3138           31.96m\n",
      "        10          47.8880          124.18m\n",
      "        56          47.7573           77.37m\n",
      "         8          61.0794           70.52m\n",
      "        16          44.4312          129.69m\n",
      "        21          44.3199           47.40m\n",
      "[CV 1/5; 9/36] END learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=50;, score=-124.248 total time=34.3min\n",
      "[CV 2/5; 12/36] START learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         981.2693           33.95m\n",
      "         4         132.1711           68.84m\n",
      "         5         152.9064           56.29m\n",
      "         2         586.2908           78.05m\n",
      "        12          49.8117          132.04m\n",
      "        11          47.8276          121.55m\n",
      "         7          49.2283           61.76m\n",
      "         6          81.4632           60.97m\n",
      "        57          47.7573           75.59m\n",
      "        17          44.3803          127.79m\n",
      "         9          54.0187           70.44m\n",
      "         2         400.4081           78.43m\n",
      "         6          84.7855           55.00m\n",
      "         8          37.7773           59.01m\n",
      "        12          47.7908          120.62m\n",
      "        13          49.8028          131.25m\n",
      "         3         340.7842          106.54m\n",
      "         5          85.3893           69.99m\n",
      "         7          74.2797           60.75m\n",
      "        18          44.3763          126.14m\n",
      "         9          33.9201           56.90m\n",
      "        58          47.7573           73.89m\n",
      "         7          69.8024           56.14m\n",
      "         3         179.3529          108.99m\n",
      "        13          47.7771          120.79m\n",
      "        14          49.7986          130.30m\n",
      "        10          52.2073           70.16m\n",
      "         4         229.8408          116.67m\n",
      "         6          72.5512           71.88m\n",
      "         8          57.2759           61.72m\n",
      "        10          32.2354           55.22m\n",
      "        19          44.3452          125.53m\n",
      "         8          64.5402           55.91m\n",
      "        59          47.7573           72.14m\n",
      "         4         129.4480          118.98m\n",
      "        15          49.7942          128.30m\n",
      "[CV 4/5; 10/36] END learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=100;, score=-95.496 total time=22.7min\n",
      "[CV 3/5; 12/36] START learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "         5         161.3421          118.82m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        14          47.7695          121.92m\n",
      "         1         902.6599           34.57m\n",
      "         7          66.6050           69.72m\n",
      "        11          50.0677           69.89m\n",
      "        11          30.3797           54.56m\n",
      "        60          47.7573           70.01m\n",
      "        20          44.3210          124.00m\n",
      "         9          55.0591           62.09m\n",
      "         9          59.0912           55.02m\n",
      "         5         103.5974          125.35m\n",
      "         2         499.6575           77.84m\n",
      "         6          98.3093          126.38m\n",
      "        15          47.7634          121.16m\n",
      "        12          29.4809           52.47m\n",
      "        61          47.7572           68.00m\n",
      "        21          44.3199          121.83m\n",
      "[CV 1/5; 10/36] END learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=100;, score=-124.248 total time=32.4min\n",
      "[CV 4/5; 12/36] START learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         8          56.7313           69.75m\n",
      "         1         860.8251           32.65m\n",
      "        12          49.0238           68.85m\n",
      "        10          57.1019           55.03m\n",
      "         6          58.3266          127.28m\n",
      "        10          53.8373           62.51m\n",
      "         3         262.6485          109.29m\n",
      "        16          47.7593          120.47m\n",
      "         7          69.1107          129.86m\n",
      "        13          29.0831           51.70m\n",
      "         2         392.4241           71.00m\n",
      "        62          47.7572           66.26m\n",
      "        11          55.4021           53.89m\n",
      "         9          51.5883           68.72m\n",
      "        14          28.8203           49.15m\n",
      "[CV 2/5; 11/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=50;, score=-114.692 total time=19.1min\n",
      "[CV 5/5; 12/36] START learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        13          48.2942           67.25m\n",
      "         7          49.2283          130.28m\n",
      "         8          61.0794          127.03m\n",
      "        11          53.1495           61.78m\n",
      "         4         177.3770          119.70m\n",
      "         1         994.5360           32.60m\n",
      "        17          47.7585          120.17m\n",
      "[CV 5/5; 10/36] END learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=100;, score=-58.693 total time=24.6min\n",
      "[CV 1/5; 13/36] START learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1899.0452           15.48m\n",
      "         3         234.1413           99.68m\n",
      "        63          47.7572           64.37m\n",
      "        12          53.8549           52.57m\n",
      "         2         983.4170           14.26m\n",
      "         8          37.7773          127.26m\n",
      "         3         696.1531           14.44m\n",
      "         2         372.1152           72.40m\n",
      "        14          47.9676           64.81m\n",
      "        10          50.6161           68.17m\n",
      "         4         361.1893           15.05m\n",
      "         9          54.0187          128.89m\n",
      "        12          52.6787           60.14m\n",
      "         5         105.8279          124.53m\n",
      "         5         192.5100           15.55m\n",
      "         4         174.9865          111.88m\n",
      "        64          47.7572           62.51m\n",
      "        13          52.9798           51.80m\n",
      "         6         138.9550           16.16m\n",
      "         9          33.9201          125.78m\n",
      "         3         282.3256           97.46m\n",
      "         7          98.1754           16.39m\n",
      "[CV 1/5; 13/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=50;, score=-120.223 total time= 2.7min\n",
      "[CV 2/5; 13/36] START learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        15          47.8298           62.75m\n",
      "         1        3151.2243           14.50m\n",
      "        10          52.2073          131.01m\n",
      "        13          52.5242           59.18m\n",
      "        11          49.6138           67.41m\n",
      "         6          81.4632          130.52m\n",
      "         2        1071.4978           14.16m\n",
      "         5         152.9064          115.49m\n",
      "         3         542.7910           14.05m\n",
      "        10          32.2354          123.63m\n",
      "        65          47.7572           60.68m\n",
      "        14          52.3478           50.61m\n",
      "         4         249.3333           14.70m\n",
      "         4         132.1711          113.75m\n",
      "         5         153.3629           16.30m\n",
      "         6          84.7855          114.85m\n",
      "         7          74.2797          131.53m\n",
      "        12          49.1234           65.57m\n",
      "        16          47.6786           61.56m\n",
      "        11          50.0677          131.88m\n",
      "        14          52.2286           57.96m\n",
      "         6          84.6666           17.40m\n",
      "        15          51.8028           49.17m\n",
      "        11          30.3797          123.88m\n",
      "        66          47.7572           58.86m\n",
      "         7          51.8957           17.86m\n",
      "         5          85.3893          118.06m\n",
      "         8          39.2128           18.35m\n",
      "         7          69.8024          119.31m\n",
      "        16          51.1908           47.77m\n",
      "         8          57.2759          133.42m\n",
      "        15          52.0568           56.23m\n",
      "        12          29.4809          122.54m\n",
      "        67          47.7572           56.96m\n",
      "        12          49.0238          131.73m\n",
      "        17          47.6080           59.61m\n",
      "         9          34.1240           18.59m\n",
      "[CV 2/5; 13/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=50;, score=-103.656 total time= 4.1min\n",
      "[CV 3/5; 13/36] START learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        2999.3116           15.54m\n",
      "        13          48.7491           65.10m\n",
      "         2        1102.9615           14.82m\n",
      "         6          72.5512          122.15m\n",
      "         3         520.8858           14.93m\n",
      "         8          64.5402          120.67m\n",
      "         4         237.3863           15.66m\n",
      "        68          47.7572           55.16m\n",
      "         9          55.0591          133.89m\n",
      "        13          29.0831          122.62m\n",
      "        13          48.2942          130.80m\n",
      "        17          51.0313           47.04m\n",
      "        16          51.9315           55.17m\n",
      "[CV 3/5; 11/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=50;, score=-63.673 total time=26.0min\n",
      "[CV 4/5; 13/36] START learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         5         145.9303           15.75m\n",
      "        14          48.6247           62.52m\n",
      "         7          66.6050          119.62m\n",
      "        18          47.5580           58.30m\n",
      "         1        2529.4378           17.25m\n",
      "         6         110.4451           15.88m\n",
      "[CV 3/5; 13/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=50;, score=-54.510 total time= 2.2min\n",
      "[CV 5/5; 13/36] START learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        14          28.8203          118.34m\n",
      "[CV 2/5; 12/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100;, score=-114.692 total time=19.3min\n",
      "[CV 1/5; 14/36] START learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2        1026.7911           18.60m\n",
      "         1        3218.5901           15.16m\n",
      "         9          59.0912          120.51m\n",
      "         1        1899.0452           30.17m\n",
      "        14          47.9676          127.68m\n",
      "         3         481.7594           17.80m\n",
      "         2        1305.1508           15.01m\n",
      "        69          47.7572           53.33m\n",
      "         2         983.4170           28.21m\n",
      "        18          50.7296           45.96m\n",
      "[CV 4/5; 11/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=50;, score=-90.352 total time=25.9min\n",
      "[CV 2/5; 14/36] START learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         3         528.2261           14.75m\n",
      "         4         274.6815           17.08m\n",
      "        10          53.8373          135.39m\n",
      "         3         696.1531           29.02m\n",
      "         8          56.7313          120.68m\n",
      "        15          48.4029           60.53m\n",
      "         1        3151.2243           37.26m\n",
      "         4         231.4702           14.98m\n",
      "         5         182.1353           16.93m\n",
      "         4         361.1893           30.80m\n",
      "         2        1071.4978           34.41m\n",
      "         5         141.8588           15.11m\n",
      "        15          47.8298          125.29m\n",
      "        19          47.5210           56.82m\n",
      "         6         102.4838           17.54m\n",
      "         5         192.5100           31.96m\n",
      "         3         542.7910           33.05m\n",
      "        10          57.1019          121.94m\n",
      "        70          47.7572           51.46m\n",
      "         6         111.6096           16.00m\n",
      "[CV 5/5; 13/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=50;, score=-58.017 total time= 2.2min\n",
      "[CV 3/5; 14/36] START learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         6         138.9550           33.55m\n",
      "         4         249.3333           34.36m\n",
      "         7          76.4132           17.82m\n",
      "[CV 4/5; 13/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=50;, score=-96.008 total time= 2.9min\n",
      "[CV 4/5; 14/36] START learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        16          48.2156           57.78m\n",
      "         1        2999.3116           31.00m\n",
      "         9          51.5883          120.11m\n",
      "        11          53.1495          134.15m\n",
      "         1        2529.4378           30.75m\n",
      "         7          98.1754           34.48m\n",
      "[CV 1/5; 14/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100;, score=-120.223 total time= 2.6min\n",
      "[CV 5/5; 14/36] START learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2        1102.9615           29.90m\n",
      "         5         153.3629           36.86m\n",
      "         2        1026.7911           31.17m\n",
      "         1        3218.5901           30.65m\n",
      "         3         520.8858           30.35m\n",
      "         3         481.7594           31.34m\n",
      "        11          55.4021          121.13m\n",
      "         2        1305.1508           30.86m\n",
      "         6          84.6666           39.96m\n",
      "         4         237.3863           32.15m\n",
      "        16          47.6786          125.26m\n",
      "        20          47.4838           54.83m\n",
      "         4         274.6815           31.22m\n",
      "         3         528.2261           30.74m\n",
      "        71          47.7572           49.73m\n",
      "         5         145.9303           33.06m\n",
      "         7          51.8957           41.19m\n",
      "        12          52.6787          132.52m\n",
      "        10          50.6161          120.66m\n",
      "         4         231.4702           31.85m\n",
      "         5         182.1353           33.11m\n",
      "        17          48.1631           56.37m\n",
      "         6         110.4451           33.70m\n",
      "[CV 3/5; 14/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100;, score=-54.510 total time= 2.2min\n",
      "[CV 1/5; 15/36] START learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         5         141.8588           32.47m\n",
      "         8          39.2128           42.40m\n",
      "         6         102.4838           35.31m\n",
      "         1        2075.1286           15.50m\n",
      "        12          53.8549          120.05m\n",
      "        17          47.6080          123.30m\n",
      "         2        1323.3872           15.62m\n",
      "         6         111.6096           34.89m\n",
      "[CV 5/5; 14/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100;, score=-58.017 total time= 2.2min\n",
      "[CV 2/5; 15/36] START learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         7          76.4132           36.57m\n",
      "[CV 4/5; 14/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100;, score=-96.008 total time= 2.8min\n",
      "[CV 3/5; 15/36] START learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         9          34.1240           43.17m\n",
      "[CV 2/5; 14/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100;, score=-103.656 total time= 4.3min\n",
      "[CV 4/5; 15/36] START learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        72          47.7572           47.93m\n",
      "         1        3284.0420           14.78m\n",
      "         3         942.9794           16.41m\n",
      "         1        3164.7536           15.72m\n",
      "         1        2750.8584           17.97m\n",
      "        11          49.6138          120.77m\n",
      "        21          47.4609           53.24m\n",
      "         2        1419.4620           14.13m\n",
      "         2        1315.0728           14.85m\n",
      "        13          52.5242          132.82m\n",
      "         4         582.8049           17.17m\n",
      "         2        1184.0386           15.78m\n",
      "         3         898.4680           14.91m\n",
      "        18          47.9360           54.73m\n",
      "         3         781.2353           15.57m\n",
      "         3         761.7676           16.59m\n",
      "         4         458.2409           15.27m\n",
      "        13          52.9798          120.01m\n",
      "        73          47.7569           46.02m\n",
      "         5         379.1724           18.45m\n",
      "         4         462.0405           16.02m\n",
      "        18          47.5580          122.39m\n",
      "         4         483.9140           17.01m\n",
      "         5         273.4793           16.49m\n",
      "        12          49.1234          118.95m\n",
      "         6         285.3625           19.30m\n",
      "         5         336.4531           17.35m\n",
      "         5         259.6757           17.68m\n",
      "         6         175.1331           17.40m\n",
      "        14          52.2286          131.68m\n",
      "        19          47.8727           52.44m\n",
      "         7         183.3552           20.03m\n",
      "         6         166.6123           18.00m\n",
      "         6         270.8113           18.43m\n",
      "[CV 3/5; 15/36] END learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=50;, score=-55.638 total time= 2.5min\n",
      "[CV 5/5; 15/36] START learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "        22          45.4532           51.72m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        14          52.3478          119.30m\n",
      "         7         107.9258           18.21m\n",
      "         1        3358.1380           18.52m\n",
      "        74          47.7569           44.31m\n",
      "         8         142.7645           20.34m\n",
      "         7         116.6540           18.64m\n",
      "        19          47.5210          121.60m\n",
      "         2        1414.2529           17.05m\n",
      "        13          48.7491          119.94m\n",
      "         8          90.2013           18.68m\n",
      "[CV 2/5; 15/36] END learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=50;, score=-106.835 total time= 3.6min\n",
      "[CV 1/5; 16/36] START learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         3         624.4023           16.71m\n",
      "         8          99.8694           19.07m\n",
      "         9         103.8121           20.48m\n",
      "        20          47.8461           50.39m\n",
      "        15          52.0568          130.04m\n",
      "         1        2075.1286           35.16m\n",
      "         4         328.6203           17.11m\n",
      "        15          51.8028          117.88m\n",
      "         2        1323.3872           32.65m\n",
      "         9          87.6621           18.89m\n",
      "        10          72.2462           20.42m\n",
      "        20          47.4838          119.65m\n",
      "        75          47.7569           42.55m\n",
      "        23          45.3100           50.06m\n",
      "         3         942.9794           35.13m\n",
      "        14          48.6247          116.91m\n",
      "         5         208.3883           18.67m\n",
      "        10          68.0029           18.88m\n",
      "         4         582.8049           35.70m\n",
      "        11          67.3502           20.39m\n",
      "        21          47.8142           48.25m\n",
      "         6         138.0986           18.91m\n",
      "[CV 5/5; 15/36] END learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=50;, score=-55.010 total time= 2.6min\n",
      "[CV 2/5; 16/36] START learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        11          61.8905           18.90m\n",
      "         1        3284.0420           30.37m\n",
      "        16          51.1908          116.40m\n",
      "         5         379.1724           38.27m\n",
      "        16          51.9315          129.63m\n",
      "[CV 3/5; 12/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100;, score=-63.673 total time=24.7min\n",
      "[CV 3/5; 16/36] START learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        12          63.8375           20.14m\n",
      "         2        1419.4620           29.68m\n",
      "         1        3164.7536           31.05m\n",
      "        15          48.4029          115.47m\n",
      "        76          47.7569           40.76m\n",
      "        12          57.5509           18.70m\n",
      "         6         285.3625           40.09m\n",
      "        21          47.4609          118.46m\n",
      "        13          59.3723           19.74m\n",
      "         3         898.4680           33.10m\n",
      "         2        1315.0728           30.39m\n",
      "        24          45.1968           48.14m\n",
      "         4         458.2409           33.64m\n",
      "        13          56.0193           18.46m\n",
      "         3         781.2353           34.58m\n",
      "         7         183.3552           42.05m\n",
      "        14          57.0229           19.19m\n",
      "        22          47.7967           46.62m\n",
      "        16          48.2156          112.19m\n",
      "         4         462.0405           35.74m\n",
      "         5         273.4793           37.48m\n",
      "        17          51.0313          116.27m\n",
      "        14          54.4495           18.30m\n",
      "         8         142.7645           43.49m\n",
      "        15          54.7075           18.75m\n",
      "[CV 1/5; 15/36] END learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=50;, score=-117.885 total time= 8.0min\n",
      "[CV 4/5; 16/36] START learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         5         336.4531           36.74m\n",
      "        77          47.7569           39.03m\n",
      "         1        2750.8584           30.75m\n",
      "        15          52.7254           17.93m\n",
      "        22          45.4532          117.61m\n",
      "         6         175.1331           41.19m\n",
      "         9         103.8121           43.92m\n",
      "         2        1184.0386           29.72m\n",
      "        25          45.1097           46.12m\n",
      "         6         270.8113           39.30m\n",
      "[CV 3/5; 16/36] END learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=100;, score=-55.638 total time= 2.5min\n",
      "[CV 5/5; 16/36] START learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         3         761.7676           32.81m\n",
      "        16          52.0893           17.66m\n",
      "         1        3358.1380           30.37m\n",
      "        17          48.1631          111.62m\n",
      "        10          72.2462           44.48m\n",
      "         7         107.9258           43.60m\n",
      "        23          47.7874           45.20m\n",
      "        18          50.7296          115.57m\n",
      "[CV 4/5; 12/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100;, score=-90.352 total time=25.4min\n",
      "[CV 1/5; 17/36] START learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         4         483.9140           34.06m\n",
      "         2        1414.2529           32.81m\n",
      "        17          51.3923           17.24m\n",
      "        11          67.3502           44.58m\n",
      "         1        1898.8364           15.99m\n",
      "        78          47.7569           37.30m\n",
      "         8          90.2013           45.83m\n",
      "[CV 2/5; 16/36] END learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=100;, score=-106.835 total time= 4.0min\n",
      "[CV 2/5; 17/36] START learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         3         624.4023           35.93m\n",
      "         5         259.6757           36.44m\n",
      "         2         983.1577           15.57m\n",
      "        23          45.3100          116.32m\n",
      "         1        3151.0301           16.34m\n",
      "        18          50.9783           16.80m\n",
      "        12          63.8375           44.58m\n",
      "         4         328.6203           37.06m\n",
      "         3         696.0061           16.14m\n",
      "         6         166.6123           38.61m\n",
      "        18          47.9360          110.35m\n",
      "         2        1071.1869           16.51m\n",
      "        19          50.7806           16.36m\n",
      "        13          59.3723           44.38m\n",
      "         5         208.3883           39.42m\n",
      "         4         363.5446           17.23m\n",
      "        24          47.7801           43.44m\n",
      "        26          45.0443           44.63m\n",
      "         3         542.4582           17.79m\n",
      "         7         116.6540           41.31m\n",
      "        14          57.0229           43.93m\n",
      "        79          47.7569           35.57m\n",
      "[CV 5/5; 12/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100;, score=-59.004 total time=133.8min\n",
      "         6         138.0986           40.98m\n",
      "[CV 5/5; 16/36] END learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=100;, score=-55.010 total time= 2.6min\n",
      "[CV 3/5; 17/36] START learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "        20          50.6677           15.93m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         5         194.1874           18.27m\n",
      "        24          45.1968          114.41m\n",
      "        19          47.8727          107.68m\n",
      "         4         249.1320           19.18m\n",
      "         8          99.8694           42.52m\n",
      "         1        2999.1157           16.11m\n",
      "        15          54.7075           43.47m\n",
      "[CV 1/5; 16/36] END learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=100;, score=-117.885 total time= 7.7min\n",
      "[CV 4/5; 17/36] START learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        21          50.5060           15.52m\n",
      "         6         140.2118           19.31m\n",
      "         2        1102.7216           15.67m\n",
      "         5         153.1418           20.49m\n",
      "         9          87.6621           42.09m\n",
      "         1        2529.1603           14.17m\n",
      "         3         520.8616           15.44m\n",
      "        25          47.7760           41.55m\n",
      "         2        1026.6147           13.67m\n",
      "        27          44.9940           42.55m\n",
      "        20          47.8461          105.35m\n",
      "        25          45.1097          111.88m\n",
      "         7          98.7328           19.85m\n",
      "        22          50.4139           15.09m\n",
      "        10          68.0029           41.96m\n",
      "         3         481.6718           13.96m\n",
      "         4         237.2573           16.15m\n",
      "         6          85.1013           21.79m\n",
      "         4         274.4836           15.25m\n",
      "        11          61.8905           42.14m\n",
      "         8          88.5277           20.31m\n",
      "         5         145.8011           16.76m\n",
      "        23          50.3123           14.68m\n",
      "         7          52.1189           22.59m\n",
      "        21          47.8142          102.67m\n",
      "         5         181.5674           16.36m\n",
      "        12          57.5509           42.00m\n",
      "        28          44.9549           40.21m\n",
      "        26          47.7744           39.64m\n",
      "        24          50.2706           14.22m\n",
      "         6         110.2378           18.51m\n",
      "[CV 3/5; 17/36] END learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=50;, score=-56.682 total time= 2.5min\n",
      "[CV 5/5; 17/36] START learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         9          79.5299           20.81m\n",
      "[CV 1/5; 17/36] END learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=50;, score=-120.586 total time= 4.6min\n",
      "[CV 1/5; 18/36] START learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        26          45.0443          110.77m\n",
      "        13          56.0193           41.79m\n",
      "         1        1898.8364           28.61m\n",
      "         6         102.0372           17.67m\n",
      "         1        3218.3291           16.73m\n",
      "         8          39.8276           23.61m\n",
      "         2         983.1577           28.19m\n",
      "        25          50.2435           13.79m\n",
      "[CV 4/5; 15/36] END learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=50;, score=-89.153 total time=13.8min\n",
      "[CV 2/5; 18/36] START learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2        1304.9617           16.59m\n",
      "        27          47.7707           37.37m\n",
      "        14          54.4495           41.83m\n",
      "        22          47.7967          101.30m\n",
      "         3         696.0061           29.25m\n",
      "         1        3151.0301           33.53m\n",
      "         3         528.0377           16.38m\n",
      "         7          72.0498           19.33m\n",
      "        29          44.9245           38.02m\n",
      "         9          33.8806           24.23m\n",
      "[CV 2/5; 17/36] END learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=50;, score=-103.938 total time= 5.3min\n",
      "[CV 3/5; 18/36] START learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2        1071.1869           32.90m\n",
      "        15          52.7254           41.37m\n",
      "         4         363.5446           31.57m\n",
      "         4         231.3164           16.89m\n",
      "         1        2999.1157           28.83m\n",
      "         3         542.4582           32.34m\n",
      "        27          44.9940          109.21m\n",
      "         2        1102.7216           28.99m\n",
      "         5         194.1874           34.08m\n",
      "        16          52.0893           41.16m\n",
      "        28          47.7678           35.37m\n",
      "         8          62.7082           21.13m\n",
      "         5         141.7149           18.00m\n",
      "         4         249.1320           34.19m\n",
      "         3         520.8616           29.31m\n",
      "        23          47.7874           99.97m\n",
      "        17          51.3923           40.77m\n",
      "         6         140.2118           37.49m\n",
      "         4         237.2573           31.71m\n",
      "         6         111.6237           19.67m\n",
      "[CV 5/5; 17/36] END learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=50;, score=-57.049 total time= 2.7min\n",
      "[CV 4/5; 18/36] START learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        30          44.8905           36.02m\n",
      "         5         153.1418           39.35m\n",
      "        18          50.9783           40.36m\n",
      "        28          44.9549          106.85m\n",
      "         9          57.4816           22.84m\n",
      "        29          47.7646           33.33m\n",
      "         5         145.8011           33.19m\n",
      "         1        2529.1603           28.54m\n",
      "         7          98.7328           39.79m\n",
      "         2        1026.6147           28.05m\n",
      "        19          50.7806           40.00m\n",
      "         6          85.1013           44.12m\n",
      "        24          47.7801           98.97m\n",
      "         3         481.6718           29.32m\n",
      "         6         110.2378           37.27m\n",
      "[CV 3/5; 18/36] END learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=100;, score=-56.682 total time= 2.4min\n",
      "[CV 5/5; 18/36] START learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         8          88.5277           41.77m\n",
      "        10          55.3042           23.68m\n",
      "         1        3218.3291           27.97m\n",
      "        30          47.7631           31.35m\n",
      "        20          50.6677           39.65m\n",
      "         4         274.4836           32.22m\n",
      "         7          52.1189           47.35m\n",
      "         2        1304.9617           29.35m\n",
      "        29          44.9245          104.94m\n",
      "        31          44.8724           34.05m\n",
      "         9          79.5299           43.70m\n",
      "[CV 1/5; 18/36] END learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=100;, score=-120.586 total time= 4.3min\n",
      "[CV 1/5; 19/36] START learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        21          50.5060           39.16m\n",
      "         5         181.5674           34.60m\n",
      "         3         528.0377           31.08m\n",
      "         1        2074.9482           14.06m\n",
      "        11          53.1688           24.79m\n",
      "        25          47.7760           97.91m\n",
      "         2        1323.2898           14.04m\n",
      "        22          50.4139           38.81m\n",
      "         4         231.3164           33.12m\n",
      "        31          47.7613           29.49m\n",
      "         8          39.8276           50.95m\n",
      "         6         102.0372           37.75m\n",
      "         3         942.2784           16.75m\n",
      "         5         141.7149           36.02m\n",
      "        23          50.3123           38.46m\n",
      "        30          44.8905          103.52m\n",
      "        12          51.9175           25.14m\n",
      "         4         585.2610           17.19m\n",
      "[CV 4/5; 17/36] END learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=50;, score=-93.203 total time= 8.0min\n",
      "[CV 2/5; 19/36] START learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         9          33.8806           53.36m\n",
      "[CV 2/5; 18/36] END learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=100;, score=-103.938 total time= 5.3min\n",
      "[CV 3/5; 19/36] START learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         7          72.0498           43.05m\n",
      "        32          44.8583           32.22m\n",
      "[CV 1/5; 11/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=50;, score=-113.547 total time=57.3min\n",
      "[CV 4/5; 19/36] START learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        24          50.2706           37.97m\n",
      "         6         111.6237           39.37m\n",
      "[CV 5/5; 18/36] END learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=100;, score=-57.049 total time= 2.5min\n",
      "[CV 5/5; 19/36] START learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        3283.8813           16.46m\n",
      "        26          47.7744           96.60m\n",
      "         1        2750.6237           13.72m\n",
      "         1        3164.5876           15.93m\n",
      "        32          47.7606           27.82m\n",
      "         1        3357.9238           13.11m\n",
      "         5         381.9509           19.51m\n",
      "        25          50.2435           37.50m\n",
      "[CV 4/5; 16/36] END learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=100;, score=-89.153 total time=12.5min\n",
      "[CV 1/5; 20/36] START learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2        1314.8394           14.74m\n",
      "         2        1183.7923           14.03m\n",
      "         2        1419.3051           16.44m\n",
      "         8          62.7082           45.70m\n",
      "         2        1414.0271           13.51m\n",
      "         1        2074.9482           27.51m\n",
      "         3         780.9788           15.00m\n",
      "         3         898.3051           17.47m\n",
      "         3         761.5655           16.04m\n",
      "         3         623.9557           14.08m\n",
      "        31          44.8724          101.80m\n",
      "         2        1323.2898           27.25m\n",
      "        27          47.7707           94.15m\n",
      "         6         287.1717           20.88m\n",
      "         4         461.8323           17.03m\n",
      "        33          47.7600           26.01m\n",
      "         9          57.4816           48.02m\n",
      "         4         483.6090           17.28m\n",
      "         4         328.1798           15.66m\n",
      "         3         942.2784           32.63m\n",
      "         4         457.3058           20.13m\n",
      "         5         336.1673           17.82m\n",
      "         7         184.7207           22.16m\n",
      "         4         585.2610           34.54m\n",
      "         5         199.8168           17.50m\n",
      "         5         259.7520           19.53m\n",
      "        10          55.3042           49.40m\n",
      "        28          47.7678           92.39m\n",
      "         5         265.3422           22.23m\n",
      "         5         381.9509           39.62m\n",
      "        34          47.7594           24.35m\n",
      "         6         166.8764           19.87m\n",
      "        32          44.8583          100.62m\n",
      "[CV 1/5; 12/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100;, score=-113.547 total time=47.4min\n",
      "[CV 2/5; 20/36] START learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         6         270.4131           20.66m\n",
      "         6         126.6187           19.23m\n",
      "[CV 5/5; 19/36] END learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=50;, score=-54.821 total time= 2.6min\n",
      "[CV 3/5; 20/36] START learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         8         146.3800           23.26m\n",
      "         1        3283.8813           27.03m\n",
      "         1        3164.5876           29.03m\n",
      "        11          53.1688           51.62m\n",
      "         6         197.1172           24.18m\n",
      "[CV 2/5; 19/36] END learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=50;, score=-103.249 total time= 3.3min\n",
      "[CV 4/5; 20/36] START learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        29          47.7646           90.37m\n",
      "         2        1419.3051           27.25m\n",
      "         6         287.1717           43.17m\n",
      "         2        1314.8394           28.56m\n",
      "         7         116.8295           21.42m\n",
      "         1        2750.6237           34.53m\n",
      "         7         175.9568           22.16m\n",
      "         9          97.9131           23.90m\n",
      "        35          47.7590           22.59m\n",
      "         3         898.3051           29.51m\n",
      "         3         780.9788           29.71m\n",
      "        12          51.9175           52.23m\n",
      "[CV 4/5; 18/36] END learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=100;, score=-93.203 total time= 7.1min\n",
      "[CV 5/5; 20/36] START learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2        1183.7923           35.87m\n",
      "         7         184.7207           46.51m\n",
      "         1        3357.9238           28.61m\n",
      "        30          47.7631           88.35m\n",
      "         4         457.3058           35.09m\n",
      "         4         461.8323           34.17m\n",
      "         8          99.9438           22.98m\n",
      "         3         761.5655           38.66m\n",
      "         2        1414.0271           29.38m\n",
      "        10          80.5466           24.30m\n",
      "         8         137.7956           23.91m\n",
      "         5         336.1673           36.16m\n",
      "         3         623.9557           30.78m\n",
      "        36          47.7588           20.95m\n",
      "         5         265.3422           39.30m\n",
      "         8         146.3800           49.44m\n",
      "         4         483.6090           41.57m\n",
      "         9          84.7570           23.28m\n",
      "        31          47.7613           86.63m\n",
      "         4         328.1798           34.45m\n",
      "         9         126.0119           24.41m\n",
      "[CV 3/5; 19/36] END learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=50;, score=-56.719 total time= 5.4min\n",
      "[CV 1/5; 21/36] START learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        11          64.6639           24.81m\n",
      "         6         270.4131           41.97m\n",
      "         6         197.1172           43.74m\n",
      "[CV 2/5; 20/36] END learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=100;, score=-103.249 total time= 2.8min\n",
      "[CV 2/5; 21/36] START learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1898.8355           14.33m\n",
      "         5         259.7520           46.91m\n",
      "         9          97.9131           51.94m\n",
      "         1        3151.0297           13.58m\n",
      "         5         199.8168           38.22m\n",
      "        10          73.8145           23.57m\n",
      "         2         983.1547           14.34m\n",
      "         2        1071.1872           13.66m\n",
      "        12          58.6873           24.71m\n",
      "        37          47.7587           19.42m\n",
      "         7         175.9568           45.72m\n",
      "         3         695.9891           14.74m\n",
      "         6         166.8764           47.92m\n",
      "         3         542.4506           13.60m\n",
      "        10          80.5466           53.64m\n",
      "         6         126.6187           41.94m\n",
      "[CV 5/5; 20/36] END learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=100;, score=-54.821 total time= 2.7min\n",
      "[CV 3/5; 21/36] START learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        32          47.7606           85.56m\n",
      "        11          66.9623           23.86m\n",
      "         4         363.5572           16.15m\n",
      "         1        2999.1154           15.66m\n",
      "         4         249.1241           15.24m\n",
      "         7         116.8295           51.69m\n",
      "         8         137.7956           49.58m\n",
      "        13          54.6155           24.84m\n",
      "         5         194.1994           17.00m\n",
      "         2        1102.7196           16.63m\n",
      "        11          64.6639           55.24m\n",
      "         5         153.1549           17.67m\n",
      "        38          47.7585           17.89m\n",
      "        12          64.3654           24.12m\n",
      "         3         520.8547           17.16m\n",
      "        33          47.7600           83.94m\n",
      "         9         126.0119           51.69m\n",
      "[CV 3/5; 20/36] END learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=100;, score=-56.719 total time= 5.1min\n",
      "[CV 4/5; 21/36] START learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "         6         140.1924           18.95m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        14          52.4702           24.72m\n",
      "         8          99.9438           56.46m\n",
      "         4         237.2428           18.16m\n",
      "        12          58.6873           55.68m\n",
      "         1        2529.1582           13.66m\n",
      "         6          85.2221           20.52m\n",
      "         2        1026.6065           13.58m\n",
      "        13          62.9341           24.29m\n",
      "         5         145.7874           18.59m\n",
      "         7          98.7067           20.45m\n",
      "[CV 1/5; 21/36] END learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=50;, score=-120.896 total time= 3.3min\n",
      "[CV 5/5; 21/36] START learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        15          50.2429           24.14m\n",
      "        39          47.7584           16.33m\n",
      "         3         481.6253           14.30m\n",
      "        34          47.7594           82.67m\n",
      "         1        3218.3273           14.44m\n",
      "         9          84.7570           58.15m\n",
      "        13          54.6155           56.63m\n",
      "         7          52.3741           22.45m\n",
      "         2        1304.9565           15.12m\n",
      "         6         110.2385           20.34m\n",
      "[CV 3/5; 21/36] END learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=50;, score=-52.990 total time= 2.8min\n",
      "[CV 1/5; 22/36] START learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "         4         274.4382           16.16m\n",
      "        14          61.4293           23.99m\n",
      "[CV 4/5; 19/36] END learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=50;, score=-95.835 total time= 9.3min\n",
      "[CV 2/5; 22/36] START learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "        16          47.9878           23.82m\n",
      "         1        3151.0297           29.28m\n",
      "         3         528.0072           15.96m\n",
      "         1        1898.8355           35.16m\n",
      "        10          73.8145           58.90m\n",
      "         5         181.9101           17.58m\n",
      "        35          47.7590           80.91m\n",
      "        14          52.4702           57.26m\n",
      "         2        1071.1872           29.95m\n",
      "         2         983.1547           34.41m\n",
      "         4         231.3085           16.56m\n",
      "         8          39.3738           24.44m\n",
      "         3         542.4506           29.41m\n",
      "        40          47.7583           14.84m\n",
      "        17          47.1018           23.34m\n",
      "         3         695.9891           35.89m\n",
      "        11          66.9623           59.62m\n",
      "         6         102.1394           19.61m\n",
      "[CV 4/5; 21/36] END learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=50;, score=-92.102 total time= 2.7min\n",
      "[CV 3/5; 22/36] START learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        15          50.2429           56.91m\n",
      "         5         141.7017           17.69m\n",
      "         4         249.1241           34.64m\n",
      "         1        2999.1154           27.70m\n",
      "         4         363.5572           38.01m\n",
      "        36          47.7588           79.46m\n",
      "         2        1102.7196           29.34m\n",
      "        18          46.3661           22.82m\n",
      "         9          33.9674           25.83m\n",
      "         5         194.1994           39.61m\n",
      "        12          64.3654           60.34m\n",
      "        16          47.9878           57.18m\n",
      "         6         111.6540           20.65m\n",
      "[CV 5/5; 21/36] END learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=50;, score=-56.515 total time= 2.8min\n",
      "[CV 4/5; 22/36] START learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        41          47.7581           13.29m\n",
      "         5         153.1549           41.78m\n",
      "         3         520.8547           30.12m\n",
      "         1        2529.1582           29.92m\n",
      "         4         237.2428           33.61m\n",
      "        19          45.9566           22.23m\n",
      "         6         140.1924           43.45m\n",
      "         2        1026.6065           30.89m\n",
      "        17          47.1018           57.25m\n",
      "        37          47.7587           78.31m\n",
      "        13          62.9341           61.13m\n",
      "        10          31.1641           27.20m\n",
      "[CV 2/5; 21/36] END learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=50;, score=-104.115 total time= 6.8min\n",
      "[CV 5/5; 22/36] START learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         5         145.7874           34.97m\n",
      "         3         481.6253           31.84m\n",
      "         6          85.2221           49.77m\n",
      "         1        3218.3273           26.90m\n",
      "        20          45.6213           21.52m\n",
      "        42          47.7580           11.78m\n",
      "         7          98.7067           46.97m\n",
      "[CV 1/5; 22/36] END learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=100;, score=-120.896 total time= 3.5min\n",
      "[CV 1/5; 23/36] START learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         4         274.4382           34.34m\n",
      "         2        1304.9565           28.73m\n",
      "        18          46.3661           57.09m\n",
      "         6         110.2385           39.65m\n",
      "[CV 3/5; 22/36] END learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=100;, score=-52.990 total time= 2.5min\n",
      "[CV 2/5; 23/36] START learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "        14          61.4293           60.99m\n",
      "[CV 4/5; 20/36] END learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=100;, score=-95.835 total time= 9.9min\n",
      "[CV 3/5; 23/36] START learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        2074.9474           14.63m\n",
      "         3         528.0072           31.37m\n",
      "         5         181.9101           36.95m\n",
      "         1        3164.5873           14.13m\n",
      "         1        3283.8811           14.57m\n",
      "         2        1323.3268           13.86m\n",
      "         7          52.3741           55.76m\n",
      "        21          45.4117           20.99m\n",
      "        38          47.7585           77.21m\n",
      "         2        1419.3053           13.97m\n",
      "         2        1314.8834           13.97m\n",
      "         4         231.3085           34.26m\n",
      "        19          45.9566           56.67m\n",
      "         3         942.4488           15.83m\n",
      "         3         781.0059           14.60m\n",
      "         3         898.3300           14.70m\n",
      "         6         102.1394           41.14m\n",
      "[CV 4/5; 22/36] END learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=100;, score=-92.102 total time= 2.6min\n",
      "[CV 4/5; 23/36] START learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        43          47.7580           10.29m\n",
      "         5         141.7017           37.75m\n",
      "         4         585.1552           17.13m\n",
      "         1        2750.6228           14.18m\n",
      "        22          45.1869           20.42m\n",
      "        20          45.6213           56.11m\n",
      "         4         461.8349           16.69m\n",
      "         4         457.3004           17.84m\n",
      "         2        1183.7099           14.04m\n",
      "         8          39.3738           62.01m\n",
      "        39          47.7584           75.90m\n",
      "         5         382.3629           18.80m\n",
      "         3         761.4974           14.97m\n",
      "        23          45.0278           19.67m\n",
      "[CV 1/5; 19/36] END learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=50;, score=-119.537 total time=16.8min\n",
      "[CV 5/5; 23/36] START learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         5         336.1345           18.37m\n",
      "         6         111.6540           44.56m\n",
      "[CV 5/5; 22/36] END learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=100;, score=-56.515 total time= 2.9min\n",
      "[CV 1/5; 24/36] START learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         5         265.3560           20.28m\n",
      "         1        3357.9223           15.22m\n",
      "        21          45.4117           56.06m\n",
      "         1        2074.9474           28.49m\n",
      "         4         483.6098           16.20m\n",
      "        44          47.7579            8.81m\n",
      "         2        1414.0441           15.04m\n",
      "         2        1323.3268           28.11m\n",
      "         6         287.4835           21.49m\n",
      "         6         270.3812           20.41m\n",
      "[CV 3/5; 23/36] END learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=50;, score=-55.483 total time= 2.8min\n",
      "[CV 2/5; 24/36] START learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         9          33.9674           66.91m\n",
      "         3         623.9873           15.27m\n",
      "         5         259.8126           18.47m\n",
      "         6         197.3085           22.38m\n",
      "[CV 2/5; 23/36] END learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=50;, score=-104.026 total time= 3.1min\n",
      "[CV 3/5; 24/36] START learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         3         942.4488           32.76m\n",
      "         1        3283.8811           29.73m\n",
      "        40          47.7583           75.04m\n",
      "        22          45.1869           55.98m\n",
      "         1        3164.5873           29.20m\n",
      "         2        1419.3053           29.28m\n",
      "         4         328.1562           16.74m\n",
      "         4         585.1552           36.30m\n",
      "         6         166.6783           19.14m\n",
      "         2        1314.8834           29.11m\n",
      "         7         184.2993           24.40m\n",
      "        45          47.7579            7.33m\n",
      "         3         898.3300           32.49m\n",
      "        23          45.0278           55.23m\n",
      "[CV 1/5; 20/36] END learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=100;, score=-119.537 total time=16.5min\n",
      "[CV 4/5; 24/36] START learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         3         781.0059           30.55m\n",
      "         5         199.9972           18.45m\n",
      "        10          31.1641           71.70m\n",
      "[CV 2/5; 22/36] END learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=100;, score=-104.115 total time= 8.0min\n",
      "[CV 5/5; 24/36] START learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         5         382.3629           40.28m\n",
      "         1        2750.6228           27.38m\n",
      "         4         457.3004           38.63m\n",
      "        41          47.7581           73.77m\n",
      "         2        1183.7099           28.53m\n",
      "         4         461.8349           34.97m\n",
      "         7         115.4553           21.75m\n",
      "         1        3357.9223           34.69m\n",
      "         8         145.3588           26.74m\n",
      "         6         126.8471           20.73m\n",
      "[CV 5/5; 23/36] END learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=50;, score=-55.165 total time= 2.8min\n",
      "[CV 1/5; 25/36] START learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         3         761.4974           31.06m\n",
      "         2        1414.0441           37.13m\n",
      "         6         287.4835           45.77m\n",
      "        46          47.7578            5.85m\n",
      "         5         336.1345           38.99m\n",
      "         1        3678.2047           13.29m\n",
      "         5         265.3560           44.42m\n",
      "         4         483.6098           33.84m\n",
      "         3         623.9873           37.87m\n",
      "         2        3188.5482           12.79m\n",
      "         8         100.7765           24.20m\n",
      "         3        2708.2366           12.81m\n",
      "         9         106.0810           27.80m\n",
      "        42          47.7580           72.72m\n",
      "         6         270.3812           44.14m\n",
      "[CV 3/5; 24/36] END learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=100;, score=-55.483 total time= 2.8min\n",
      "[CV 2/5; 25/36] START learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         4        2382.8239           12.40m\n",
      "         4         328.1562           42.35m\n",
      "         5         259.8126           39.17m\n",
      "         7         184.2993           52.14m\n",
      "         6         197.3085           50.54m\n",
      "[CV 2/5; 24/36] END learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=100;, score=-104.026 total time= 3.2min\n",
      "[CV 3/5; 25/36] START learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        8408.1251           12.44m\n",
      "         5        2071.3229           11.96m\n",
      "         1        8189.4348           14.13m\n",
      "         2        6962.6814           12.28m\n",
      "         6        1817.6710           11.74m\n",
      "        47          47.7578            4.38m\n",
      "         6         166.6783           40.94m\n",
      "         2        6761.2518           13.25m\n",
      "         3        5744.2445           11.65m\n",
      "         5         199.9972           47.50m\n",
      "         7        1596.7325           11.32m\n",
      "         9          89.0284           26.21m\n",
      "        10          86.2038           28.61m\n",
      "         3        5601.0634           13.37m\n",
      "         4        4757.9729           11.53m\n",
      "         8        1418.7850           11.05m\n",
      "         8         145.3588           57.28m\n",
      "         5        3957.4973           11.35m\n",
      "         4        4652.7815           13.20m\n",
      "         9        1261.3735           10.77m\n",
      "        43          47.7580           71.63m\n",
      "         6        3276.6430           11.08m\n",
      "        10        1121.8709           10.44m\n",
      "         5        3884.5572           12.83m\n",
      "         7         115.4553           47.05m\n",
      "         6         126.8471           54.19m\n",
      "[CV 5/5; 24/36] END learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=100;, score=-55.165 total time= 3.5min\n",
      "[CV 4/5; 25/36] START learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "         7        2739.4357           10.82m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        48          47.7576            2.91m\n",
      "        11         993.5020           10.11m\n",
      "         6        3261.3093           12.52m\n",
      "[CV 3/5; 25/36] END learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=50;, score=-60.794 total time= 1.7min\n",
      "[CV 5/5; 25/36] START learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        10          78.2233           27.72m\n",
      "         8        2285.8084           10.63m\n",
      "        12         887.7798            9.85m\n",
      "         1        6927.7363           16.15m\n",
      "         1        8616.6103           13.74m\n",
      "        11          78.6431           29.44m\n",
      "         9         106.0810           60.34m\n",
      "         9        1918.4960           10.41m\n",
      "        13         800.8066            9.55m\n",
      "         2        5718.6767           15.68m\n",
      "         2        7114.8032           14.19m\n",
      "        10        1618.2903           10.14m\n",
      "        14         729.0103            9.23m\n",
      "[CV 1/5; 25/36] END learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=50;, score=-119.804 total time= 3.6min\n",
      "[CV 1/5; 26/36] START learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         8         100.7765           53.21m\n",
      "        44          47.7579           70.58m\n",
      "         3        4733.4902           15.47m\n",
      "         3        5897.3429           13.89m\n",
      "        11        1371.6429            9.84m\n",
      "         1        3678.2047           25.96m\n",
      "         4        4907.1446           13.61m\n",
      "        12        1160.6488            9.68m\n",
      "         4        3934.6482           15.25m\n",
      "         2        3188.5482           26.36m\n",
      "        10          86.2038           63.20m\n",
      "        11          65.2759           28.88m\n",
      "        49          47.7575            1.46m\n",
      "         5        4107.4348           13.17m\n",
      "        13         990.8567            9.48m\n",
      "        12          74.6139           30.15m\n",
      "         3        2708.2366           26.11m\n",
      "         5        3297.9991           14.81m\n",
      "        14         850.7604            9.22m\n",
      "         6        3461.1407           12.86m\n",
      "[CV 5/5; 25/36] END learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=50;, score=-54.289 total time= 1.8min\n",
      "[CV 2/5; 26/36] START learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         4        2382.8239           25.96m\n",
      "         6        2756.3233           14.35m\n",
      "         9          89.0284           58.61m\n",
      "        15         736.4074            8.98m\n",
      "         1        8408.1251           26.31m\n",
      "         5        2071.3229           26.21m\n",
      "         7        2335.8925           13.87m\n",
      "        16         614.0745            8.79m\n",
      "         2        6962.6814           26.12m\n",
      "        45          47.7579           69.52m\n",
      "         6        1817.6710           26.56m\n",
      "        12          61.7194           29.19m\n",
      "         8        1984.5456           13.58m\n",
      "        11          78.6431           66.08m\n",
      "        13          62.4276           29.98m\n",
      "         3        5744.2445           24.96m\n",
      "        17         515.3580            8.57m\n",
      "         7        1596.7325           26.14m\n",
      "         4        4757.9729           24.88m\n",
      "         9        1698.9148           13.23m\n",
      "[CV 4/5; 25/36] END learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=50;, score=-91.496 total time= 2.9min\n",
      "[CV 3/5; 26/36] START learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        18         436.9547            8.35m\n",
      "         8        1418.7850           25.64m\n",
      "         5        3957.4973           24.49m\n",
      "        50          47.7575            0.00s\n",
      "[CV 5/5; 11/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=50;, score=-59.005 total time=73.1min\n",
      "[CV 4/5; 26/36] START learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "        10          78.2233           62.16m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        19         370.9558            8.11m\n",
      "         1        8189.4348           32.88m\n",
      "         9        1261.3735           25.20m\n",
      "         6        3276.6430           24.25m\n",
      "        20         319.2061            7.88m\n",
      "         1        6927.7363           30.21m\n",
      "        10        1121.8709           24.76m\n",
      "         2        6761.2518           32.29m\n",
      "        14          58.8290           29.59m\n",
      "        13          58.0978           29.32m\n",
      "         7        2739.4357           23.75m\n",
      "        46          47.7578           68.28m\n",
      "         2        5718.6767           27.66m\n",
      "        21         275.3018            7.65m\n",
      "        11         993.5020           24.54m\n",
      "        12          74.6139           69.09m\n",
      "         3        5601.0634           31.56m\n",
      "         8        2285.8084           23.39m\n",
      "         3        4733.4902           26.88m\n",
      "        22         241.1247            7.45m\n",
      "        12         887.7798           24.26m\n",
      "         9        1918.4960           23.21m\n",
      "         4        4652.7815           31.18m\n",
      "         4        3934.6482           26.22m\n",
      "        13         800.8066           23.84m\n",
      "        23         211.7314            7.24m\n",
      "        11          65.2759           65.68m\n",
      "        10        1618.2903           22.99m\n",
      "         5        3884.5572           30.74m\n",
      "         5        3297.9991           26.06m\n",
      "        14         729.0103           23.37m\n",
      "[CV 1/5; 26/36] END learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=100;, score=-119.804 total time= 3.8min\n",
      "[CV 5/5; 26/36] START learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        11        1371.6429           22.54m\n",
      "        24         187.6444            7.02m\n",
      "        15          50.4078           29.51m\n",
      "        14          55.1786           29.32m\n",
      "         6        2756.3233           25.77m\n",
      "        13          62.4276           69.72m\n",
      "         6        3261.3093           30.24m\n",
      "[CV 3/5; 26/36] END learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=100;, score=-60.794 total time= 1.9min\n",
      "[CV 1/5; 27/36] START learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        8616.6103           25.40m\n",
      "        12        1160.6488           22.31m\n",
      "        25         166.8737            6.78m\n",
      "         7        2335.8925           25.69m\n",
      "         2        7114.8032           26.01m\n",
      "        13         990.8567           22.12m\n",
      "         1        3722.8125           15.96m\n",
      "        47          47.7578           67.28m\n",
      "        26         149.9980            6.56m\n",
      "         8        1984.5456           25.48m\n",
      "        14         850.7604           21.87m\n",
      "         3        5897.3429           26.92m\n",
      "        12          61.7194           67.32m\n",
      "         2        3269.0648           15.61m\n",
      "        27         135.7866            6.33m\n",
      "         9        1698.9148           25.09m\n",
      "[CV 4/5; 26/36] END learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=100;, score=-91.496 total time= 2.5min\n",
      "[CV 2/5; 27/36] START learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        15         736.4074           21.68m\n",
      "         4        4907.1446           27.50m\n",
      "        15          52.4060           29.05m\n",
      "         3        2895.3356           14.97m\n",
      "        14          58.8290           70.02m\n",
      "         1        8441.7723           13.55m\n",
      "        28         123.9479            6.08m\n",
      "        16         614.0745           21.58m\n",
      "        16          49.5365           29.56m\n",
      "         5        4107.4348           27.24m\n",
      "         4        2515.2133           14.66m\n",
      "         2        7023.9728           13.08m\n",
      "        17         515.3580           21.49m\n",
      "        29         113.7703            5.83m\n",
      "         6        3461.1407           26.90m\n",
      "[CV 5/5; 26/36] END learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=100;, score=-54.289 total time= 1.7min\n",
      "[CV 3/5; 27/36] START learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        48          47.7576           65.93m\n",
      "         5        2206.4071           14.27m\n",
      "         3        5862.8493           12.40m\n",
      "        18         436.9547           21.33m\n",
      "        13          58.0978           68.70m\n",
      "        30         103.8732            5.59m\n",
      "         1        8231.3468           13.74m\n",
      "         4        4916.1690           12.30m\n",
      "         6        1949.7140           13.92m\n",
      "        19         370.9558           21.10m\n",
      "        16          51.6455           28.78m\n",
      "         2        6837.0013           13.79m\n",
      "        15          50.4078           71.07m\n",
      "        31          95.7589            5.36m\n",
      "         5        4148.7259           11.95m\n",
      "         7        1757.0425           13.63m\n",
      "        20         319.2061           20.91m\n",
      "         3        5707.4672           13.45m\n",
      "         6        3549.9379           11.56m\n",
      "        32          89.0574            5.11m\n",
      "        21         275.3018           20.78m\n",
      "         8        1554.3416           13.29m\n",
      "        17          48.5926           29.74m\n",
      "         4        4772.3381           13.03m\n",
      "         7        3035.5929           11.22m\n",
      "        33          83.4765            4.86m\n",
      "        14          55.1786           69.96m\n",
      "        22         241.1247           20.69m\n",
      "         9        1399.0968           12.98m\n",
      "         8        2607.2239           10.91m\n",
      "         5        4011.9374           12.81m\n",
      "        49          47.7575           64.98m\n",
      "        23         211.7314           20.57m\n",
      "         9        2255.7883           10.68m\n",
      "        10        1289.0091           12.67m\n",
      "         6        3394.1474           12.41m\n",
      "[CV 3/5; 27/36] END learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=50;, score=-62.295 total time= 1.7min\n",
      "[CV 4/5; 27/36] START learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        34          78.6210            4.62m\n",
      "        17          51.1851           28.80m\n",
      "        16          49.5365           72.61m\n",
      "        10        1984.9991           10.45m\n",
      "        18          46.5440           28.83m\n",
      "        24         187.6444           20.47m\n",
      "         1        6983.8296           14.15m\n",
      "        11        1177.8558           12.33m\n",
      "        35          73.3736            4.34m\n",
      "        11        1745.3941           10.23m\n",
      "         2        5819.4321           13.67m\n",
      "        25         166.8737           20.35m\n",
      "        12        1075.9643           12.08m\n",
      "        36          69.1610            4.08m\n",
      "        15          52.4060           70.55m\n",
      "        12        1546.1651            9.96m\n",
      "         3        4854.7519           13.15m\n",
      "        26         149.9980           20.21m\n",
      "        13         996.3603           11.75m\n",
      "        37          65.6524            3.80m\n",
      "        13        1381.5015            9.69m\n",
      "         4        4072.1325           12.82m\n",
      "        18          50.9786           28.18m\n",
      "        27         135.7866           20.07m\n",
      "        19          45.9914           28.11m\n",
      "        14         903.5926           11.44m\n",
      "        14        1257.2697            9.41m\n",
      "        38          61.4536            3.53m\n",
      "[CV 2/5; 25/36] END learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=50;, score=-102.722 total time=11.2min\n",
      "[CV 5/5; 27/36] START learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "         5        3432.5272           12.34m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        28         123.9479           19.89m\n",
      "        15        1145.3208            9.15m\n",
      "        15         833.0397           11.13m\n",
      "        17          48.5926           74.44m\n",
      "         6        2903.4154           11.97m\n",
      "         1        8651.9624           13.08m\n",
      "        50          47.7575           64.28m\n",
      "        16          51.6455           71.13m\n",
      "        16        1023.7603            8.95m\n",
      "        29         113.7703           19.71m\n",
      "         7        2479.9596           11.50m\n",
      "         2        7178.7713           12.70m\n",
      "        16         769.4663           10.83m\n",
      "        19          50.5804           27.41m\n",
      "         8        2156.1935           11.14m\n",
      "        17         899.8821            8.74m\n",
      "         3        5979.9404           12.57m\n",
      "        30         103.8732           19.56m\n",
      "        17         722.5891           10.57m\n",
      "         9        1857.5846           10.99m\n",
      "[CV 4/5; 27/36] END learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=50;, score=-93.392 total time= 2.4min\n",
      "[CV 1/5; 28/36] START learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        18         796.6684            8.53m\n",
      "         4        5008.8548           12.44m\n",
      "        20          45.4169           27.71m\n",
      "[CV 1/5; 23/36] END learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=50;, score=-118.101 total time=18.5min\n",
      "[CV 2/5; 28/36] START learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        18          46.5440           73.55m\n",
      "        31          95.7589           19.48m\n",
      "        18         673.1884           10.35m\n",
      "        19         720.3448            8.26m\n",
      "         5        4216.7209           12.17m\n",
      "         1        3722.8125           32.32m\n",
      "         1        8441.7723           25.43m\n",
      "        32          89.0574           19.29m\n",
      "         6        3575.1954           11.92m\n",
      "[CV 5/5; 27/36] END learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=50;, score=-54.147 total time= 1.6min\n",
      "[CV 3/5; 28/36] START learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "        19         610.5709           10.09m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        20         622.4787            8.06m\n",
      "        17          51.1851           72.12m\n",
      "         2        7023.9728           24.90m\n",
      "         2        3269.0648           29.97m\n",
      "        51          47.7575           63.06m\n",
      "        33          83.4765           19.12m\n",
      "         1        8231.3468           27.50m\n",
      "         3        2895.3356           28.34m\n",
      "         3        5862.8493           25.98m\n",
      "        21         539.5893            7.86m\n",
      "        20          50.3672           27.08m\n",
      "        20         552.2858            9.79m\n",
      "        19          45.9914           73.07m\n",
      "         2        6837.0013           26.84m\n",
      "         4        2515.2133           28.17m\n",
      "         4        4916.1690           26.20m\n",
      "        34          78.6210           19.01m\n",
      "        22         470.7610            7.64m\n",
      "        21         500.1775            9.59m\n",
      "         3        5707.4672           26.28m\n",
      "         5        4148.7259           25.69m\n",
      "         5        2206.4071           28.01m\n",
      "        35          73.3736           18.79m\n",
      "        23         433.0622            7.39m\n",
      "        18          50.9786           71.84m\n",
      "         4        4772.3381           25.88m\n",
      "        22         454.6731            9.28m\n",
      "         6        1949.7140           26.89m\n",
      "         6        3549.9379           25.84m\n",
      "        24         382.3145            7.15m\n",
      "[CV 2/5; 27/36] END learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=50;, score=-98.370 total time= 6.6min\n",
      "[CV 4/5; 28/36] START learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        36          69.1610           18.60m\n",
      "         5        4011.9374           25.37m\n",
      "        52          47.7575           61.76m\n",
      "         7        1757.0425           26.27m\n",
      "         7        3035.5929           25.81m\n",
      "         1        6983.8296           25.49m\n",
      "        23         421.1942            9.05m\n",
      "        21          50.2034           26.62m\n",
      "         6        3394.1474           24.68m\n",
      "[CV 3/5; 28/36] END learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=100;, score=-62.295 total time= 1.6min\n",
      "[CV 5/5; 28/36] START learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        37          65.6524           18.36m\n",
      "         8        1554.3416           25.66m\n",
      "         8        2607.2239           25.25m\n",
      "        20          45.4169           73.52m\n",
      "[CV 1/5; 24/36] END learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=100;, score=-118.101 total time=18.4min\n",
      "[CV 1/5; 29/36] START learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2        5819.4321           25.46m\n",
      "         1        8651.9624           25.50m\n",
      "        24         378.6394            8.79m\n",
      "        19          50.5804           71.20m\n",
      "        38          61.4536           18.17m\n",
      "[CV 2/5; 26/36] END learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=100;, score=-102.722 total time=11.1min\n",
      "[CV 2/5; 29/36] START learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "         9        1399.0968           25.63m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         9        2255.7883           24.74m\n",
      "         3        4854.7519           24.76m\n",
      "         1        3678.1518           13.45m\n",
      "         2        7178.7713           24.92m\n",
      "        10        1984.9991           24.37m\n",
      "         1        8408.0759           14.23m\n",
      "        10        1289.0091           25.45m\n",
      "         4        4072.1325           24.51m\n",
      "         2        3188.4385           13.24m\n",
      "         3        5979.9404           24.76m\n",
      "        25         341.4731            8.56m\n",
      "[CV 1/5; 27/36] END learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=50;, score=-120.830 total time= 8.6min\n",
      "[CV 3/5; 29/36] START learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        11        1745.3941           24.07m\n",
      "        11        1177.8558           25.04m\n",
      "         2        6962.6044           13.55m\n",
      "         5        3432.5272           24.40m\n",
      "         3        2708.1190           13.18m\n",
      "         4        5008.8548           24.78m\n",
      "        12        1546.1651           23.59m\n",
      "        22          50.0930           26.03m\n",
      "[CV 4/5; 23/36] END learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=50;, score=-92.745 total time=20.5min\n",
      "[CV 4/5; 29/36] START learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "         1        8189.3852           16.89m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         6        2903.4154           23.93m\n",
      "         3        5744.1505           12.85m\n",
      "        12        1075.9643           24.64m\n",
      "         4        2382.7087           12.64m\n",
      "         5        4216.7209           24.37m\n",
      "        13        1381.5015           23.21m\n",
      "        53          47.7574           60.75m\n",
      "         7        2479.9596           23.34m\n",
      "        13         996.3603           24.17m\n",
      "         4        4757.8182           12.40m\n",
      "         1        6927.6660           15.09m\n",
      "         2        6761.1213           16.48m\n",
      "        20          50.3672           71.47m\n",
      "         6        3575.1954           24.06m\n",
      "[CV 5/5; 28/36] END learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=100;, score=-54.147 total time= 1.5min\n",
      "[CV 5/5; 29/36] START learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         5        2071.1958           12.32m\n",
      "        14        1257.2697           22.80m\n",
      "         8        2156.1935           22.85m\n",
      "        14         903.5926           23.73m\n",
      "         5        3957.3443           12.14m\n",
      "         2        5718.5682           14.59m\n",
      "         6        1817.5374           11.93m\n",
      "         1        8616.5442           12.99m\n",
      "         3        5600.9108           16.01m\n",
      "        15        1145.3208           22.41m\n",
      "         9        1857.5846           22.59m\n",
      "[CV 4/5; 28/36] END learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=100;, score=-93.392 total time= 2.2min\n",
      "[CV 1/5; 30/36] START learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        15         833.0397           23.31m\n",
      "         6        3276.4821           11.84m\n",
      "         3        4733.3064           13.75m\n",
      "         7        1596.6255           11.67m\n",
      "         2        7114.6980           12.88m\n",
      "        16        1023.7603           22.17m\n",
      "         1        3678.1518           26.25m\n",
      "         4        4652.6375           15.76m\n",
      "        16         769.4663           22.87m\n",
      "         4        3934.4319           13.12m\n",
      "         7        2739.2829           11.88m\n",
      "         3        5897.1994           12.56m\n",
      "         8        1418.7064           11.38m\n",
      "        17         899.8821           21.93m\n",
      "         2        3188.4385           26.11m\n",
      "        17         722.5891           22.58m\n",
      "         5        3884.3998           15.37m\n",
      "         5        3297.8092           12.61m\n",
      "         9        1261.2957           11.03m\n",
      "         4        4906.9905           12.27m\n",
      "         8        2285.6324           11.84m\n",
      "        54          47.7574           59.42m\n",
      "        21          50.2034           71.55m\n",
      "        18         796.6684           21.70m\n",
      "         3        2708.1190           26.14m\n",
      "        18         673.1884           22.41m\n",
      "         6        2756.1528           12.27m\n",
      "         6        3261.1475           14.92m\n",
      "[CV 3/5; 29/36] END learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=50;, score=-61.041 total time= 2.1min\n",
      "[CV 2/5; 30/36] START learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        10        1121.7915           10.76m\n",
      "         5        4107.2732           11.97m\n",
      "         9        1918.3388           11.63m\n",
      "        19         720.3448           21.37m\n",
      "         4        2382.7087           25.68m\n",
      "        19         610.5709           22.16m\n",
      "        11         993.4276           10.48m\n",
      "         7        2335.7048           12.14m\n",
      "         6        3460.9726           11.78m\n",
      "[CV 5/5; 29/36] END learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=50;, score=-54.331 total time= 1.6min\n",
      "[CV 3/5; 30/36] START learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        8408.0759           31.54m\n",
      "        10        1618.1704           11.29m\n",
      "        20         622.4787           21.21m\n",
      "         5        2071.1958           25.53m\n",
      "        20         552.2858           21.85m\n",
      "        12         887.7010           10.27m\n",
      "         8        1984.3418           11.95m\n",
      "         1        8189.3852           28.46m\n",
      "        11        1371.5615           10.95m\n",
      "         2        6962.6044           31.29m\n",
      "         6        1817.5374           25.20m\n",
      "        21         539.5893           21.09m\n",
      "        21         500.1775           21.78m\n",
      "        13         800.7365           10.00m\n",
      "         9        1698.7225           11.64m\n",
      "[CV 4/5; 29/36] END learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=50;, score=-91.513 total time= 2.6min\n",
      "[CV 4/5; 30/36] START learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "         2        6761.1213           27.52m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        12        1160.5530           10.71m\n",
      "         7        1596.6255           24.86m\n",
      "        22          50.0930           71.44m\n",
      "[CV 4/5; 24/36] END learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=100;, score=-92.745 total time=20.2min\n",
      "[CV 5/5; 30/36] START learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "         3        5744.1505           30.69m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        22         470.7610           20.93m\n",
      "        22         454.6731           21.50m\n",
      "        55          47.7573           58.11m\n",
      "        14         728.9682            9.73m\n",
      "[CV 1/5; 29/36] END learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=50;, score=-119.901 total time= 3.8min\n",
      "[CV 1/5; 31/36] START learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         3        5600.9108           26.89m\n",
      "         1        6927.6660           29.13m\n",
      "        13         990.5816           10.44m\n",
      "         8        1418.7064           24.72m\n",
      "         1        8616.5442           25.57m\n",
      "         4        4757.8182           29.93m\n",
      "        23         433.0622           20.69m\n",
      "        23         421.1942           21.39m\n",
      "         1        3722.7668           12.90m\n",
      "         4        4652.6375           26.65m\n",
      "         2        5718.5682           28.32m\n",
      "         9        1261.2957           24.30m\n",
      "        14         850.3758           10.13m\n",
      "         2        7114.6980           25.25m\n",
      "         5        3957.3443           29.56m\n",
      "        24         382.3145           20.54m\n",
      "[CV 2/5; 28/36] END learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=100;, score=-98.370 total time= 6.5min\n",
      "[CV 2/5; 31/36] START learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2        3269.0083           12.62m\n",
      "        24         378.6394           21.24m\n",
      "         5        3884.3998           26.42m\n",
      "        10        1121.7915           24.10m\n",
      "        15         735.9398            9.84m\n",
      "         3        5897.1994           25.12m\n",
      "         3        4733.3064           28.96m\n",
      "         1        8441.7316           11.73m\n",
      "         6        3276.4821           29.00m\n",
      "         3        2895.2048           12.38m\n",
      "         6        3261.1475           26.02m\n",
      "[CV 3/5; 30/36] END learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=100;, score=-61.041 total time= 1.7min\n",
      "[CV 3/5; 31/36] START learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        11         993.4276           23.83m\n",
      "         4        4906.9905           25.08m\n",
      "        16         613.5647            9.60m\n",
      "        25         341.4731           21.21m\n",
      "[CV 1/5; 28/36] END learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=100;, score=-120.830 total time= 7.1min\n",
      "[CV 4/5; 31/36] START learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "         2        7023.8709           11.66m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         4        3934.4319           28.90m\n",
      "         7        2739.2829           28.71m\n",
      "         4        2515.0882           12.08m\n",
      "         1        8231.3047           13.27m\n",
      "        12         887.7010           23.61m\n",
      "         5        4107.2732           24.92m\n",
      "         3        5862.7433           11.67m\n",
      "        17         514.8692            9.35m\n",
      "         5        3297.8092           28.04m\n",
      "         1        6983.7701           14.70m\n",
      "         5        2206.2599           11.70m\n",
      "        56          47.7573           56.88m\n",
      "         8        2285.6324           28.46m\n",
      "         2        6836.9043           12.57m\n",
      "        13         800.7365           23.32m\n",
      "         6        3460.9726           24.86m\n",
      "[CV 5/5; 30/36] END learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=100;, score=-54.331 total time= 1.6min\n",
      "[CV 5/5; 31/36] START learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         4        4916.0451           11.79m\n",
      "         2        5819.3226           13.35m\n",
      "         6        2756.1528           27.64m\n",
      "        18         436.4913            9.09m\n",
      "         6        1949.5390           11.51m\n",
      "         3        5707.3465           12.15m\n",
      "         9        1918.3388           28.07m\n",
      "        14         728.9682           23.03m\n",
      "[CV 1/5; 30/36] END learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=100;, score=-119.901 total time= 3.8min\n",
      "[CV 1/5; 32/36] START learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        8651.9082           12.49m\n",
      "         5        4148.6176           11.73m\n",
      "         3        4854.6067           12.72m\n",
      "         7        2335.7048           27.48m\n",
      "        19         370.5145            8.85m\n",
      "         7        1756.8524           11.24m\n",
      "         4        4772.1957           11.95m\n",
      "         1        3722.7668           25.26m\n",
      "         2        7178.6876           12.23m\n",
      "        10        1618.1704           27.69m\n",
      "         6        3549.8240           11.42m\n",
      "         4        4072.0345           12.47m\n",
      "         8        1554.2109           10.96m\n",
      "         8        1984.3418           26.97m\n",
      "        20         318.8184            8.60m\n",
      "         5        4011.8035           11.65m\n",
      "         2        3269.0083           25.03m\n",
      "         3        5979.8153           12.06m\n",
      "        11        1371.5615           27.35m\n",
      "         7        3035.4671           11.35m\n",
      "         5        3432.3510           12.30m\n",
      "         9        1398.9513           10.70m\n",
      "         9        1698.7225           26.40m\n",
      "[CV 4/5; 30/36] END learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=100;, score=-91.513 total time= 2.6min\n",
      "[CV 2/5; 32/36] START learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         6        3394.0194           11.45m\n",
      "[CV 3/5; 31/36] END learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=50;, score=-61.680 total time= 1.6min\n",
      "[CV 3/5; 32/36] START learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        21         274.8710            8.35m\n",
      "         3        2895.2048           25.30m\n",
      "         4        5008.7268           12.01m\n",
      "         8        2607.0935           10.99m\n",
      "        57          47.7573           55.55m\n",
      "        12        1160.5530           27.21m\n",
      "        10        1288.8388           10.44m\n",
      "         1        8441.7316           24.67m\n",
      "         6        2903.2905           12.32m\n",
      "         1        8231.3047           26.07m\n",
      "         4        2515.0882           25.03m\n",
      "         5        4216.5482           11.83m\n",
      "        22         240.6487            8.12m\n",
      "         9        2255.7410           10.70m\n",
      "         2        7023.8709           24.50m\n",
      "        11        1177.7173           10.22m\n",
      "        13         990.5816           27.06m\n",
      "         7        2479.8335           12.17m\n",
      "         2        6836.9043           24.99m\n",
      "         5        2206.2599           24.41m\n",
      "         6        3575.0395           11.57m\n",
      "[CV 5/5; 31/36] END learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=50;, score=-54.151 total time= 1.6min\n",
      "[CV 4/5; 32/36] START learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        10        1984.9018           10.47m\n",
      "        23         211.1765            7.88m\n",
      "         3        5862.7433           24.17m\n",
      "        12        1075.8142            9.99m\n",
      "         3        5707.3465           25.23m\n",
      "         8        2156.0502           11.87m\n",
      "        14         850.3758           26.86m\n",
      "         6        1949.5390           24.63m\n",
      "         1        6983.7701           25.60m\n",
      "        11        1745.5941           10.25m\n",
      "         4        4916.0451           24.33m\n",
      "        13         996.3030            9.75m\n",
      "        24         187.0552            7.65m\n",
      "         4        4772.1957           25.20m\n",
      "         9        1857.4745           11.69m\n",
      "[CV 4/5; 31/36] END learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=50;, score=-93.507 total time= 2.6min\n",
      "[CV 5/5; 32/36] START learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "         7        1756.8524           24.38m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2        5819.3226           24.95m\n",
      "        15         735.9398           26.63m\n",
      "        12        1546.3362            9.95m\n",
      "         5        4148.6176           24.29m\n",
      "        14         902.8323            9.54m\n",
      "         5        4011.8035           24.79m\n",
      "         1        8651.9082           26.16m\n",
      "         3        4854.6067           24.93m\n",
      "         8        1554.2109           24.64m\n",
      "        25         166.2786            7.43m\n",
      "        13        1381.6260            9.75m\n",
      "         6        3549.8240           23.86m\n",
      "        16         613.5647           26.56m\n",
      "        15         832.4390            9.26m\n",
      "[CV 1/5; 31/36] END learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=50;, score=-120.879 total time= 4.0min\n",
      "[CV 1/5; 33/36] START learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        58          47.7573           54.35m\n",
      "         6        3394.0194           24.55m\n",
      "[CV 3/5; 32/36] END learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=100;, score=-61.680 total time= 1.6min\n",
      "[CV 2/5; 33/36] START learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         4        4072.0345           24.69m\n",
      "         2        7178.6876           26.94m\n",
      "         9        1398.9513           24.47m\n",
      "        14        1257.3930            9.51m\n",
      "         7        3035.4671           23.78m\n",
      "        26         149.4357            7.19m\n",
      "         1        3678.1516           12.77m\n",
      "         1        8408.0758           14.37m\n",
      "        17         514.8692           26.44m\n",
      "         5        3432.3510           24.38m\n",
      "         3        5979.8153           26.81m\n",
      "        10        1288.8388           24.06m\n",
      "         8        2607.0935           23.37m\n",
      "        15        1145.4949            9.26m\n",
      "         2        3188.4382           12.50m\n",
      "        27         135.2998            6.95m\n",
      "         6        2903.2905           24.08m\n",
      "         2        6962.6046           13.48m\n",
      "        11        1177.7173           23.79m\n",
      "         9        2255.7410           23.06m\n",
      "        18         436.4913           26.28m\n",
      "         4        5008.7268           27.10m\n",
      "        16        1023.9474            9.05m\n",
      "         3        2708.1184           12.28m\n",
      "         7        2479.8335           23.84m\n",
      "         3        5744.1508           12.95m\n",
      "        12        1075.8142           23.53m\n",
      "        28         123.5845            6.69m\n",
      "         5        4216.5482           26.61m\n",
      "        10        1984.9018           23.22m\n",
      "        17         900.1120            8.84m\n",
      "         4        2382.7052           11.90m\n",
      "        19         370.5145           26.17m\n",
      "         4        4757.8184           12.26m\n",
      "         8        2156.0502           23.65m\n",
      "        13         996.3030           23.22m\n",
      "         6        3575.0395           26.77m\n",
      "[CV 5/5; 32/36] END learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=100;, score=-54.151 total time= 1.7min\n",
      "[CV 3/5; 33/36] START learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        11        1745.5941           23.30m\n",
      "         5        2071.1913           11.82m\n",
      "        29         113.6451            6.45m\n",
      "        18         796.6991            8.65m\n",
      "         5        3957.3446           11.94m\n",
      "         9        1857.4745           23.46m\n",
      "[CV 4/5; 32/36] END learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=100;, score=-93.507 total time= 2.3min\n",
      "[CV 4/5; 33/36] START learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        59          47.7573           53.11m\n",
      "        20         318.8184           26.04m\n",
      "        14         902.8323           23.04m\n",
      "        12        1546.3362           23.08m\n",
      "         6        1817.5371           11.50m\n",
      "         1        8189.3851           14.94m\n",
      "         6        3276.4813           11.67m\n",
      "         1        6927.6655           13.37m\n",
      "        19         720.4138            8.45m\n",
      "        15         832.4390           22.75m\n",
      "[CV 1/5; 32/36] END learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=100;, score=-120.879 total time= 4.0min\n",
      "[CV 5/5; 33/36] START learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        30         103.7525            6.22m\n",
      "        21         274.8710           25.94m\n",
      "         7        1596.6248           11.21m\n",
      "         2        6761.1210           13.76m\n",
      "        13        1381.6260           23.23m\n",
      "         7        2739.2805           11.35m\n",
      "         2        5718.5673           12.66m\n",
      "        20         622.1874            8.23m\n",
      "         1        8616.5437           12.90m\n",
      "         8        1418.6854           11.01m\n",
      "         8        2285.6313           11.07m\n",
      "        14        1257.3930           23.02m\n",
      "         3        4733.3064           12.31m\n",
      "         3        5600.9104           14.03m\n",
      "        22         240.6487           25.92m\n",
      "        31          95.6700            6.01m\n",
      "         2        7114.6974           12.35m\n",
      "        60          47.7573           51.58m\n",
      "        21         538.8647            8.06m\n",
      "         9        1261.2728           10.71m\n",
      "         9        1918.3339           10.77m\n",
      "         4        3934.4400           12.08m\n",
      "         4        4652.6374           13.45m\n",
      "        15        1145.4949           22.99m\n",
      "         3        5897.1972           12.14m\n",
      "        10        1121.7712           10.46m\n",
      "        23         211.1765           25.85m\n",
      "        10        1618.1637           10.56m\n",
      "        32          88.9056            5.76m\n",
      "        22         469.8764            7.89m\n",
      "         5        3297.8120           11.80m\n",
      "         5        3884.3999           13.07m\n",
      "        16        1023.9474           22.97m\n",
      "         4        4906.9880           11.92m\n",
      "        11         993.4048           10.19m\n",
      "        11        1371.5545           10.31m\n",
      "         6        3261.1491           12.64m\n",
      "[CV 3/5; 33/36] END learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=50;, score=-60.777 total time= 1.7min\n",
      "[CV 1/5; 34/36] START learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         6        2756.1586           11.92m\n",
      "         5        4107.2732           11.72m\n",
      "        24         187.0552           25.81m\n",
      "        17         900.1120           22.98m\n",
      "        23         432.4886            7.77m\n",
      "        33          83.2835            5.50m\n",
      "        12         887.6800            9.92m\n",
      "        12        1160.5553           10.02m\n",
      "         1        3678.1516           25.70m\n",
      "         7        2335.7159           11.93m\n",
      "         6        3460.9730           11.59m\n",
      "[CV 5/5; 33/36] END learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=50;, score=-54.250 total time= 1.6min\n",
      "[CV 2/5; 34/36] START learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        61          47.7572           50.14m\n",
      "        13         800.7167            9.64m\n",
      "        18         796.6991           23.00m\n",
      "        24         381.7646            7.56m\n",
      "        13         990.5759            9.78m\n",
      "        25         166.2786           25.77m\n",
      "         2        3188.4382           25.10m\n",
      "        34          76.6783            5.23m\n",
      "[CV 2/5; 29/36] END learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=50;, score=-102.384 total time=11.1min\n",
      "[CV 3/5; 34/36] START learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        8408.0758           24.70m\n",
      "         8        1984.3523           11.62m\n",
      "        14         728.9529            9.39m\n",
      "        14         850.3620            9.53m\n",
      "         3        2708.1184           25.62m\n",
      "        19         720.4138           23.03m\n",
      "         1        8189.3851           28.06m\n",
      "         2        6962.6046           24.86m\n",
      "         9        1698.7343           11.29m\n",
      "[CV 4/5; 33/36] END learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=50;, score=-91.577 total time= 2.5min\n",
      "[CV 4/5; 34/36] START learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        25         339.9445            7.39m\n",
      "        26         149.4357           25.51m\n",
      "        15         671.2035            9.14m\n",
      "[CV 1/5; 33/36] END learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=50;, score=-120.022 total time= 3.9min\n",
      "[CV 5/5; 34/36] START learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        15         735.9282            9.32m\n",
      "         4        2382.7052           25.20m\n",
      "         3        5744.1508           24.38m\n",
      "         2        6761.1210           27.28m\n",
      "         1        6927.6655           26.79m\n",
      "        20         622.1874           23.08m\n",
      "         1        8616.5437           26.18m\n",
      "        27         135.2998           25.26m\n",
      "         5        2071.1913           25.17m\n",
      "         4        4757.8184           24.21m\n",
      "        26         303.1459            7.27m\n",
      "         3        5600.9104           26.73m\n",
      "        16         613.5542            9.26m\n",
      "         2        5718.5673           25.99m\n",
      "         2        7114.6974           25.41m\n",
      "        21         538.8647           23.24m\n",
      "         6        1817.5371           24.88m\n",
      "         5        3957.3446           24.20m\n",
      "        28         123.5845           25.00m\n",
      "         3        4733.3064           25.81m\n",
      "         4        4652.6374           26.69m\n",
      "         3        5897.1972           25.22m\n",
      "        17         514.8661            9.19m\n",
      "        62          47.7572           48.92m\n",
      "        27         273.5666            7.12m\n",
      "         7        1596.6248           24.58m\n",
      "         6        3276.4813           23.90m\n",
      "         4        3934.4400           25.45m\n",
      "         5        3884.3999           26.44m\n",
      "         4        4906.9880           24.90m\n",
      "        22         469.8764           23.32m\n",
      "        29         113.6451           24.73m\n",
      "         7        2739.2805           23.69m\n",
      "         8        1418.6854           24.29m\n",
      "        18         436.4804            9.04m\n",
      "         5        3297.8120           25.16m\n",
      "         6        3261.1491           26.11m\n",
      "[CV 3/5; 34/36] END learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=100;, score=-60.777 total time= 1.7min\n",
      "[CV 1/5; 35/36] START learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         5        4107.2732           24.90m\n",
      "        28         249.5722            6.93m\n",
      "         8        2285.6313           23.58m\n",
      "         9        1261.2728           24.13m\n",
      "        23         432.4886           23.50m\n",
      "        19         370.4890            8.83m\n",
      "        30         103.7525           24.61m\n",
      "         6        2756.1586           25.10m\n",
      "         6        3460.9730           24.99m\n",
      "[CV 5/5; 34/36] END learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=100;, score=-54.250 total time= 1.6min\n",
      "[CV 2/5; 35/36] START learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "         1        3722.7666           14.44m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         9        1918.3339           23.32m\n",
      "        10        1121.7712           24.16m\n",
      "         7        2335.7159           24.98m\n",
      "        29         229.5238            6.71m\n",
      "        24         381.7646           23.38m\n",
      "         1        8441.7315           12.65m\n",
      "         2        3269.0082           13.61m\n",
      "        20         318.7918            8.69m\n",
      "        10        1618.1637           23.07m\n",
      "        31          95.6700           24.56m\n",
      "        11         993.4048           24.04m\n",
      "         8        1984.3523           24.83m\n",
      "        63          47.7572           47.62m\n",
      "         2        7023.8705           12.32m\n",
      "         3        2895.2045           13.16m\n",
      "        25         339.9445           23.25m\n",
      "        11        1371.5545           22.88m\n",
      "        30         200.9952            6.46m\n",
      "        21         274.8486            8.58m\n",
      "        12         887.6800           23.99m\n",
      "         9        1698.7343           24.44m\n",
      "[CV 4/5; 34/36] END learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=100;, score=-91.577 total time= 2.4min\n",
      "[CV 3/5; 35/36] START learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         3        5862.7423           11.85m\n",
      "         4        2515.0881           12.73m\n",
      "        32          88.9056           24.40m\n",
      "        12        1160.5553           22.73m\n",
      "         4        4916.0399           11.74m\n",
      "         1        8231.3047           13.62m\n",
      "        13         800.7167           23.95m\n",
      "         5        2206.2604           12.29m\n",
      "        31         185.6949            6.22m\n",
      "        26         303.1459           23.44m\n",
      "        22         240.6303            8.44m\n",
      "        13         990.5759           22.57m\n",
      "         5        4148.6121           11.47m\n",
      "         2        6836.9038           13.00m\n",
      "        33          83.2835           24.23m\n",
      "         6        1949.5372           12.05m\n",
      "        14         728.9529           23.92m\n",
      "        14         850.3620           22.33m\n",
      "         6        3549.8193           11.26m\n",
      "        23         211.1661            8.29m\n",
      "        27         273.5666           23.53m\n",
      "         3        5707.3461           12.75m\n",
      "         7        1756.8482           11.72m\n",
      "        15         671.2035           23.68m\n",
      "[CV 1/5; 34/36] END learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=100;, score=-120.022 total time= 4.2min\n",
      "[CV 4/5; 35/36] START learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        32         170.4524            6.03m\n",
      "[CV 2/5; 31/36] END learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=50;, score=-102.375 total time=10.7min\n",
      "[CV 5/5; 35/36] START learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        15         735.9282           22.12m\n",
      "        64          47.7572           46.31m\n",
      "        34          76.6783           24.03m\n",
      "[CV 2/5; 30/36] END learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=100;, score=-102.384 total time=12.4min\n",
      "[CV 1/5; 36/36] START learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         7        3035.4684           11.09m\n",
      "         4        4772.1935           12.56m\n",
      "         8        1554.2123           11.43m\n",
      "         1        8651.9078           12.94m\n",
      "         1        6983.7699           14.06m\n",
      "        24         187.0473            8.09m\n",
      "        16         613.5542           22.06m\n",
      "        28         249.5722           23.54m\n",
      "         8        2607.0872           10.77m\n",
      "         1        3722.7666           30.66m\n",
      "         9        1398.9492           11.00m\n",
      "         5        4011.8017           12.47m\n",
      "         2        5819.3235           12.87m\n",
      "         2        7178.6858           12.45m\n",
      "        17         514.8661           21.96m\n",
      "         9        2255.7885           10.54m\n",
      "        25         166.2730            7.83m\n",
      "        10        1288.8453           10.71m\n",
      "         2        3269.0082           30.62m\n",
      "        29         229.5238           23.47m\n",
      "         3        4854.6082           12.36m\n",
      "         3        5979.8140           12.17m\n",
      "         6        3394.0350           12.55m\n",
      "[CV 3/5; 35/36] END learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=50;, score=-62.131 total time= 1.7min\n",
      "[CV 2/5; 36/36] START learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        18         436.4804           21.85m\n",
      "        10        1984.9318           10.36m\n",
      "        11        1177.7004           10.47m\n",
      "        26         149.4246            7.58m\n",
      "         4        4072.0324           12.03m\n",
      "         3        2895.2045           30.76m\n",
      "         4        5008.7196           12.00m\n",
      "         1        8441.7315           30.39m\n",
      "        11        1745.6230           10.15m\n",
      "        19         370.4890           21.71m\n",
      "        30         200.9952           23.41m\n",
      "        12        1075.8165           10.19m\n",
      "         5        3432.3491           11.80m\n",
      "        65          47.7572           45.00m\n",
      "         4        2515.0881           29.45m\n",
      "         5        4216.5557           11.88m\n",
      "        27         135.2923            7.30m\n",
      "         2        7023.8705           29.66m\n",
      "        12        1546.3368            9.94m\n",
      "        13         996.2895            9.92m\n",
      "        20         318.7918           21.65m\n",
      "         6        2903.2756           11.50m\n",
      "         5        2206.2604           28.19m\n",
      "         6        3575.0458           11.71m\n",
      "[CV 5/5; 35/36] END learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=50;, score=-54.104 total time= 1.6min\n",
      "[CV 3/5; 36/36] START learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        31         185.6949           23.34m\n",
      "         3        5862.7423           29.22m\n",
      "        13        1381.6296            9.75m\n",
      "        28         123.5792            7.02m\n",
      "         7        2479.7763           11.18m\n",
      "         6        1949.5372           27.51m\n",
      "        14         902.8378            9.87m\n",
      "         1        8231.3047           25.06m\n",
      "        21         274.8486           21.83m\n",
      "         4        4916.0399           27.50m\n",
      "        14        1257.4351            9.56m\n",
      "         8        2156.0917           11.03m\n",
      "         7        1756.8482           26.93m\n",
      "         2        6836.9038           25.81m\n",
      "        29         113.6389            6.77m\n",
      "         5        4148.6121           26.55m\n",
      "        15         832.4253            9.77m\n",
      "        32         170.4524           23.53m\n",
      "[CV 2/5; 32/36] END learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=100;, score=-102.375 total time=11.1min\n",
      "[CV 4/5; 36/36] START learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        15        1145.5176            9.29m\n",
      "        22         240.6303           22.03m\n",
      "         9        1857.4881           10.70m\n",
      "[CV 4/5; 35/36] END learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=50;, score=-94.155 total time= 2.4min\n",
      "[CV 5/5; 36/36] START learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         8        1554.2123           26.36m\n",
      "         3        5707.3461           25.39m\n",
      "         6        3549.8193           25.87m\n",
      "        16         768.8104            9.50m\n",
      "        66          47.7572           43.69m\n",
      "         1        6983.7699           28.18m\n",
      "         1        8651.9078           26.52m\n",
      "        16        1023.9670            9.12m\n",
      "        30         103.7457            6.53m\n",
      "         4        4772.1935           25.31m\n",
      "         9        1398.9492           26.15m\n",
      "         7        3035.4684           25.30m\n",
      "        23         211.1661           22.18m\n",
      "        17         721.4230            9.20m\n",
      "         2        5819.3235           26.84m\n",
      "         2        7178.6858           25.92m\n",
      "         5        4011.8017           25.15m\n",
      "        10        1288.8453           25.73m\n",
      "        17         900.1208            9.00m\n",
      "         8        2607.0872           25.11m\n",
      "         3        4854.6082           26.13m\n",
      "        31          95.6611            6.31m\n",
      "        18         672.0328            9.05m\n",
      "         3        5979.8140           25.49m\n",
      "        24         187.0473           22.29m\n",
      "         6        3394.0350           24.82m\n",
      "[CV 3/5; 36/36] END learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=100;, score=-62.131 total time= 1.6min\n",
      "        11        1177.7004           25.30m\n",
      "        18         796.7900            8.76m\n",
      "         9        2255.7885           24.84m\n",
      "         4        4072.0324           25.06m\n",
      "         4        5008.7196           24.80m\n",
      "        19         609.5735            8.82m\n",
      "        12        1075.8165           24.67m\n",
      "        10        1984.9318           24.34m\n",
      "        25         166.2730           22.18m\n",
      "        19         720.4805            8.51m\n",
      "        32          88.8964            6.03m\n",
      "         5        3432.3491           24.26m\n",
      "        67          47.7572           42.28m\n",
      "         5        4216.5557           24.04m\n",
      "        13         996.2895           23.94m\n",
      "        11        1745.6230           23.78m\n",
      "        20         550.9478            8.58m\n",
      "         6        2903.2756           23.51m\n",
      "        26         149.4246           21.99m\n",
      "         6        3575.0458           23.59m\n",
      "[CV 5/5; 36/36] END learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=100;, score=-54.104 total time= 1.5min\n",
      "        20         622.3921            8.29m\n",
      "        14         902.8378           23.47m\n",
      "        12        1546.3368           23.14m\n",
      "        33          83.2744            5.74m\n",
      "         7        2479.7763           22.56m\n",
      "        21         498.8880            8.28m\n",
      "        21         539.2370            8.00m\n",
      "        27         135.2923           21.64m\n",
      "        15         832.4253           22.94m\n",
      "        13        1381.6296           22.58m\n",
      "         8        2156.0917           21.93m\n",
      "        22         464.4163            7.93m\n",
      "        34          76.6695            5.42m\n",
      "        16         768.8104           22.36m\n",
      "         9        1857.4881           21.36m\n",
      "[CV 4/5; 36/36] END learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=100;, score=-94.155 total time= 2.1min\n",
      "        14        1257.4351           22.12m\n",
      "        28         123.5792           21.32m\n",
      "        22         470.2032            7.78m\n",
      "        23         427.8795            7.63m\n",
      "        17         721.4230           21.72m\n",
      "        68          47.7572           40.85m\n",
      "        15        1145.5176           21.45m\n",
      "        35          71.9359            5.08m\n",
      "[CV 2/5; 33/36] END learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=50;, score=-102.787 total time=11.9min\n",
      "        29         113.6389           20.98m\n",
      "        23         432.7486            7.51m\n",
      "        18         672.0328           21.24m\n",
      "        24         398.7347            7.30m\n",
      "        16        1023.9670           20.89m\n",
      "        24         381.9834            7.18m\n",
      "        19         609.5735           20.81m\n",
      "        30         103.7457           20.67m\n",
      "        17         900.1208           20.45m\n",
      "        25         357.5336            7.05m\n",
      "        25         340.0719            6.86m\n",
      "[CV 2/5; 35/36] END learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=50;, score=-98.427 total time= 6.9min\n",
      "        20         550.9478           20.36m\n",
      "        18         796.7900           19.98m\n",
      "        31          95.6611           20.41m\n",
      "        26         323.4910            6.74m\n",
      "        69          47.7572           39.35m\n",
      "        19         720.4805           19.49m\n",
      "        21         498.8880           19.91m\n",
      "        32          88.8964           20.07m\n",
      "        22         464.4163           19.46m\n",
      "        20         622.3921           19.15m\n",
      "        27         290.0738            6.45m\n",
      "        21         539.2370           18.81m\n",
      "        23         427.8795           19.12m\n",
      "        33          83.2744           19.80m\n",
      "        28         259.8757            6.18m\n",
      "[CV 1/5; 35/36] END learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=50;, score=-119.996 total time= 7.9min\n",
      "        70          47.7572           37.83m\n",
      "        24         398.7347           18.75m\n",
      "        22         470.2032           18.59m\n",
      "        34          76.6695           19.45m\n",
      "        23         432.7486           18.33m\n",
      "        25         357.5336           18.59m\n",
      "        35          71.9359           19.11m\n",
      "[CV 2/5; 34/36] END learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=100;, score=-102.787 total time=10.3min\n",
      "        24         381.9834           17.98m\n",
      "        26         323.4910           18.28m\n",
      "        25         340.0719           17.61m\n",
      "[CV 2/5; 36/36] END learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=100;, score=-98.427 total time= 5.9min\n",
      "        27         290.0738           17.98m\n",
      "        71          47.7572           36.37m\n",
      "        28         259.8757           17.69m\n",
      "[CV 1/5; 36/36] END learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=100;, score=-119.996 total time= 6.9min\n",
      "        72          47.7572           34.85m\n",
      "        73          47.7569           33.29m\n",
      "        74          47.7569           31.85m\n",
      "        75          47.7569           30.41m\n",
      "        76          47.7569           28.98m\n",
      "        77          47.7569           27.59m\n",
      "        78          47.7569           26.22m\n",
      "        79          47.7569           24.86m\n",
      "[CV 5/5; 12/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100;, score=-59.004 total time=93.5min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        3712.0335            6.67m\n",
      "         2        2375.7903            6.72m\n",
      "         3        1687.9582            6.88m\n",
      "         4         747.7356            7.23m\n",
      "         5         456.9083            7.92m\n",
      "         6         302.8289            8.10m\n"
     ]
    }
   ],
   "source": [
    "# X = train_norm.drop(columns=['median_normal','id']).to_numpy()\n",
    "# y = train_norm['median_normal'].to_numpy()\n",
    "\n",
    "# with parallel_backend('threading',n_jobs=10):\n",
    "#     norm_gs = GridSearchCV(\n",
    "#         gbr['norm'],params,\n",
    "#         scoring='neg_root_mean_squared_error',\n",
    "#         verbose=10\n",
    "#     ).fit(\n",
    "#         X,y\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "[CV 1/5; 1/36] START learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "[CV 2/5; 1/36] START learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "[CV 3/5; 1/36] START learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "[CV 4/5; 1/36] START learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "[CV 5/5; 1/36] START learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "[CV 1/5; 2/36] START learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "[CV 2/5; 2/36] START learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "[CV 3/5; 2/36] START learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "[CV 4/5; 2/36] START learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "[CV 5/5; 2/36] START learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         852.7424            5.89m\n",
      "         1        1041.2194            5.95m\n",
      "         1         852.7424           11.90m\n",
      "         1        1041.2194           12.06m\n",
      "         1         969.4243            6.31m\n",
      "         1         865.7967            6.33m\n",
      "         1         601.8204           13.07m\n",
      "         1         601.8204            6.70m\n",
      "         1         969.4243           14.27m\n",
      "         1         865.7967           14.80m\n",
      "         2         256.6845           22.23m\n",
      "         2         256.6845           10.94m\n",
      "         2         248.3636           11.74m\n",
      "         2         607.5254           11.96m\n",
      "         2         310.1645           24.93m\n",
      "         2         656.2162           24.95m\n",
      "         2         310.1645           12.38m\n",
      "         2         656.2162           12.47m\n",
      "         2         607.5254           27.01m\n",
      "         2         248.3636           28.78m\n",
      "         3         201.2891           26.95m\n",
      "         3         247.1724           27.99m\n",
      "         3         201.2891           13.59m\n",
      "         3         144.1435           13.68m\n",
      "         3         465.0892           13.74m\n",
      "         3         247.1724           13.71m\n",
      "         3         544.5402           28.27m\n",
      "         3         544.5402           14.32m\n",
      "         3         465.0892           30.34m\n",
      "         3         144.1435           34.26m\n",
      "         4         212.1137           28.89m\n",
      "         4         185.5773           29.09m\n",
      "         4         212.1137           13.98m\n",
      "         4         114.4835           14.20m\n",
      "         4         185.5773           14.33m\n",
      "         4         223.6509           30.25m\n",
      "         4         351.6511           14.67m\n",
      "         4         223.6509           15.31m\n",
      "         4         351.6511           33.28m\n",
      "         4         114.4835           35.73m\n",
      "         5         197.0887           29.65m\n",
      "         5         197.0887           14.18m\n",
      "         5         102.7733           14.36m\n",
      "         5         175.4090           30.55m\n",
      "         5         175.4090           14.67m\n",
      "         5         292.3417           14.84m\n",
      "         5         204.2922           31.36m\n",
      "         5         204.2922           15.69m\n",
      "         5         292.3417           33.35m\n",
      "         6         164.3546           30.04m\n",
      "         5         102.7733           36.53m\n",
      "         6         189.6268           30.23m\n",
      "         6         164.3546           14.22m\n",
      "         6          96.5464           14.35m\n",
      "         6         192.4450           31.28m\n",
      "         6         189.6268           14.71m\n",
      "         6         252.7439           14.89m\n",
      "         6         192.4450           15.32m\n",
      "         6         252.7439           33.46m\n",
      "         7          91.8647           14.15m\n",
      "         7         186.4952           30.79m\n",
      "         7         162.2297           14.28m\n",
      "         7         162.2297           31.22m\n",
      "         7         185.8515           31.35m\n",
      "         6          96.5464           37.09m\n",
      "         7         186.4952           14.69m\n",
      "         7         222.4277           14.86m\n",
      "         7         185.8515           15.07m\n",
      "         7         222.4277           33.79m\n",
      "         8          88.7326           13.90m\n",
      "         8         160.0376           14.07m\n",
      "[CV 5/5; 1/36] END learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=50;, score=-54.161 total time= 2.7min\n",
      "[CV 1/5; 3/36] START learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         8         160.0376           31.11m\n",
      "[CV 5/5; 2/36] END learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=100;, score=-54.161 total time= 2.7min\n",
      "[CV 2/5; 3/36] START learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         8         184.6110           31.35m\n",
      "         8         183.0286           31.54m\n",
      "         8         184.6110           14.56m\n",
      "         7          91.8647           37.03m\n",
      "         8         201.7335           14.66m\n",
      "         1         958.1743            6.36m\n",
      "         8         183.0286           14.85m\n",
      "         1         699.9151            7.94m\n",
      "         8         201.7335           33.86m\n",
      "         9          87.3163           13.70m\n",
      "         9         180.3407           31.06m\n",
      "         9         183.0032           31.35m\n",
      "[CV 2/5; 2/36] END learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=100;, score=-55.149 total time= 3.1min\n",
      "[CV 3/5; 3/36] START learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         9         183.0032           14.29m\n",
      "[CV 2/5; 1/36] END learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=50;, score=-55.149 total time= 3.1min\n",
      "[CV 4/5; 3/36] START learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         9         180.3407           14.42m\n",
      "         2         308.4007           11.49m\n",
      "         9         188.4576           14.46m\n",
      "         8          88.7326           36.70m\n",
      "         1        1113.4356            5.85m\n",
      "         1        1193.6062            6.05m\n",
      "         2         406.2346           13.48m\n",
      "         9         188.4576           33.98m\n",
      "        10          86.6740           13.50m\n",
      "        10         179.3187           31.14m\n",
      "        10         173.6855           13.98m\n",
      "         3         240.5920           13.08m\n",
      "        10         179.3187           14.23m\n",
      "         2         712.6714           12.42m\n",
      "         9          87.3163           36.66m\n",
      "         2         819.1019           12.36m\n",
      "         3         319.3776           15.19m\n",
      "        10         173.6855           33.38m\n",
      "        11          85.5714           13.21m\n",
      "        11         178.4780           30.96m\n",
      "        11         147.7826           13.59m\n",
      "         4         207.7416           13.98m\n",
      "        11         178.4780           13.92m\n",
      "         3         606.4932           13.97m\n",
      "         3         352.3926           13.83m\n",
      "         4         267.5621           15.39m\n",
      "        12          84.9007           12.86m\n",
      "        10          86.6740           36.69m\n",
      "        11         147.7826           33.03m\n",
      "        12         177.1298           30.36m\n",
      "[CV 3/5; 2/36] END learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=100;, score=-40.874 total time= 4.2min\n",
      "[CV 5/5; 3/36] START learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        12         143.5582           13.29m\n",
      "        12         177.1298           13.42m\n",
      "[CV 3/5; 1/36] END learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=50;, score=-40.874 total time= 4.2min\n",
      "[CV 1/5; 4/36] START learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         970.3451            5.81m\n",
      "         5         135.9065           14.26m\n",
      "         4         270.3906           14.32m\n",
      "        13          84.2498           12.43m\n",
      "         1         958.1743           12.82m\n",
      "         4         282.2496           14.49m\n",
      "         5         244.5065           15.40m\n",
      "        12         143.5582           32.68m\n",
      "        11          85.5714           36.38m\n",
      "        13         142.5674           12.97m\n",
      "         2         381.2440           11.88m\n",
      "         6         117.7808           14.46m\n",
      "        14          83.7547           12.01m\n",
      "         2         308.4007           23.58m\n",
      "         5         241.3358           14.53m\n",
      "         5         247.8345           14.90m\n",
      "         6         224.7554           15.42m\n",
      "        13         142.5674           32.35m\n",
      "        12          84.9007           35.98m\n",
      "        14         142.0984           12.67m\n",
      "         3         277.2564           13.58m\n",
      "        15          83.5119           11.73m\n",
      "         7         107.5213           14.43m\n",
      "         3         240.5920           27.20m\n",
      "         6         207.9055           14.63m\n",
      "         7         213.0764           14.96m\n",
      "         6         220.4075           15.03m\n",
      "        14         142.0984           32.00m\n",
      "        15         141.4845           12.26m\n",
      "        13          84.2498           35.33m\n",
      "        16          83.3181           11.39m\n",
      "         4         233.4849           13.99m\n",
      "         8         100.9644           14.34m\n",
      "         4         207.7416           29.34m\n",
      "         7         180.6467           14.51m\n",
      "[CV 4/5; 3/36] END learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=50;, score=-46.766 total time= 2.4min\n",
      "[CV 2/5; 4/36] START learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         7         208.2894           14.78m\n",
      "         8         203.1983           14.93m\n",
      "        15         141.4845           31.51m\n",
      "        16         140.9191           11.87m\n",
      "[CV 4/5; 1/36] END learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=50;, score=-47.945 total time= 5.6min\n",
      "[CV 3/5; 4/36] START learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        14          83.7547           34.66m\n",
      "         1         699.9151           15.47m\n",
      "         1        1113.4356           11.47m\n",
      "         5         196.7909           14.09m\n",
      "        17          83.1219           11.10m\n",
      "         9          96.2514           13.96m\n",
      "         8         198.0216           14.30m\n",
      "         5         135.9065           30.60m\n",
      "         9         198.4044           14.48m\n",
      "        16         140.9191           30.95m\n",
      "[CV 4/5; 2/36] END learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=100;, score=-47.945 total time= 5.9min\n",
      "[CV 4/5; 4/36] START learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        18          82.9489           10.68m\n",
      "         1        1193.6062           12.55m\n",
      "        15          83.5119           34.32m\n",
      "         2         406.2346           27.16m\n",
      "         6         186.2552           14.25m\n",
      "         2         712.6714           24.59m\n",
      "[CV 5/5; 3/36] END learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=50;, score=-44.293 total time= 1.9min\n",
      "[CV 5/5; 4/36] START learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        10          93.3386           13.92m\n",
      "         9         191.4895           14.09m\n",
      "         1         970.3451           12.72m\n",
      "        10         193.2853           14.09m\n",
      "         6         117.7808           31.29m\n",
      "        19          82.8533           10.26m\n",
      "         2         819.1019           25.24m\n",
      "         3         319.3776           29.97m\n",
      "         3         606.4932           27.99m\n",
      "        16          83.3181           33.88m\n",
      "        10         186.4470           13.86m\n",
      "         2         381.2440           25.01m\n",
      "        11         191.1450           13.82m\n",
      "        11          92.0366           13.94m\n",
      "         7         107.5213           31.60m\n",
      "        20          82.8225           10.01m\n",
      "[CV 1/5; 1/36] END learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=50;, score=-52.381 total time= 6.7min\n",
      "[CV 1/5; 5/36] START learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         3         352.3926           28.61m\n",
      "         4         267.5621           30.62m\n",
      "         4         270.3906           29.20m\n",
      "         1         863.5464            6.85m\n",
      "        11         182.5545           13.38m\n",
      "        17          83.1219           33.65m\n",
      "         3         277.2564           28.47m\n",
      "        12          90.4347           13.68m\n",
      "         8         100.9644           31.76m\n",
      "        12         187.8837           13.69m\n",
      "         5         244.5065           30.86m\n",
      "         4         282.2496           30.40m\n",
      "         5         247.8345           30.29m\n",
      "        12         181.3419           13.10m\n",
      "        18          82.9489           33.00m\n",
      "         9          96.2514           31.44m\n",
      "         4         233.4849           30.66m\n",
      "        13         186.2838           13.32m\n",
      "[CV 2/5; 3/36] END learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=50;, score=-55.723 total time= 4.7min\n",
      "[CV 2/5; 5/36] START learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        13          89.4279           13.43m\n",
      "         2         236.7250           17.75m\n",
      "         6         224.7554           31.13m\n",
      "         5         241.3358           30.74m\n",
      "         6         220.4075           30.25m\n",
      "         1         600.8077            7.13m\n",
      "        13         179.6546           12.74m\n",
      "        19          82.8533           32.32m\n",
      "        10          93.3386           31.35m\n",
      "        14          88.2664           13.01m\n",
      "         5         196.7909           31.50m\n",
      "         7         213.0764           30.64m\n",
      "         7         208.2894           30.18m\n",
      "         6         207.9055           31.29m\n",
      "         3         127.4797           20.79m\n",
      "        14         178.8090           12.65m\n",
      "        20          82.8225           32.21m\n",
      "[CV 1/5; 2/36] END learning_rate=1.0, max_depth=50, min_samples_split=10, n_estimators=100;, score=-52.381 total time= 8.1min\n",
      "[CV 3/5; 5/36] START learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        11          92.0366           31.20m\n",
      "        15          87.7978           12.69m\n",
      "         6         186.2552           31.95m\n",
      "[CV 5/5; 4/36] END learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=100;, score=-44.293 total time= 2.0min\n",
      "[CV 4/5; 5/36] START learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         8         198.0216           29.81m\n",
      "         8         203.1983           30.92m\n",
      "         2         279.2852           19.31m\n",
      "         1        1038.8389            8.01m\n",
      "         7         180.6467           31.31m\n",
      "[CV 4/5; 4/36] END learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=100;, score=-46.766 total time= 2.4min\n",
      "[CV 5/5; 5/36] START learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         967.6422            6.26m\n",
      "         1         851.8136            6.13m\n",
      "        15         178.2037           12.35m\n",
      "        12          90.4347           30.84m\n",
      "        16          86.6588           12.27m\n",
      "[CV 1/5; 3/36] END learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=50;, score=-52.322 total time= 5.8min\n",
      "[CV 1/5; 6/36] START learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         9         198.4044           30.67m\n",
      "         9         191.4895           29.90m\n",
      "         1         863.5464           13.93m\n",
      "         4         104.4514           23.23m\n",
      "        16         177.5155           11.95m\n",
      "[CV 3/5; 3/36] END learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=50;, score=-41.065 total time= 5.6min\n",
      "[CV 2/5; 6/36] START learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         3         227.3590           21.35m\n",
      "        13          89.4279           30.63m\n",
      "         1         600.8077           14.63m\n",
      "        10         186.4470           29.84m\n",
      "         2         638.0527           20.34m\n",
      "        10         193.2853           30.65m\n",
      "         2         595.1583           18.60m\n",
      "         2         248.4406           16.09m\n",
      "        14          88.2664           30.24m\n",
      "        11         182.5545           29.39m\n",
      "         2         236.7250           37.23m\n",
      "         5          94.3851           23.89m\n",
      "        11         191.1450           30.97m\n",
      "         4         201.7732           22.41m\n",
      "        15          87.7978           30.03m\n",
      "         3         421.7305           22.14m\n",
      "        12         181.3419           29.26m\n",
      "         2         279.2852           40.96m\n",
      "         3         191.1703           21.30m\n",
      "         3         513.4347           25.46m\n",
      "        12         187.8837           31.00m\n",
      "         3         127.4797           44.69m\n",
      "        16          86.6588           29.50m\n",
      "[CV 1/5; 4/36] END learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=100;, score=-52.322 total time= 5.6min\n",
      "[CV 3/5; 6/36] START learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        13         179.6546           29.07m\n",
      "         5         186.1219           22.83m\n",
      "         6          88.3216           24.33m\n",
      "         1        1038.8389           13.28m\n",
      "        13         186.2838           30.58m\n",
      "[CV 2/5; 4/36] END learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=100;, score=-55.723 total time= 4.6min\n",
      "[CV 4/5; 6/36] START learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         3         227.3590           45.78m\n",
      "         4         323.6447           23.91m\n",
      "         1         967.6422           14.47m\n",
      "        14         178.8090           28.96m\n",
      "         4         169.9066           23.87m\n",
      "         4         199.8056           27.42m\n",
      "         6         182.7432           23.13m\n",
      "         4         104.4514           50.27m\n",
      "         2         638.0527           34.86m\n",
      "        15         178.2037           28.77m\n",
      "         4         201.7732           47.96m\n",
      "         7          85.8613           25.01m\n",
      "         2         595.1583           39.35m\n",
      "         5         270.3509           25.19m\n",
      "         5         161.1385           24.23m\n",
      "        16         177.5155           28.36m\n",
      "[CV 3/5; 4/36] END learning_rate=1.0, max_depth=50, min_samples_split=20, n_estimators=100;, score=-41.065 total time= 5.4min\n",
      "[CV 5/5; 6/36] START learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         5         189.8808           26.83m\n",
      "         1         851.8136           12.61m\n",
      "         3         513.4347           43.34m\n",
      "         5          94.3851           52.26m\n",
      "         7         180.3460           23.48m\n",
      "         5         186.1219           48.88m\n",
      "         8          84.4566           24.47m\n",
      "         3         421.7305           46.23m\n",
      "         6         158.4980           24.32m\n",
      "[CV 5/5; 5/36] END learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=50;, score=-43.591 total time= 3.3min\n",
      "[CV 1/5; 7/36] START learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2         248.4406           33.49m\n",
      "         6         235.2848           26.00m\n",
      "         8         179.4183           22.57m\n",
      "         1         956.5110            6.55m\n",
      "         6         182.4082           27.28m\n",
      "         4         199.8056           47.15m\n",
      "         9          83.4447           23.66m\n",
      "         6          88.3216           54.09m\n",
      "         6         182.7432           49.83m\n",
      "         4         323.6447           50.43m\n",
      "         2         295.0713           16.60m\n",
      "         9         179.0288           22.40m\n",
      "         7         209.6731           25.76m\n",
      "         5         189.8808           46.90m\n",
      "         3         191.1703           44.17m\n",
      "        10          83.1451           23.00m\n",
      "         7         180.3460           51.05m\n",
      "         7         177.4425           27.78m\n",
      "         7          85.8613           55.54m\n",
      "        10         178.4979           21.77m\n",
      "         5         270.3509           53.51m\n",
      "         3         202.0645           21.36m\n",
      "         6         182.4082           48.59m\n",
      "         8         192.0456           25.66m\n",
      "         8         179.4183           49.50m\n",
      "         4         169.9066           50.17m\n",
      "        11          82.9947           22.79m\n",
      "         8          84.4566           54.75m\n",
      "         8         175.8983           27.62m\n",
      "        11         178.1646           21.50m\n",
      "        12          82.7980           21.71m\n",
      "         6         235.2848           55.42m\n",
      "         9         179.0288           49.46m\n",
      "         7         177.4425           50.06m\n",
      "         4         132.8158           24.40m\n",
      "         9         179.7014           25.45m\n",
      "         5         161.1385           52.09m\n",
      "         9          83.4447           53.73m\n",
      "         9         175.2331           27.24m\n",
      "        12         178.0311           21.07m\n",
      "        10         178.4979           48.67m\n",
      "        13          82.7001           21.49m\n",
      "         7         209.6731           55.24m\n",
      "         8         175.8983           50.38m\n",
      "        10         167.4790           24.56m\n",
      "[CV 4/5; 5/36] END learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=50;, score=-49.025 total time= 6.1min\n",
      "[CV 2/5; 7/36] START learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        10          83.1451           52.86m\n",
      "         6         158.4980           53.23m\n",
      "[CV 5/5; 6/36] END learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=100;, score=-43.591 total time= 3.4min\n",
      "[CV 3/5; 7/36] START learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "         5         110.3985           25.29m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         699.1737            7.23m\n",
      "         1        1111.3349            6.11m\n",
      "        13         177.9057           20.82m\n",
      "        10         174.9330           26.71m\n",
      "        14          82.6327           20.76m\n",
      "         9         175.2331           50.26m\n",
      "        11         178.1646           49.35m\n",
      "         8         192.0456           55.73m\n",
      "        11          82.9947           52.99m\n",
      "         6         101.2238           25.26m\n",
      "[CV 1/5; 7/36] END learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=50;, score=-51.456 total time= 3.5min\n",
      "[CV 4/5; 7/36] START learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2         383.4243           18.39m\n",
      "         1        1192.2852            6.65m\n",
      "         2         653.4749           19.34m\n",
      "        14         177.8682           20.24m\n",
      "        11         174.5216           25.92m\n",
      "        15          82.5923           20.33m\n",
      "        10         174.9330           49.91m\n",
      "        12          82.7980           51.29m\n",
      "        12         178.0311           49.36m\n",
      "         9         179.7014           56.02m\n",
      "         3         304.4282           21.92m\n",
      "         3         540.2379           22.52m\n",
      "         2         795.2293           19.86m\n",
      "        15         177.8144           19.89m\n",
      "        11         174.5216           49.11m\n",
      "        16          82.5711           19.82m\n",
      "        12         174.4226           25.34m\n",
      "        13         177.9057           49.70m\n",
      "        13          82.7001           51.60m\n",
      "        10         167.4790           54.80m\n",
      "[CV 4/5; 6/36] END learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=100;, score=-49.025 total time= 6.1min\n",
      "[CV 5/5; 7/36] START learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         969.5941            6.14m\n",
      "         4         253.3717           23.26m\n",
      "        16         177.7896           19.24m\n",
      "         4         219.9983           23.95m\n",
      "         3         316.3089           22.79m\n",
      "        12         174.4226           48.68m\n",
      "        17          82.5568           19.38m\n",
      "        14          82.6327           50.60m\n",
      "        14         177.8682           49.17m\n",
      "        13         174.3318           25.02m\n",
      "        17         177.7703           18.44m\n",
      "         5         235.5974           23.54m\n",
      "         2         364.2159           17.44m\n",
      "         5         204.1261           24.17m\n",
      "        13         174.3318           48.80m\n",
      "        18          82.5499           18.66m\n",
      "         4         243.9307           24.85m\n",
      "        15          82.5923           50.35m\n",
      "        15         177.8144           49.39m\n",
      "         6         216.8105           23.83m\n",
      "        18         177.7581           18.04m\n",
      "         3         249.7776           21.93m\n",
      "        14         174.2921           24.53m\n",
      "        14         174.2921           48.61m\n",
      "        19          82.5460           18.15m\n",
      "         6         195.9284           25.06m\n",
      "         5         210.8228           25.69m\n",
      "        16          82.5711           49.94m\n",
      "        16         177.7896           48.64m\n",
      "        19         177.7546           17.36m\n",
      "         7         203.9151           23.54m\n",
      "         4         214.7220           23.48m\n",
      "        15         174.2669           23.91m\n",
      "        20          82.5430           17.52m\n",
      "        15         174.2669           48.14m\n",
      "         7         190.3442           24.94m\n",
      "         6         179.8719           25.40m\n",
      "[CV 4/5; 7/36] END learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=50;, score=-47.187 total time= 3.5min\n",
      "[CV 1/5; 8/36] START learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        17         177.7703           47.67m\n",
      "        17          82.5568           49.65m\n",
      "         1         956.5110           13.79m\n",
      "        20         177.7529           16.89m\n",
      "         8         194.6555           23.49m\n",
      "         5         191.4522           23.80m\n",
      "        21          82.5413           17.02m\n",
      "        16         174.2551           48.01m\n",
      "         8         186.4060           24.53m\n",
      "        16         174.2551           23.43m\n",
      "        18          82.5499           48.69m\n",
      "        18         177.7581           47.46m\n",
      "         2         295.0713           34.04m\n",
      "        21         177.7517           16.30m\n",
      "[CV 2/5; 5/36] END learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=50;, score=-55.237 total time=11.8min\n",
      "[CV 2/5; 8/36] START learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         699.1737           14.76m\n",
      "         6         170.8670           23.84m\n",
      "         9         192.3560           23.53m\n",
      "        17         174.2445           46.95m\n",
      "        19         177.7546           46.49m\n",
      "        17         174.2445           22.52m\n",
      "        22          82.5403           16.57m\n",
      "         9         182.7228           24.31m\n",
      "[CV 3/5; 7/36] END learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=50;, score=-40.292 total time= 5.3min\n",
      "[CV 3/5; 8/36] START learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        19          82.5460           48.24m\n",
      "         3         202.0645           42.94m\n",
      "         1        1111.3349           12.57m\n",
      "        18         174.2389           45.93m\n",
      "         2         383.4243           37.59m\n",
      "        10         187.7359           23.04m\n",
      "         7         163.5337           23.78m\n",
      "        18         174.2389           21.61m\n",
      "        20         177.7529           46.07m\n",
      "        20          82.5430           47.48m\n",
      "        19         174.2355           45.09m\n",
      "[CV 3/5; 6/36] END learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=100;, score=-40.143 total time=10.6min\n",
      "[CV 4/5; 8/36] START learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        23          82.5396           16.17m\n",
      "         2         653.4749           38.58m\n",
      "         4         132.8158           48.80m\n",
      "         1        1192.2852           13.26m\n",
      "         3         304.4282           45.43m\n",
      "         8         160.7445           23.32m\n",
      "        11         186.6187           22.58m\n",
      "        21         177.7517           45.32m\n",
      "[CV 2/5; 6/36] END learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=100;, score=-55.237 total time=12.1min\n",
      "[CV 5/5; 8/36] START learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        19         174.2355           20.78m\n",
      "[CV 3/5; 5/36] END learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=50;, score=-40.143 total time=12.8min\n",
      "[CV 1/5; 9/36] START learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        24          82.5366           15.38m\n",
      "         1         969.5941           12.19m\n",
      "        21          82.5413           46.97m\n",
      "         1         863.5463            8.60m\n",
      "         3         540.2379           45.46m\n",
      "         9         158.6473           22.80m\n",
      "         5         110.3985           51.33m\n",
      "         2         795.2293           37.71m\n",
      "         4         253.3717           48.65m\n",
      "        12         184.5475           22.65m\n",
      "         2         364.2159           35.73m\n",
      "        25          82.5361           14.86m\n",
      "        22          82.5403           46.62m\n",
      "         4         219.9983           48.88m\n",
      "         3         316.3089           43.72m\n",
      "        10         157.6480           22.56m\n",
      "         6         101.2238           52.00m\n",
      "[CV 1/5; 8/36] END learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=100;, score=-51.456 total time= 3.3min\n",
      "[CV 2/5; 9/36] START learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "         5         235.5974           49.72m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2         233.4682           25.82m\n",
      "         1         600.8074            7.09m\n",
      "        13         183.7489           22.10m\n",
      "         3         249.7776           44.96m\n",
      "        26          82.5357           14.33m\n",
      "        23          82.5396           46.28m\n",
      "         5         204.1261           50.68m\n",
      "        11         156.7496           22.15m\n",
      "         6         216.8105           50.83m\n",
      "         4         243.9307           49.05m\n",
      "        14         182.5862           21.37m\n",
      "        27          82.5354           13.60m\n",
      "        24          82.5366           45.07m\n",
      "         2         273.8096           22.60m\n",
      "         4         214.7220           48.52m\n",
      "         7         203.9151           50.86m\n",
      "         6         195.9284           51.73m\n",
      "        12         155.9073           21.85m\n",
      "[CV 5/5; 7/36] END learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=50;, score=-45.257 total time= 6.9min\n",
      "[CV 3/5; 9/36] START learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         3         125.7195           35.65m\n",
      "         5         210.8228           51.82m\n",
      "         1        1038.8387            6.29m\n",
      "        28          82.5352           13.01m\n",
      "[CV 1/5; 5/36] END learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=50;, score=-51.048 total time=16.6min\n",
      "[CV 4/5; 9/36] START learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        15         181.8506           20.96m\n",
      "        25          82.5361           44.70m\n",
      "         1         967.6394            6.66m\n",
      "         5         191.4522           49.61m\n",
      "         8         194.6555           51.12m\n",
      "         3         220.5851           28.87m\n",
      "         7         190.3442           53.04m\n",
      "         6         179.8719           52.18m\n",
      "[CV 4/5; 8/36] END learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=100;, score=-47.187 total time= 3.3min\n",
      "[CV 5/5; 9/36] START learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2         633.7319           19.37m\n",
      "         1         851.8128            6.12m\n",
      "        16         181.4232           20.56m\n",
      "[CV 2/5; 7/36] END learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=50;, score=-56.873 total time= 9.7min\n",
      "[CV 1/5; 10/36] START learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        26          82.5357           44.21m\n",
      "         6         170.8670           50.34m\n",
      "         1         863.5463           16.53m\n",
      "         2         590.7187           23.59m\n",
      "         8         186.4060           52.37m\n",
      "         9         192.3560           51.66m\n",
      "         4         100.0821           40.39m\n",
      "        27          82.5354           43.13m\n",
      "         2         246.4968           18.57m\n",
      "         7         163.5337           50.89m\n",
      "         4         201.6957           32.55m\n",
      "         3         521.6266           27.03m\n",
      "        10         187.7359           51.02m\n",
      "         2         233.4682           45.74m\n",
      "         9         182.7228           52.85m\n",
      "[CV 3/5; 8/36] END learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=100;, score=-40.292 total time= 5.2min\n",
      "[CV 2/5; 10/36] START learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        28          82.5352           42.69m\n",
      "[CV 1/5; 6/36] END learning_rate=1.0, max_depth=100, min_samples_split=10, n_estimators=100;, score=-51.048 total time=16.6min\n",
      "[CV 3/5; 10/36] START learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         600.8074           14.64m\n",
      "         1        1038.8387           12.41m\n",
      "         8         160.7445           50.79m\n",
      "         3         420.5316           30.96m\n",
      "         5         188.9361           32.50m\n",
      "         5          93.1019           41.96m\n",
      "         3         191.2245           27.31m\n",
      "        11         186.6187           51.17m\n",
      "         4         201.9684           30.53m\n",
      "         9         158.6473           50.68m\n",
      "         2         633.7319           39.05m\n",
      "         3         125.7195           63.31m\n",
      "         2         273.8096           47.58m\n",
      "         6         182.7711           32.74m\n",
      "         4         318.7114           35.10m\n",
      "        12         184.5475           52.39m\n",
      "        10         157.6480           50.94m\n",
      "         4         172.4442           30.69m\n",
      "         6          87.2543           42.43m\n",
      "         5         187.4148           32.55m\n",
      "         3         521.6266           57.62m\n",
      "         3         220.5851           64.08m\n",
      "         4         100.0821           72.42m\n",
      "        13         183.7489           52.34m\n",
      "         7         180.8276           31.99m\n",
      "        11         156.7496           50.59m\n",
      "         5         267.9230           37.15m\n",
      "         7          84.5215           40.99m\n",
      "         5         159.1673           33.57m\n",
      "         6         178.1255           33.39m\n",
      "        14         182.5862           51.86m\n",
      "        12         155.9073           51.43m\n",
      "[CV 5/5; 8/36] END learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=100;, score=-45.257 total time= 7.0min\n",
      "[CV 4/5; 10/36] START learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         4         201.9684           66.33m\n",
      "         1         967.6394           13.82m\n",
      "         4         201.6957           71.85m\n",
      "         8         179.2325           32.53m\n",
      "         5          93.1019           77.88m\n",
      "         6         231.9817           37.77m\n",
      "        15         181.8506           52.18m\n",
      "         8          83.8884           40.14m\n",
      "         6         156.7544           34.33m\n",
      "         7         175.8383           34.83m\n",
      "         5         188.9361           72.48m\n",
      "         5         187.4148           70.83m\n",
      "         2         590.7187           52.55m\n",
      "         9         178.3430           32.34m\n",
      "        16         181.4232           52.64m\n",
      "[CV 2/5; 8/36] END learning_rate=1.0, max_depth=100, min_samples_split=20, n_estimators=100;, score=-56.873 total time=10.0min\n",
      "[CV 5/5; 10/36] START learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         6          87.2543           82.61m\n",
      "         7         155.1905           34.25m\n",
      "         1         851.8128           15.71m\n",
      "         7         207.9250           38.26m\n",
      "         9          83.0222           39.53m\n",
      "         6         182.7711           73.45m\n",
      "         6         178.1255           73.16m\n",
      "         8         174.9384           35.17m\n",
      "        10         178.1168           32.03m\n",
      "         3         420.5316           67.84m\n",
      "         7          84.5215           81.38m\n",
      "         2         246.4968           45.83m\n",
      "         8         190.8112           37.36m\n",
      "        10          82.7275           38.22m\n",
      "         8         154.6538           34.77m\n",
      "         7         180.8276           72.70m\n",
      "        11         177.9711           31.43m\n",
      "         7         175.8383           76.66m\n",
      "         4         318.7114           77.44m\n",
      "         8          83.8884           81.58m\n",
      "         9         174.7137           36.42m\n",
      "        11          82.6478           36.68m\n",
      "         9         178.7175           36.47m\n",
      "         3         191.2245           68.22m\n",
      "         9         154.4246           34.66m\n",
      "         8         179.2325           74.93m\n",
      "        12         177.8668           30.65m\n",
      "         8         174.9384           77.19m\n",
      "        12          82.6078           35.27m\n",
      "        10         174.4916           35.94m\n",
      "        10         166.9556           35.61m\n",
      "         9          83.0222           82.72m\n",
      "         5         267.9230           82.72m\n",
      "         9         178.3430           75.38m\n",
      "         4         172.4442           77.43m\n",
      "        10         154.3285           34.73m\n",
      "        13         177.8140           30.48m\n",
      "        13          82.5842           34.00m\n",
      "         9         174.7137           80.23m\n",
      "        10          82.7275           81.85m\n",
      "        11         174.4121           35.55m\n",
      "        11         143.3940           34.98m\n",
      "         6         231.9817           85.91m\n",
      "        11         154.2828           33.83m\n",
      "        14         177.7868           29.75m\n",
      "        10         178.1168           75.99m\n",
      "        14          82.5657           33.14m\n",
      "         5         159.1673           85.86m\n",
      "        11          82.6478           79.92m\n",
      "        12         139.9364           33.93m\n",
      "        10         174.4916           80.34m\n",
      "        12         174.3891           34.63m\n",
      "        12         154.2537           33.36m\n",
      "        11         177.9711           75.75m\n",
      "        15         177.7709           29.24m\n",
      "         7         207.9250           88.51m\n",
      "        15          82.5570           32.15m\n",
      "        12          82.6078           78.66m\n",
      "        13         174.3621           33.43m\n",
      "        13         139.7583           33.04m\n",
      "         6         156.7544           88.68m\n",
      "        11         174.4121           80.63m\n",
      "        12         177.8668           75.02m\n",
      "        13         154.2306           32.61m\n",
      "        16         177.7666           28.85m\n",
      "         8         190.8112           87.33m\n",
      "        16          82.5484           31.36m\n",
      "        13          82.5842           77.65m\n",
      "        14         139.6832           32.44m\n",
      "        14         174.3507           32.91m\n",
      "        12         174.3891           79.81m\n",
      "         7         155.1905           89.54m\n",
      "        13         177.8140           75.83m\n",
      "         9         178.7175           85.89m\n",
      "        14         154.2115           32.30m\n",
      "        17         177.7587           28.30m\n",
      "        14          82.5657           77.25m\n",
      "        17          82.5430           30.56m\n",
      "[CV 1/5; 9/36] END learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=50;, score=-51.993 total time=15.8min\n",
      "[CV 1/5; 11/36] START learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         956.5110            6.97m\n",
      "        13         174.3621           78.32m\n",
      "        15         174.3450           32.06m\n",
      "        15         139.6659           31.93m\n",
      "        14         177.7868           74.86m\n",
      "        15         154.1956           31.26m\n",
      "        10         166.9556           84.89m\n",
      "         8         154.6538           92.07m\n",
      "         2         290.5836           18.09m\n",
      "        18         177.7535           27.75m\n",
      "        15          82.5570           76.50m\n",
      "        16         174.3401           31.14m\n",
      "        14         174.3507           78.40m\n",
      "        16         139.6537           31.06m\n",
      "        16         154.1893           30.10m\n",
      "        15         177.7709           74.49m\n",
      "        19         177.7508           26.75m\n",
      "         3         196.6952           26.44m\n",
      "        11         143.3940           84.62m\n",
      "         9         154.4246           92.99m\n",
      "        16          82.5484           76.18m\n",
      "        17         174.3375           30.29m\n",
      "        17         139.6400           30.05m\n",
      "        15         174.3450           77.83m\n",
      "        17         154.1849           29.38m\n",
      "        12         139.9364           82.91m\n",
      "        20         177.7499           25.99m\n",
      "        16         177.7666           74.58m\n",
      "         4         126.1424           32.49m\n",
      "        17          82.5430           76.03m\n",
      "[CV 1/5; 10/36] END learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=100;, score=-51.993 total time=15.6min\n",
      "[CV 2/5; 11/36] START learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        18         174.3323           29.37m\n",
      "         1         699.1734            6.92m\n",
      "        10         154.3285           94.47m\n",
      "        16         174.3401           77.12m\n",
      "        18         154.1826           28.43m\n",
      "        18         139.6303           29.57m\n",
      "        13         139.7583           81.60m\n",
      "        21         177.7494           25.29m\n",
      "        17         177.7587           74.25m\n",
      "        19         174.2995           28.15m\n",
      "         5         106.2619           34.45m\n",
      "         2         378.7326           22.04m\n",
      "        17         174.3375           76.13m\n",
      "        19         154.1809           27.65m\n",
      "        11         154.2828           93.15m\n",
      "        19         139.6245           28.65m\n",
      "[CV 4/5; 9/36] END learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=50;, score=-46.868 total time=17.6min\n",
      "[CV 3/5; 11/36] START learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1111.3348            6.27m\n",
      "        22         177.7489           24.39m\n",
      "        14         139.6832           80.99m\n",
      "        18         177.7535           74.56m\n",
      "        20         174.2972           27.43m\n",
      "         3         298.8063           28.61m\n",
      "         6          96.3411           35.33m\n",
      "[CV 1/5; 11/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=50;, score=-51.958 total time= 4.8min\n",
      "[CV 4/5; 11/36] START learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1192.2830            6.39m\n",
      "        18         174.3323           75.18m\n",
      "         2         644.8129           20.26m\n",
      "        20         154.1787           26.96m\n",
      "        23         177.7488           23.50m\n",
      "        12         154.2537           92.98m\n",
      "        15         139.6659           80.71m\n",
      "        19         177.7508           73.12m\n",
      "         4         247.4551           31.44m\n",
      "        19         174.2995           73.44m\n",
      "         2         787.5872           22.24m\n",
      "        21         174.2957           26.63m\n",
      "         3         531.5661           28.80m\n",
      "        24         177.7486           22.59m\n",
      "        21         154.1781           26.15m\n",
      "        16         139.6537           79.67m\n",
      "        13         154.2306           92.33m\n",
      "         3         299.8188           26.44m\n",
      "        20         177.7499           72.81m\n",
      "         5         233.5944           33.79m\n",
      "        20         174.2972           72.98m\n",
      "        22         174.2949           25.75m\n",
      "        25         177.7485           21.67m\n",
      "         4         214.8572           31.87m\n",
      "        22         154.1777           25.30m\n",
      "        17         139.6400           78.44m\n",
      "         4         233.2706           31.52m\n",
      "         6         210.6796           34.15m\n",
      "        26         177.7484           20.70m\n",
      "        21         177.7494           72.55m\n",
      "        14         154.2115           92.70m\n",
      "        21         174.2957           72.46m\n",
      "        23         174.2943           25.00m\n",
      "        23         154.1773           24.19m\n",
      "         5         200.5369           33.45m\n",
      "        27         177.7484           19.82m\n",
      "        18         139.6303           78.93m\n",
      "        22         177.7489           71.53m\n",
      "         7         203.2062           35.00m\n",
      "         5         207.4232           34.80m\n",
      "        24         174.2939           24.02m\n",
      "        24         154.1772           23.27m\n",
      "        22         174.2949           71.69m\n",
      "        15         154.1956           91.02m\n",
      "         6         191.7448           34.43m\n",
      "        28         177.7483           18.92m\n",
      "        23         177.7488           70.74m\n",
      "        19         139.6245           78.04m\n",
      "[CV 4/5; 10/36] END learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=100;, score=-46.868 total time=18.3min\n",
      "[CV 5/5; 11/36] START learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        25         154.1770           22.34m\n",
      "        25         174.2934           23.12m\n",
      "        16         154.1893           89.21m\n",
      "         1         969.5931            6.11m\n",
      "         8         193.5360           35.31m\n",
      "         6         177.6500           36.21m\n",
      "        23         174.2943           71.26m\n",
      "         7         186.3624           34.92m\n",
      "        29         177.7483           17.89m\n",
      "        26         154.1768           21.33m\n",
      "        24         177.7486           69.68m\n",
      "         9         190.0154           33.91m\n",
      "         2         360.0696           21.80m\n",
      "        26         174.2930           22.26m\n",
      "        24         174.2939           70.18m\n",
      "        30         177.7482           17.02m\n",
      "        17         154.1849           88.60m\n",
      "         8         182.8137           34.76m\n",
      "         7         154.6600           37.20m\n",
      "        27         154.1767           20.44m\n",
      "        25         177.7485           68.52m\n",
      "        10         185.7642           33.24m\n",
      "         3         245.6695           31.35m\n",
      "        25         174.2934           69.31m\n",
      "        27         174.2928           21.40m\n",
      "        31         177.7482           16.17m\n",
      "         9         180.8786           34.30m\n",
      "        18         154.1826           87.15m\n",
      "         8         150.4467           37.05m\n",
      "        26         177.7484           67.16m\n",
      "        28         154.1767           19.57m\n",
      "        11         184.5194           33.13m\n",
      "        32         177.7482           15.30m\n",
      "         4         194.3695           33.67m\n",
      "        26         174.2930           68.64m\n",
      "        10         179.6730           33.66m\n",
      "        28         174.2927           20.58m\n",
      "        19         154.1809           86.09m\n",
      "        27         177.7484           66.29m\n",
      "         9         148.2762           37.41m\n",
      "        29         154.1766           18.74m\n",
      "        12         181.8811           32.48m\n",
      "        33         177.7482           14.48m\n",
      "         5         177.4469           34.50m\n",
      "        11         177.1007           32.83m\n",
      "        27         174.2928           67.97m\n",
      "        29         174.2920           19.68m\n",
      "[CV 3/5; 9/36] END learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=50;, score=-44.447 total time=27.2min\n",
      "[CV 1/5; 12/36] START learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        28         177.7483           65.34m\n",
      "         1         956.5110           13.54m\n",
      "        30         154.1765           17.76m\n",
      "        10         145.6428           36.68m\n",
      "        20         154.1787           85.36m\n",
      "        34         177.7482           13.62m\n",
      "        13         181.2971           32.18m\n",
      "        12         176.6537           31.81m\n",
      "         6         168.7613           35.84m\n",
      "         2         290.5836           36.94m\n",
      "        29         177.7483           63.97m\n",
      "        28         174.2927           67.39m\n",
      "        31         154.1765           16.97m\n",
      "        11         144.1171           36.10m\n",
      "        21         154.1781           84.27m\n",
      "        35         177.7482           12.84m\n",
      "[CV 2/5; 9/36] END learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=50;, score=-55.597 total time=30.0min\n",
      "[CV 2/5; 12/36] START learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        13         175.9892           31.29m\n",
      "         7         163.7099           35.21m\n",
      "[CV 5/5; 11/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=50;, score=-53.339 total time= 5.7min\n",
      "[CV 3/5; 12/36] START learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        14         180.1362           31.66m\n",
      "        30         177.7482           62.77m\n",
      "         1         699.1734           14.22m\n",
      "         3         196.6952           54.10m\n",
      "         1        1111.3348           12.14m\n",
      "        29         174.2920           66.61m\n",
      "[CV 3/5; 10/36] END learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=100;, score=-44.447 total time=27.2min\n",
      "[CV 4/5; 12/36] START learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1192.2830           13.44m\n",
      "        32         154.1765           16.15m\n",
      "        12         143.4637           35.53m\n",
      "        14         175.4782           30.33m\n",
      "         2         378.7326           44.61m\n",
      "         2         644.8129           41.46m\n",
      "        22         154.1777           83.21m\n",
      "        31         177.7482           61.82m\n",
      "        15         179.8623           30.86m\n",
      "         4         126.1424           66.17m\n",
      "         2         787.5872           44.97m\n",
      "        33         154.1764           15.24m\n",
      "        15         175.1773           29.54m\n",
      "        13         142.4332           34.51m\n",
      "        23         154.1773           81.59m\n",
      "         3         298.8063           58.99m\n",
      "        32         177.7482           60.88m\n",
      "         3         531.5661           59.16m\n",
      "        16         179.0320           30.06m\n",
      "[CV 2/5; 11/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=50;, score=-56.098 total time=14.2min\n",
      "[CV 5/5; 12/36] START learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         969.5931           14.64m\n",
      "         3         299.8188           54.02m\n",
      "         5         106.2619           71.28m\n",
      "        34         154.1764           14.35m\n",
      "        16         174.9964           28.70m\n",
      "        14         142.2213           33.54m\n",
      "         4         247.4551           65.72m\n",
      "        33         177.7482           60.11m\n",
      "         4         214.8572           66.20m\n",
      "        24         154.1772           80.48m\n",
      "         2         360.0696           47.44m\n",
      "         4         233.2706           65.19m\n",
      "         6          96.3411           74.31m\n",
      "[CV 1/5; 12/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100;, score=-51.958 total time= 4.8min\n",
      "[CV 1/5; 13/36] START learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        35         154.1764           13.39m\n",
      "[CV 5/5; 9/36] END learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=50;, score=-47.505 total time=31.3min\n",
      "[CV 2/5; 13/36] START learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1157.8371            6.48m\n",
      "         1         915.4315            6.84m\n",
      "        17         174.8525           27.93m\n",
      "         2         859.9639            6.04m\n",
      "         2         641.3549            6.65m\n",
      "        34         177.7482           59.14m\n",
      "         3         732.4561            6.97m\n",
      "         3         494.7076            6.73m\n",
      "         5         233.5944           71.22m\n",
      "         5         200.5369           70.25m\n",
      "        15         141.3932           33.13m\n",
      "[CV 4/5; 11/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=50;, score=-46.631 total time=14.2min\n",
      "[CV 3/5; 13/36] START learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         4         356.8313            7.17m\n",
      "         4         375.9848            7.17m\n",
      "        25         154.1770           79.27m\n",
      "         1        1359.4267            5.39m\n",
      "         3         245.6695           65.75m\n",
      "         2         902.0501            6.03m\n",
      "         5         179.9537            7.46m\n",
      "         5         325.9125            8.13m\n",
      "         3         714.6164            6.77m\n",
      "         6         132.7279            7.54m\n",
      "        18         174.7630           27.08m\n",
      "         5         207.4232           72.46m\n",
      "         6         247.2114            8.75m\n",
      "         4         431.7248            7.35m\n",
      "         7         118.2119            7.90m\n",
      "         6         210.6796           72.10m\n",
      "         5         285.4033            8.05m\n",
      "         6         191.7448           72.86m\n",
      "        35         177.7482           58.60m\n",
      "[CV 2/5; 10/36] END learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=100;, score=-55.597 total time=31.6min\n",
      "[CV 4/5; 13/36] START learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "         7         217.1005            9.17m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         8         111.7377            8.07m\n",
      "        26         154.1768           77.83m\n",
      "         4         194.3695           69.84m\n",
      "         1        1284.5567            5.88m\n",
      "         6         222.5558            8.64m\n",
      "         2         981.6572            6.06m\n",
      "        19         174.6855           26.12m\n",
      "         8         199.4042            9.41m\n",
      "         9         104.3834            8.36m\n",
      "         3         700.9753            6.75m\n",
      "         7         199.0928            8.81m\n",
      "         9         193.4035            9.66m\n",
      "        10          99.7301            8.57m\n",
      "         6         177.6500           76.47m\n",
      "         4         594.8918            7.39m\n",
      "         7         203.2062           73.35m\n",
      "         8         190.5993            9.30m\n",
      "         5         533.1360            8.16m\n",
      "        11          89.4515            8.72m\n",
      "        10         186.6644            9.84m\n",
      "         7         186.3624           75.01m\n",
      "         5         177.4469           72.96m\n",
      "        27         154.1767           76.77m\n",
      "         9         185.8668            9.54m\n",
      "        12          86.1629            8.71m\n",
      "[CV 1/5; 13/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=50;, score=-49.729 total time= 2.8min\n",
      "[CV 5/5; 13/36] START learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "         6         277.6513            9.09m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        11         184.0750            9.82m\n",
      "         1        1187.5662            5.83m\n",
      "        20         174.6161           25.63m\n",
      "        10         180.1285            9.45m\n",
      "         2         816.4563            5.82m\n",
      "         7         202.8887            9.64m\n",
      "        12         182.6224            9.79m\n",
      "         3         381.2542            6.07m\n",
      "        11         178.4258            9.34m\n",
      "[CV 3/5; 13/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=50;, score=-36.716 total time= 2.6min\n",
      "[CV 1/5; 14/36] START learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         7         154.6600           79.73m\n",
      "         8         193.5360           75.01m\n",
      "         4         260.1737            6.55m\n",
      "         1        1157.8371           12.73m\n",
      "         8         182.8137           75.56m\n",
      "         8         190.0230           10.06m\n",
      "        13         180.7697            9.72m\n",
      "         2         859.9639           12.35m\n",
      "         5         209.4418            7.05m\n",
      "         6         168.7613           77.06m\n",
      "         3         732.4561           14.27m\n",
      "        28         154.1767           75.84m\n",
      "        14         180.0664            9.64m\n",
      "        21         174.5483           24.83m\n",
      "         9         180.5479           10.41m\n",
      "         6         196.2222            7.77m\n",
      "         4         356.8313           15.01m\n",
      "         9         190.0154           72.96m\n",
      "         5         179.9537           16.12m\n",
      "        15         179.5036            9.48m\n",
      "         7         176.8975            8.35m\n",
      "        10         154.6899           10.43m\n",
      "         8         150.4467           80.35m\n",
      "         6         132.7279           16.39m\n",
      "         9         180.8786           75.45m\n",
      "         8         167.1490            8.58m\n",
      "        16         179.0827            9.34m\n",
      "        11         151.2019           10.39m\n",
      "[CV 4/5; 13/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=50;, score=-40.012 total time= 2.9min\n",
      "[CV 2/5; 14/36] START learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         7         163.7099           76.76m\n",
      "[CV 5/5; 12/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100;, score=-53.339 total time= 5.8min\n",
      "[CV 3/5; 14/36] START learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         7         118.2119           17.26m\n",
      "         1        1359.4267           11.90m\n",
      "         1         915.4315           13.79m\n",
      "        17         178.8308            9.06m\n",
      "         2         902.0501           12.41m\n",
      "         2         641.3549           13.74m\n",
      "         9         162.7010            9.08m\n",
      "[CV 5/5; 13/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=50;, score=-38.047 total time= 2.0min\n",
      "[CV 4/5; 14/36] START learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         8         111.7377           17.83m\n",
      "        10         185.7642           72.36m\n",
      "        29         154.1766           75.01m\n",
      "        22         174.5003           24.28m\n",
      "[CV 3/5; 11/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=50;, score=-39.315 total time=19.1min\n",
      "[CV 5/5; 14/36] START learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "         1        1284.5567           12.46m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         3         714.6164           13.41m\n",
      "         3         494.7076           13.96m\n",
      "        18         178.6088            8.81m\n",
      "[CV 2/5; 13/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=50;, score=-53.904 total time= 5.0min\n",
      "[CV 1/5; 15/36] START learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1187.5662           12.00m\n",
      "         2         981.6572           13.12m\n",
      "         9         104.3834           18.65m\n",
      "         4         375.9848           15.10m\n",
      "         1        1227.1204            6.19m\n",
      "         4         431.7248           15.46m\n",
      "         2         816.4563           11.74m\n",
      "        10         179.6730           74.70m\n",
      "         3         700.9753           13.95m\n",
      "         2         917.8198            5.88m\n",
      "         3         381.2542           12.15m\n",
      "        10          99.7301           19.19m\n",
      "         5         325.9125           17.02m\n",
      "         9         148.2762           81.77m\n",
      "         4         594.8918           14.82m\n",
      "         5         285.4033           17.72m\n",
      "         3         791.1197            7.29m\n",
      "         4         260.1737           13.41m\n",
      "         5         533.1360           16.16m\n",
      "         4         400.7525            7.55m\n",
      "        11          89.4515           19.72m\n",
      "         5         209.4418           14.67m\n",
      "         6         247.2114           18.82m\n",
      "         6         222.5558           19.01m\n",
      "        30         154.1765           73.52m\n",
      "        11         184.5194           72.81m\n",
      "         5         227.2499            8.06m\n",
      "         6         277.6513           17.84m\n",
      "         6         196.2222           16.56m\n",
      "         7         199.0928           19.55m\n",
      "        12          86.1629           20.03m\n",
      "[CV 1/5; 14/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100;, score=-49.729 total time= 2.7min\n",
      "[CV 2/5; 15/36] START learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        11         177.1007           73.73m\n",
      "         7         217.1005           20.71m\n",
      "         1         989.0025            6.84m\n",
      "         6         178.8986            8.57m\n",
      "         7         202.8887           19.03m\n",
      "         2         692.1903            6.72m\n",
      "         7         176.8975           18.06m\n",
      "         8         190.5993           20.82m\n",
      "        10         145.6428           81.35m\n",
      "         8         199.4042           22.04m\n",
      "         3         525.9220            7.47m\n",
      "         7         142.4172            9.27m\n",
      "         8         190.0230           20.28m\n",
      "         8         167.1490           18.87m\n",
      "         9         185.8668           21.59m\n",
      "         4         461.2762            8.04m\n",
      "        12         181.8811           72.62m\n",
      "         9         193.4035           22.92m\n",
      "        12         176.6537           72.54m\n",
      "         8         126.5825            9.34m\n",
      "         9         162.7010           19.76m\n",
      "         9         180.5479           21.05m\n",
      "[CV 5/5; 14/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100;, score=-38.047 total time= 2.0min\n",
      "[CV 3/5; 15/36] START learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        10         180.1285           21.88m\n",
      "         5         327.6415            8.73m\n",
      "         1        1413.5889            5.80m\n",
      "        31         154.1765           72.92m\n",
      "        10         186.6644           23.41m\n",
      "         9         115.9551            9.54m\n",
      "         2        1023.9594            6.20m\n",
      "        10         154.6899           21.17m\n",
      "        11         178.4258           22.12m\n",
      "[CV 3/5; 14/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100;, score=-36.716 total time= 2.7min\n",
      "[CV 4/5; 15/36] START learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         6         276.1798            9.39m\n",
      "        11         144.1171           81.09m\n",
      "         3         814.5446            7.33m\n",
      "         1        1452.6931            6.86m\n",
      "        11         184.0750           23.50m\n",
      "        11         151.2019           21.65m\n",
      "[CV 4/5; 14/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100;, score=-40.012 total time= 2.7min\n",
      "[CV 5/5; 15/36] START learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "        10         109.6610            9.84m\n",
      "[CV 1/5; 15/36] END learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=50;, score=-48.359 total time= 2.5min\n",
      "[CV 1/5; 16/36] START learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2        1190.8852            6.69m\n",
      "         7         246.1119            9.73m\n",
      "         4         526.6922            7.95m\n",
      "         1        1275.7682            5.76m\n",
      "         1        1227.1204           12.76m\n",
      "        13         175.9892           72.51m\n",
      "         3         999.0454            7.25m\n",
      "        13         181.2971           73.03m\n",
      "         2         980.0788            5.92m\n",
      "         2         917.8198           12.45m\n",
      "        12         182.6224           23.79m\n",
      "         5         326.4626            8.51m\n",
      "         8         230.5476           10.03m\n",
      "         3         565.7503            7.03m\n",
      "         3         791.1197           15.48m\n",
      "         4         803.3179            8.40m\n",
      "        13         180.7697           23.90m\n",
      "         6         268.2726            8.78m\n",
      "         4         350.2918            7.43m\n",
      "         4         400.7525           16.20m\n",
      "        32         154.1765           72.21m\n",
      "         5         437.3033            8.99m\n",
      "        12         143.4637           80.46m\n",
      "         9         218.5263           10.43m\n",
      "         5         302.8383            8.03m\n",
      "         7         245.2019            9.17m\n",
      "         5         227.2499           17.37m\n",
      "        14         180.0664           24.03m\n",
      "        14         175.4782           71.42m\n",
      "        10         211.5421           10.33m\n",
      "        14         180.1362           72.37m\n",
      "         6         322.8646           10.05m\n",
      "         6         262.9385            8.67m\n",
      "         6         178.8986           18.61m\n",
      "         8         223.5101            9.52m\n",
      "        15         179.5036           23.92m\n",
      "        11         205.3899           10.25m\n",
      "         7         245.6358           10.46m\n",
      "         7         240.2739            9.18m\n",
      "         7         142.4172           20.48m\n",
      "        16         179.0827           23.85m\n",
      "         9         211.4971            9.96m\n",
      "        13         142.4332           79.46m\n",
      "         8         215.8971            9.31m\n",
      "        12         200.1430           10.35m\n",
      "         8         209.4507           10.63m\n",
      "        33         154.1764           71.14m\n",
      "        17         178.8308           23.47m\n",
      "         8         126.5825           21.20m\n",
      "        15         175.1773           70.74m\n",
      "        10         205.0915            9.97m\n",
      "        15         179.8623           71.63m\n",
      "         9         178.3414           10.46m\n",
      "        13         196.4582           10.21m\n",
      "        18         178.6088           23.23m\n",
      "[CV 2/5; 14/36] END learning_rate=0.5, max_depth=50, min_samples_split=10, n_estimators=100;, score=-53.904 total time= 5.1min\n",
      "[CV 2/5; 16/36] START learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         9         204.2540            9.77m\n",
      "         9         115.9551           22.10m\n",
      "        11         201.2523            9.96m\n",
      "         1         989.0025           13.49m\n",
      "         2         692.1903           13.55m\n",
      "        10         169.2210           10.59m\n",
      "        10         187.8246            9.86m\n",
      "        14         193.0711           10.22m\n",
      "        12         196.7209            9.93m\n",
      "        10         109.6610           22.98m\n",
      "[CV 1/5; 16/36] END learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=100;, score=-48.359 total time= 2.6min\n",
      "[CV 3/5; 16/36] START learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        14         142.2213           78.34m\n",
      "         3         525.9220           15.33m\n",
      "         1        1413.5889           12.18m\n",
      "        16         174.9964           69.96m\n",
      "        11         159.7384           10.52m\n",
      "        11         183.9442            9.90m\n",
      "         4         461.2762           16.44m\n",
      "         2        1023.9594           12.73m\n",
      "        16         179.0320           70.88m\n",
      "[CV 2/5; 12/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100;, score=-56.098 total time=13.5min\n",
      "[CV 4/5; 16/36] START learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        15         190.6391           10.15m\n",
      "        13         192.4382            9.89m\n",
      "        34         154.1764           70.11m\n",
      "         1        1452.6931           12.38m\n",
      "         3         814.5446           14.89m\n",
      "         5         327.6415           18.09m\n",
      "         2        1190.8852           12.40m\n",
      "        12         157.2401           10.52m\n",
      "[CV 4/5; 15/36] END learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=50;, score=-39.480 total time= 3.3min\n",
      "[CV 5/5; 16/36] START learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        12         178.3603            9.85m\n",
      "        14         190.5310            9.81m\n",
      "         1        1275.7682           11.88m\n",
      "         4         526.6922           16.78m\n",
      "        16         189.4588           10.11m\n",
      "         3         999.0454           14.11m\n",
      "         2         980.0788           11.87m\n",
      "         6         276.1798           19.51m\n",
      "        13         171.5035            9.73m\n",
      "        15         188.4803            9.64m\n",
      "         4         803.3179           16.81m\n",
      "         3         565.7503           14.21m\n",
      "         5         326.4626           18.72m\n",
      "        17         174.8525           69.24m\n",
      "        17         187.2720            9.85m\n",
      "        15         141.3932           78.24m\n",
      "[CV 4/5; 12/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100;, score=-46.631 total time=13.8min\n",
      "[CV 1/5; 17/36] START learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         7         246.1119           20.35m\n",
      "         4         350.2918           15.24m\n",
      "        14         167.2801            9.65m\n",
      "         1        1156.1494            6.82m\n",
      "        35         154.1764           68.72m\n",
      "[CV 5/5; 10/36] END learning_rate=1.0, max_depth=200, min_samples_split=10, n_estimators=100;, score=-47.505 total time=37.0min\n",
      "[CV 2/5; 17/36] START learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "         5         437.3033           18.08m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        16         185.3221            9.40m\n",
      "         6         268.2726           19.90m\n",
      "        18         185.2164            9.58m\n",
      "         2         864.5132            6.55m\n",
      "         8         230.5476           20.90m\n",
      "         1         914.6719            7.10m\n",
      "         5         302.8383           16.54m\n",
      "         2         640.3757            7.12m\n",
      "         6         322.8646           19.61m\n",
      "         3         735.3244            7.19m\n",
      "        15         165.2205            9.65m\n",
      "[CV 5/5; 15/36] END learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=50;, score=-38.416 total time= 4.1min\n",
      "[CV 3/5; 17/36] START learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         7         245.2019           21.24m\n",
      "        17         184.4211            9.25m\n",
      "         3         493.0421            7.24m\n",
      "         6         262.9385           18.05m\n",
      "        19         184.4431            9.39m\n",
      "         9         218.5263           21.94m\n",
      "         1        1357.6414            7.80m\n",
      "         4         358.6620            7.90m\n",
      "        18         174.7630           68.37m\n",
      "         7         245.6358           21.13m\n",
      "         8         223.5101           21.85m\n",
      "        18         182.9237            8.99m\n",
      "         2         900.7992            8.26m\n",
      "         4         372.5468            8.18m\n",
      "         7         240.2739           19.09m\n",
      "        10         211.5421           21.98m\n",
      "         5         173.7277            8.49m\n",
      "        20         183.8031            9.19m\n",
      "         3         712.5369            8.92m\n",
      "         8         209.4507           22.09m\n",
      "         9         211.4971           22.39m\n",
      "        19         182.0672            8.77m\n",
      "         5         323.1627            9.14m\n",
      "         8         215.8971           19.96m\n",
      "        11         205.3899           22.13m\n",
      "         6         120.7735            9.07m\n",
      "         4         429.5733            9.63m\n",
      "        20         180.9392            8.44m\n",
      "         9         178.3414           22.20m\n",
      "        21         182.6003            9.04m\n",
      "        10         205.0915           22.61m\n",
      "         6         246.3000            9.86m\n",
      "        19         174.6855           67.23m\n",
      "        12         200.1430           22.57m\n",
      "         9         204.2540           21.27m\n",
      "         7         105.0847            9.73m\n",
      "         5         295.0014           10.90m\n",
      "        10         169.2210           22.86m\n",
      "        21         180.5159            8.26m\n",
      "        11         201.2523           22.96m\n",
      "        22         181.9395            8.77m\n",
      "        13         196.4582           22.76m\n",
      "        10         187.8246           21.77m\n",
      "         7         224.5948           10.87m\n",
      "         8          95.5156           10.70m\n",
      "        11         159.7384           23.16m\n",
      "        12         196.7209           23.11m\n",
      "        22         179.9686            8.04m\n",
      "        23         180.8840            8.54m\n",
      "         6         223.3817           12.19m\n",
      "        11         183.9442           22.27m\n",
      "        14         193.0711           23.12m\n",
      "         8         204.4337           11.20m\n",
      "        23         179.4418            7.79m\n",
      "        13         192.4382           23.37m\n",
      "        12         157.2401           23.68m\n",
      "[CV 4/5; 16/36] END learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=100;, score=-39.480 total time= 3.2min\n",
      "[CV 4/5; 17/36] START learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        24         180.3991            8.26m\n",
      "[CV 2/5; 15/36] END learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=50;, score=-53.557 total time= 7.6min\n",
      "[CV 5/5; 17/36] START learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1283.2201            6.74m\n",
      "        12         178.3603           22.81m\n",
      "        20         174.6161           67.36m\n",
      "         9          90.4885           11.87m\n",
      "        15         190.6391           23.45m\n",
      "         1        1186.8696            6.01m\n",
      "         2         980.3745            6.55m\n",
      "        24         178.8555            7.57m\n",
      "         7         202.7537           13.77m\n",
      "         2         815.5039            6.08m\n",
      "        14         190.5310           23.63m\n",
      "         9         191.9861           12.07m\n",
      "        13         171.5035           22.82m\n",
      "         3         698.3460            7.26m\n",
      "         3         397.5976            7.22m\n",
      "        16         189.4588           23.59m\n",
      "        25         178.4017            7.28m\n",
      "        10          87.4727           12.29m\n",
      "        15         188.4803           23.61m\n",
      "         4         593.1697            7.76m\n",
      "        14         167.2801           22.95m\n",
      "         4         274.7818            8.10m\n",
      "         8         191.4642           14.35m\n",
      "        17         187.2720           23.33m\n",
      "        10         183.5680           12.85m\n",
      "        26         177.5980            7.03m\n",
      "        16         185.3221           23.40m\n",
      "         5         532.4080            8.61m\n",
      "        21         174.5483           66.64m\n",
      "         5         223.1537            9.29m\n",
      "        15         165.2205           23.10m\n",
      "[CV 5/5; 16/36] END learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=100;, score=-38.416 total time= 4.1min\n",
      "[CV 1/5; 18/36] START learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        18         185.2164           23.20m\n",
      "        11          85.5244           12.68m\n",
      "[CV 1/5; 17/36] END learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=50;, score=-50.019 total time= 3.6min\n",
      "[CV 2/5; 18/36] START learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1156.1494           13.78m\n",
      "        17         184.4211           23.45m\n",
      "         1         914.6719           14.59m\n",
      "        27         177.1646            6.81m\n",
      "         6         272.7190            9.74m\n",
      "         2         864.5132           13.38m\n",
      "         9         184.7922           15.31m\n",
      "         2         640.3757           14.42m\n",
      "         6         208.1615           10.15m\n",
      "        11         181.1311           13.27m\n",
      "        19         184.4431           23.22m\n",
      "         3         735.3244           14.74m\n",
      "        18         182.9237           23.23m\n",
      "         3         493.0421           14.67m\n",
      "        28         176.5769            6.61m\n",
      "         7         197.7041           10.97m\n",
      "         4         358.6620           17.46m\n",
      "         4         372.5468           17.04m\n",
      "        20         183.8031           23.26m\n",
      "         7         192.3720           11.19m\n",
      "        19         182.0672           23.07m\n",
      "        12         179.5827           13.36m\n",
      "        10         181.8665           15.81m\n",
      "        29         176.3136            6.35m\n",
      "         5         173.7277           19.08m\n",
      "        22         174.5003           66.64m\n",
      "[CV 3/5; 12/36] END learning_rate=1.0, max_depth=200, min_samples_split=20, n_estimators=100;, score=-39.315 total time=18.8min\n",
      "[CV 3/5; 18/36] START learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        20         180.9392           22.64m\n",
      "         5         323.1627           19.39m\n",
      "        21         182.6003           23.28m\n",
      "         1        1357.6414           12.62m\n",
      "         8         175.9425           12.00m\n",
      "         8         177.9625           11.90m\n",
      "        30         176.0528            6.04m\n",
      "         2         900.7992           13.15m\n",
      "         6         120.7735           20.65m\n",
      "         6         246.3000           21.04m\n",
      "        21         180.5159           22.62m\n",
      "        13         178.6993           13.75m\n",
      "        22         181.9395           23.11m\n",
      "         3         712.5369           14.58m\n",
      "        11         177.7335           16.08m\n",
      "        31         175.9333            5.76m\n",
      "         9         156.2562           12.51m\n",
      "         7         105.0847           22.07m\n",
      "         4         429.5733           16.16m\n",
      "        22         179.9686           22.65m\n",
      "         9         169.6835           12.83m\n",
      "         7         224.5948           23.42m\n",
      "        23         180.8840           23.00m\n",
      "        32         175.8032            5.48m\n",
      "         5         295.0014           18.86m\n",
      "        14         178.3038           13.95m\n",
      "        23         179.4418           22.48m\n",
      "        12         176.2966           16.29m\n",
      "[CV 3/5; 17/36] END learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=50;, score=-37.448 total time= 5.2min\n",
      "[CV 4/5; 18/36] START learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "        10         148.9754           12.88m\n",
      "         8          95.5156           24.34m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        24         180.3991           22.83m\n",
      "[CV 2/5; 16/36] END learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=100;, score=-53.557 total time= 7.2min\n",
      "[CV 5/5; 18/36] START learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         8         204.4337           24.47m\n",
      "         1        1186.8696           12.17m\n",
      "         1        1283.2201           17.40m\n",
      "        10         161.1596           13.37m\n",
      "[CV 5/5; 17/36] END learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=50;, score=-37.867 total time= 3.3min\n",
      "[CV 1/5; 19/36] START learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        33         175.6964            5.22m\n",
      "         6         223.3817           21.07m\n",
      "         2         815.5039           12.54m\n",
      "        24         178.8555           22.32m\n",
      "         1        1225.8729            6.36m\n",
      "         2         980.3745           17.38m\n",
      "        15         178.1368           13.74m\n",
      "         3         397.5976           13.62m\n",
      "         2         916.6994            6.44m\n",
      "        34         175.5773            4.91m\n",
      "         9          90.4885           26.76m\n",
      "         9         191.9861           26.66m\n",
      "        11         146.0222           13.49m\n",
      "[CV 4/5; 17/36] END learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=50;, score=-40.269 total time= 3.8min\n",
      "[CV 2/5; 19/36] START learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        25         178.4017           22.01m\n",
      "         3         698.3460           19.19m\n",
      "         4         274.7818           14.82m\n",
      "         3         788.4894            7.57m\n",
      "         7         202.7537           23.95m\n",
      "         1         988.4464            6.87m\n",
      "        35         175.4680            4.61m\n",
      "         2         690.5923            6.58m\n",
      "         4         593.1697           20.50m\n",
      "         4         398.2321            8.27m\n",
      "        26         177.5980           21.79m\n",
      "         5         223.1537           17.32m\n",
      "        16         178.0413           13.78m\n",
      "        10          87.4727           27.86m\n",
      "         3         523.6431            7.58m\n",
      "         8         191.4642           25.33m\n",
      "        10         183.5680           28.80m\n",
      "        36         175.3749            4.30m\n",
      "         5         532.4080           22.59m\n",
      "        27         177.1646           21.52m\n",
      "         5         270.4194            9.53m\n",
      "         6         208.1615           19.60m\n",
      "         4         459.0801            8.31m\n",
      "        11          85.5244           29.02m\n",
      "[CV 1/5; 18/36] END learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=100;, score=-50.019 total time= 3.6min\n",
      "[CV 3/5; 19/36] START learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        37         175.2351            4.02m\n",
      "        28         176.5769           21.39m\n",
      "         1        1412.0133            5.95m\n",
      "        17         177.9995           13.80m\n",
      "         9         184.7922           27.46m\n",
      "         6         272.7190           25.64m\n",
      "        11         181.1311           30.13m\n",
      "         5         321.7008            9.73m\n",
      "         6         220.3291           10.91m\n",
      "         7         192.3720           22.48m\n",
      "         2        1022.1126            6.36m\n",
      "        38         174.9995            3.71m\n",
      "        29         176.3136           21.19m\n",
      "         3         814.7190            7.82m\n",
      "        10         181.8665           28.98m\n",
      "         6         277.4676           11.38m\n",
      "         7         194.8410           12.05m\n",
      "[CV 1/5; 19/36] END learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=50;, score=-49.018 total time= 2.0min\n",
      "[CV 4/5; 19/36] START learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "         8         177.9625           24.40m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        12         179.5827           31.08m\n",
      "        30         176.0528           20.83m\n",
      "        18         177.9090           13.69m\n",
      "         7         197.7041           29.20m\n",
      "         4         526.9101            8.48m\n",
      "        39         174.9402            3.42m\n",
      "         1        1451.7023            6.25m\n",
      "         2        1188.7077            6.59m\n",
      "        31         175.9333           20.67m\n",
      "         7         246.4511           12.52m\n",
      "         3        1016.2092            7.33m\n",
      "        40         174.8411            3.13m\n",
      "         5         346.5335           10.51m\n",
      "        11         177.7335           30.27m\n",
      "         9         169.6835           26.77m\n",
      "        19         177.8747           13.50m\n",
      "         8         175.9425           32.24m\n",
      "        13         178.6993           32.71m\n",
      "        32         175.8032           20.38m\n",
      "        41         174.7350            2.81m\n",
      "         4         816.3641            8.94m\n",
      "         6         268.3764           11.71m\n",
      "         8         230.3024           13.18m\n",
      "        12         176.2966           31.45m\n",
      "[CV 3/5; 18/36] END learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=100;, score=-37.448 total time= 4.3min\n",
      "[CV 5/5; 19/36] START learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        42         174.6742            2.49m\n",
      "        10         161.1596           28.74m\n",
      "[CV 5/5; 18/36] END learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=100;, score=-37.867 total time= 3.2min\n",
      "[CV 1/5; 20/36] START learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "        33         175.6964           20.24m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         5         410.0088           10.48m\n",
      "         1        1275.2050            6.61m\n",
      "         9         156.2562           33.89m\n",
      "         1        1225.8729           13.27m\n",
      "        20         177.8544           13.33m\n",
      "        14         178.3038           33.77m\n",
      "         2         978.4638            6.66m\n",
      "         2         916.6994           13.50m\n",
      "        43         174.6419            2.18m\n",
      "        34         175.5773           19.95m\n",
      "         7         253.2062           12.73m\n",
      "         9         218.0312           13.78m\n",
      "         3         563.7282            7.40m\n",
      "         6         286.6350           11.41m\n",
      "         3         788.4894           16.13m\n",
      "        21         177.8310           12.94m\n",
      "        35         175.4680           19.69m\n",
      "        44         174.6179            1.88m\n",
      "         4         344.5842            7.93m\n",
      "        15         178.1368           33.84m\n",
      "        10         148.9754           35.05m\n",
      "         4         398.2321           17.70m\n",
      "        10         211.1433           13.97m\n",
      "         8         232.0528           13.66m\n",
      "        45         174.5785            1.56m\n",
      "        36         175.3749           19.42m\n",
      "         7         236.7547           12.62m\n",
      "         5         297.0384            9.26m\n",
      "         5         270.4194           19.97m\n",
      "        22         177.8023           12.64m\n",
      "        46         174.5450            1.25m\n",
      "        16         178.0413           34.69m\n",
      "        37         175.2351           19.25m\n",
      "         9         219.4185           14.11m\n",
      "        11         204.5218           14.18m\n",
      "        11         146.0222           37.38m\n",
      "[CV 4/5; 18/36] END learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=100;, score=-40.269 total time= 4.6min\n",
      "[CV 2/5; 20/36] START learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         6         260.0547           10.81m\n",
      "        47         174.5209           55.97s\n",
      "         6         220.3291           23.39m\n",
      "         8         213.8547           13.78m\n",
      "         1         988.4464           17.83m\n",
      "        38         174.9995           18.99m\n",
      "         2         690.5923           17.75m\n",
      "        23         177.7938           12.37m\n",
      "        12         195.6629           14.32m\n",
      "        48         174.5030           37.51s\n",
      "        10         207.7656           14.53m\n",
      "         7         219.5600           11.89m\n",
      "        17         177.9995           35.38m\n",
      "         7         194.8410           25.74m\n",
      "[CV 1/5; 20/36] END learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=100;, score=-49.018 total time= 1.9min\n",
      "[CV 3/5; 20/36] START learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        39         174.9402           18.75m\n",
      "         3         523.6431           20.08m\n",
      "         1        1412.0133           12.28m\n",
      "         9         194.9385           14.56m\n",
      "        49         174.4783           18.77s\n",
      "         2        1022.1126           13.36m\n",
      "         4         459.0801           22.47m\n",
      "        11         199.1878           14.57m\n",
      "         8         207.9879           12.69m\n",
      "        40         174.8411           18.55m\n",
      "        24         177.7854           12.07m\n",
      "        13         193.2716           14.39m\n",
      "         3         814.7190           16.31m\n",
      "        50         174.4603            0.00s\n",
      "[CV 3/5; 15/36] END learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=50;, score=-36.029 total time=15.6min\n",
      "[CV 4/5; 20/36] START learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        18         177.9090           35.71m\n",
      "        41         174.7350           18.14m\n",
      "         1        1451.7023           13.28m\n",
      "        10         186.0972           14.89m\n",
      "         4         526.9101           17.90m\n",
      "         5         321.7008           25.73m\n",
      "         2        1188.7077           13.52m\n",
      "        12         194.1844           14.64m\n",
      "        25         177.7764           11.66m\n",
      "        42         174.6742           17.83m\n",
      "         9         190.7044           13.59m\n",
      "        14         190.6340           14.40m\n",
      "         3        1016.2092           15.51m\n",
      "        19         177.8747           35.81m\n",
      "         5         346.5335           22.28m\n",
      "        11         176.7859           15.34m\n",
      "        43         174.6419           17.56m\n",
      "         6         277.4676           29.84m\n",
      "         4         816.3641           20.07m\n",
      "        13         191.5012           14.80m\n",
      "        10         176.0276           14.25m\n",
      "        26         177.7692           11.33m\n",
      "         6         268.3764           25.19m\n",
      "        15         188.5478           14.49m\n",
      "        44         174.6179           17.30m\n",
      "        20         177.8544           36.11m\n",
      "         5         410.0088           23.74m\n",
      "        12         173.8712           15.44m\n",
      "        45         174.5785           16.92m\n",
      "         7         246.4511           33.13m\n",
      "         7         253.2062           27.78m\n",
      "        14         189.3940           14.91m\n",
      "        11         171.4810           14.61m\n",
      "        16         185.8985           14.45m\n",
      "        46         174.5450           16.60m\n",
      "         6         286.6350           26.19m\n",
      "        21         177.8310           35.82m\n",
      "        27         177.7671           11.05m\n",
      "        13         156.3314           15.47m\n",
      "        47         174.5209           16.22m\n",
      "         8         230.3024           35.13m\n",
      "        15         185.6229           14.64m\n",
      "         8         232.0528           30.40m\n",
      "        12         165.6874           14.78m\n",
      "[CV 5/5; 19/36] END learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=50;, score=-38.262 total time= 4.7min\n",
      "[CV 5/5; 20/36] START learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         7         236.7547           29.31m\n",
      "        28         177.7632           10.58m\n",
      "        48         174.5030           15.96m\n",
      "         1        1275.2050           12.30m\n",
      "        22         177.8023           35.76m\n",
      "        17         184.5724           14.41m\n",
      "         2         978.4638           12.47m\n",
      "        14         152.9110           15.56m\n",
      "        16         184.1271           14.46m\n",
      "        49         174.4783           15.65m\n",
      "         9         218.0312           37.16m\n",
      "         9         219.4185           31.76m\n",
      "         3         563.7282           14.48m\n",
      "        29         177.7613           10.13m\n",
      "         8         213.8547           32.13m\n",
      "         4         344.5842           16.04m\n",
      "        18         183.6716           14.24m\n",
      "        50         174.4603           15.35m\n",
      "        23         177.7938           36.05m\n",
      "        15         150.4071           15.58m\n",
      "         5         297.0384           18.64m\n",
      "        10         207.7656           33.16m\n",
      "        10         211.1433           38.24m\n",
      "        17         183.1347           14.41m\n",
      "        51         174.4274           15.07m\n",
      "        30         177.7579            9.65m\n",
      "        19         182.6669           13.82m\n",
      "         9         194.9385           34.65m\n",
      "         6         260.0547           22.00m\n",
      "        52         174.4109           14.82m\n",
      "        24         177.7854           36.14m\n",
      "        11         199.1878           33.89m\n",
      "        16         148.2906           15.42m\n",
      "        31         177.7554            9.19m\n",
      "        11         204.5218           39.42m\n",
      "        18         182.4053           14.46m\n",
      "        53         174.3984           14.53m\n",
      "        20         181.8034           13.62m\n",
      "        10         186.0972           35.75m\n",
      "         7         219.5600           24.64m\n",
      "        25         177.7764           35.91m\n",
      "        12         194.1844           34.59m\n",
      "        17         147.4791           15.12m\n",
      "        54         174.3897           14.22m\n",
      "        32         177.7533            8.77m\n",
      "[CV 2/5; 17/36] END learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=50;, score=-54.266 total time=15.6min\n",
      "[CV 1/5; 21/36] START learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        21         181.1061           13.16m\n",
      "        12         195.6629           40.27m\n",
      "         8         207.9879           26.68m\n",
      "        55         174.3769           13.87m\n",
      "         1        1156.1493            6.59m\n",
      "        19         180.8491           14.31m\n",
      "        11         176.7859           37.06m\n",
      "         2         864.5130            6.46m\n",
      "        13         191.5012           35.44m\n",
      "        18         146.8002           14.87m\n",
      "         3         735.3201            7.24m\n",
      "        26         177.7692           35.97m\n",
      "        56         174.3699           13.60m\n",
      "        22         180.6553           12.74m\n",
      "         9         190.7044           28.96m\n",
      "        20         180.1666           13.90m\n",
      "        13         193.2716           40.94m\n",
      "         4         358.5857            7.98m\n",
      "        12         173.8712           37.56m\n",
      "        57         174.3656           13.33m\n",
      "[CV 3/5; 16/36] END learning_rate=0.5, max_depth=50, min_samples_split=20, n_estimators=100;, score=-36.027 total time=17.7min\n",
      "[CV 2/5; 21/36] START learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         5         173.6068            8.78m\n",
      "        14         189.3940           36.17m\n",
      "         1         914.6717            6.89m\n",
      "        19         146.1753           14.62m\n",
      "        23         180.2776           12.40m\n",
      "[CV 2/5; 19/36] END learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=50;, score=-53.802 total time=10.6min\n",
      "[CV 3/5; 21/36] START learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2         640.3730            7.07m\n",
      "        27         177.7671           36.05m\n",
      "        21         178.1533           13.52m\n",
      "        10         176.0276           30.58m\n",
      "         1        1357.6412            6.18m\n",
      "        14         190.6340           41.20m\n",
      "         6         120.6587            9.30m\n",
      "         3         493.0403            7.08m\n",
      "        13         156.3314           37.86m\n",
      "         2         900.7998            6.41m\n",
      "        15         185.6229           36.01m\n",
      "         3         712.5400            7.01m\n",
      "         4         372.4760            8.39m\n",
      "        20         142.8544           14.35m\n",
      "[CV 4/5; 19/36] END learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=50;, score=-43.742 total time= 9.6min\n",
      "[CV 4/5; 21/36] START learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        28         177.7632           35.59m\n",
      "         7         105.0430           10.37m\n",
      "        11         171.4810           31.67m\n",
      "         4         429.5634            7.76m\n",
      "        22         177.4501           13.21m\n",
      "         1        1283.2180            6.35m\n",
      "         5         323.0610            9.39m\n",
      "         2         980.3665            6.59m\n",
      "        15         188.5478           41.88m\n",
      "        14         152.9110           38.71m\n",
      "        16         184.1271           36.07m\n",
      "         5         295.0063            8.76m\n",
      "         3         698.3073            7.42m\n",
      "         8          96.3538           11.51m\n",
      "        29         177.7613           35.21m\n",
      "        12         165.6874           32.83m\n",
      "[CV 5/5; 20/36] END learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=100;, score=-38.262 total time= 4.5min\n",
      "[CV 5/5; 21/36] START learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "         6         245.9097           10.38m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        23         176.7686           12.80m\n",
      "         4         593.1663            8.04m\n",
      "         1        1186.8690            5.98m\n",
      "         6         223.3224            9.75m\n",
      "         2         815.6535            6.20m\n",
      "        17         183.1347           36.45m\n",
      "        16         185.8985           42.14m\n",
      "        15         150.4071           39.35m\n",
      "         5         535.0083            8.80m\n",
      "        30         177.7579           34.63m\n",
      "         7         224.8234           11.27m\n",
      "         3         397.7426            6.74m\n",
      "        24         176.4506           12.34m\n",
      "         9          90.9928           12.89m\n",
      "         7         202.7124           11.04m\n",
      "         4         274.9137            7.45m\n",
      "         6         273.8469            9.75m\n",
      "        18         182.4053           36.66m\n",
      "        31         177.7554           34.15m\n",
      "        16         148.2906           39.48m\n",
      "        17         184.5724           42.21m\n",
      "         8         202.8372           12.38m\n",
      "         5         223.1412            9.10m\n",
      "        25         175.6893           11.92m\n",
      "        10          86.7487           13.54m\n",
      "         8         191.1426           12.25m\n",
      "         7         198.4474           11.24m\n",
      "         6         201.5231           10.44m\n",
      "        19         180.8491           36.74m\n",
      "        17         147.4791           39.41m\n",
      "        32         177.7533           33.82m\n",
      "[CV 2/5; 18/36] END learning_rate=0.5, max_depth=100, min_samples_split=10, n_estimators=100;, score=-54.266 total time=15.9min\n",
      "[CV 1/5; 22/36] START learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        26         175.3621           11.42m\n",
      "         9         191.7552           13.28m\n",
      "        18         183.6716           42.47m\n",
      "         1        1156.1493           13.48m\n",
      "         9         187.7296           13.42m\n",
      "         2         864.5130           14.09m\n",
      "         8         176.5286           12.65m\n",
      "         7         184.6429           11.39m\n",
      "        11          85.0562           14.87m\n",
      "[CV 1/5; 21/36] END learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=50;, score=-50.626 total time= 4.2min\n",
      "[CV 2/5; 22/36] START learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "        20         180.1666           36.43m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         3         735.3201           15.83m\n",
      "        18         146.8002           39.22m\n",
      "        27         175.2043           10.99m\n",
      "[CV 3/5; 19/36] END learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=50;, score=-36.932 total time=12.9min\n",
      "[CV 3/5; 22/36] START learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         914.6717           14.12m\n",
      "        10         183.8384           14.05m\n",
      "        19         182.6669           41.96m\n",
      "         1        1357.6412           11.84m\n",
      "         4         358.5857           17.15m\n",
      "         2         640.3730           14.02m\n",
      "         2         900.7998           12.81m\n",
      "        10         181.4472           14.30m\n",
      "         3         493.0403           14.63m\n",
      "         8         170.1553           12.77m\n",
      "        21         178.1533           36.16m\n",
      "         9         154.9312           14.11m\n",
      "        19         146.1753           38.74m\n",
      "         3         712.5400           14.83m\n",
      "         5         173.6068           19.05m\n",
      "         4         372.4760           17.04m\n",
      "        11         181.0284           14.41m\n",
      "         4         429.5634           17.56m\n",
      "        20         181.8034           41.98m\n",
      "         6         120.6587           20.29m\n",
      "         5         323.0610           19.58m\n",
      "        22         177.4501           36.17m\n",
      "        20         142.8544           38.67m\n",
      "[CV 4/5; 20/36] END learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=100;, score=-43.742 total time= 9.7min\n",
      "[CV 4/5; 22/36] START learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         9         164.4852           13.97m\n",
      "[CV 5/5; 21/36] END learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=50;, score=-37.660 total time= 3.1min\n",
      "[CV 5/5; 22/36] START learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "         5         295.0063           19.84m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        11         178.4242           15.20m\n",
      "         1        1283.2180           13.15m\n",
      "         7         105.0430           22.83m\n",
      "         1        1186.8690           14.26m\n",
      "        21         181.1061           41.27m\n",
      "        10         149.3865           15.65m\n",
      "[CV 4/5; 21/36] END learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=50;, score=-40.817 total time= 3.9min\n",
      "[CV 1/5; 23/36] START learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         6         245.9097           21.70m\n",
      "         2         980.3665           13.05m\n",
      "         2         815.6535           14.82m\n",
      "         6         223.3224           21.90m\n",
      "        12         179.7470           15.33m\n",
      "         1        1225.8728            7.75m\n",
      "        23         176.7686           35.80m\n",
      "         3         698.3073           14.65m\n",
      "         3         397.7426           14.90m\n",
      "         2         916.6984            7.91m\n",
      "        22         180.6553           40.50m\n",
      "         7         224.8234           23.79m\n",
      "         4         274.9137           15.76m\n",
      "         4         593.1663           16.14m\n",
      "         8          96.3538           26.00m\n",
      "        12         177.0220           15.70m\n",
      "[CV 3/5; 21/36] END learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=50;, score=-36.823 total time= 5.0min\n",
      "[CV 2/5; 23/36] START learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         7         202.7124           24.43m\n",
      "         3         788.4628            9.16m\n",
      "         1         988.4462            6.85m\n",
      "        24         176.4506           35.32m\n",
      "         5         535.0083           17.95m\n",
      "         2         690.5866            6.62m\n",
      "         5         223.1412           18.57m\n",
      "        13         179.0326           15.81m\n",
      "         4         398.1525           10.13m\n",
      "        23         180.2776           39.95m\n",
      "[CV 2/5; 20/36] END learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=100;, score=-53.802 total time=11.9min\n",
      "[CV 3/5; 23/36] START learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         8         202.8372           26.42m\n",
      "         3         524.4462            7.81m\n",
      "         1        1412.0133            5.87m\n",
      "         6         273.8469           20.20m\n",
      "         8         191.1426           27.09m\n",
      "         9          90.9928           29.52m\n",
      "         6         201.5231           21.24m\n",
      "        25         175.6893           34.96m\n",
      "         2        1022.1119            6.22m\n",
      "         4         459.5146            8.36m\n",
      "         5         221.4839           11.09m\n",
      "         3         814.4339            7.69m\n",
      "         9         191.7552           28.47m\n",
      "         7         198.4474           23.57m\n",
      "         5         322.6236            9.91m\n",
      "         7         184.6429           24.17m\n",
      "        26         175.3621           34.42m\n",
      "        14         178.6464           16.30m\n",
      "         9         187.7296           29.72m\n",
      "        10          86.7487           31.39m\n",
      "         4         527.6160            8.74m\n",
      "         6         163.8718           12.66m\n",
      "        10         183.8384           30.68m\n",
      "         6         273.1999           11.56m\n",
      "         8         176.5286           26.74m\n",
      "        27         175.2043           34.11m\n",
      "[CV 3/5; 20/36] END learning_rate=0.5, max_depth=100, min_samples_split=20, n_estimators=100;, score=-36.932 total time=12.6min\n",
      "[CV 4/5; 23/36] START learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         5         346.1486           11.62m\n",
      "         8         170.1553           28.29m\n",
      "         1        1451.7007            6.55m\n",
      "        10         181.4472           32.55m\n",
      "         7         143.6579           14.83m\n",
      "         2        1188.5288            6.60m\n",
      "        15         178.4770           16.84m\n",
      "        11          85.0562           35.22m\n",
      "[CV 1/5; 22/36] END learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=100;, score=-50.626 total time= 4.4min\n",
      "[CV 5/5; 23/36] START learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         7         241.3711           13.02m\n",
      "        11         181.0284           32.10m\n",
      "         3        1016.6723            7.35m\n",
      "         1        1275.2042            6.33m\n",
      "         9         154.9312           29.73m\n",
      "         6         280.0923           13.39m\n",
      "         2         978.4659            6.57m\n",
      "         9         164.4852           31.28m\n",
      "[CV 5/5; 22/36] END learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=100;, score=-37.660 total time= 3.1min\n",
      "[CV 1/5; 24/36] START learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         4         816.0495            8.93m\n",
      "         1        1225.8728           13.08m\n",
      "         3         563.7704            7.80m\n",
      "        11         178.4242           35.26m\n",
      "         8         227.1627           14.06m\n",
      "         2         916.6984           13.13m\n",
      "         7         244.9090           13.81m\n",
      "         8         124.7886           16.70m\n",
      "         5         431.2458           10.08m\n",
      "         4         344.5907            8.86m\n",
      "        16         178.3568           17.10m\n",
      "         3         788.4628           15.71m\n",
      "        12         179.7470           34.75m\n",
      "        10         149.3865           32.97m\n",
      "[CV 4/5; 22/36] END learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=100;, score=-40.817 total time= 3.7min\n",
      "[CV 2/5; 24/36] START learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         988.4462           14.67m\n",
      "         4         398.1525           17.48m\n",
      "         6         294.1957           10.98m\n",
      "         5         300.7344           10.48m\n",
      "         2         690.5866           14.30m\n",
      "         8         223.8088           15.00m\n",
      "         9         216.0161           15.30m\n",
      "        12         177.0220           37.43m\n",
      "         9         114.5048           17.46m\n",
      "[CV 1/5; 23/36] END learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=50;, score=-49.264 total time= 3.8min\n",
      "[CV 3/5; 24/36] START learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "[CV 3/5; 22/36] END learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=100;, score=-36.823 total time= 5.1min\n",
      "[CV 4/5; 24/36] START learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "         5         221.4839           19.28m\n",
      "         3         524.4462           16.22m\n",
      "         1        1412.0133           11.82m\n",
      "         1        1451.7007           16.36m\n",
      "        13         179.0326           36.66m\n",
      "         2        1022.1119           13.35m\n",
      "        17         178.1742           17.18m\n",
      "         2        1188.5288           15.64m\n",
      "         4         459.5146           17.90m\n",
      "         6         256.9997           11.98m\n",
      "         7         255.2273           12.68m\n",
      "         6         163.8718           22.09m\n",
      "         9         213.9304           15.43m\n",
      "         3         814.4339           15.64m\n",
      "         3        1016.6723           16.33m\n",
      "        10         203.8623           16.01m\n",
      "         5         322.6236           21.65m\n",
      "         4         527.6160           18.15m\n",
      "         4         816.0495           20.27m\n",
      "         7         235.2494           13.22m\n",
      "         7         143.6579           26.26m\n",
      "         8         215.5948           14.44m\n",
      "        14         178.6464           38.55m\n",
      "        10         205.4426           16.20m\n",
      "        18         178.0795           17.26m\n",
      "         5         431.2458           22.61m\n",
      "         5         346.1486           23.59m\n",
      "         6         273.1999           26.08m\n",
      "        11         199.1835           16.45m\n",
      "         8         223.4342           14.77m\n",
      "         6         294.1957           24.96m\n",
      "         8         124.7886           30.35m\n",
      "        11         200.7165           16.46m\n",
      "         6         280.0923           28.17m\n",
      "        12         195.1415           16.60m\n",
      "         7         241.3711           30.82m\n",
      "         9         176.4230           16.43m\n",
      "        15         178.4770           40.50m\n",
      "        19         178.0523           17.18m\n",
      "         7         255.2273           28.17m\n",
      "         9         201.7324           15.53m\n",
      "         7         244.9090           29.51m\n",
      "         9         114.5048           33.15m\n",
      "[CV 1/5; 24/36] END learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=100;, score=-49.264 total time= 3.3min\n",
      "[CV 5/5; 24/36] START learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        12         197.2019           16.69m\n",
      "         1        1275.2042           12.30m\n",
      "         8         227.1627           33.38m\n",
      "        13         191.9437           16.91m\n",
      "         2         978.4659           12.68m\n",
      "        20         177.9961           16.87m\n",
      "        10         194.3286           16.09m\n",
      "        10         163.8532           17.78m\n",
      "         8         215.5948           31.82m\n",
      "         3         563.7704           15.00m\n",
      "        16         178.3568           42.02m\n",
      "         8         223.8088           32.27m\n",
      "         4         344.5907           16.80m\n",
      "        13         192.5657           17.16m\n",
      "         9         216.0161           36.03m\n",
      "         9         213.9304           33.81m\n",
      "        14         190.0546           17.36m\n",
      "         5         300.7344           20.33m\n",
      "        21         177.9453           16.55m\n",
      "        11         155.7847           18.39m\n",
      "[CV 4/5; 23/36] END learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=50;, score=-40.996 total time= 5.2min\n",
      "[CV 1/5; 25/36] START learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         9         176.4230           35.66m\n",
      "        11         184.7980           17.17m\n",
      "        17         178.1742           43.22m\n",
      "         1        1812.0077            7.77m\n",
      "         2        1632.9808            7.96m\n",
      "        10         203.8623           38.30m\n",
      "        14         189.4684           17.42m\n",
      "         6         256.9997           24.48m\n",
      "        10         205.4426           35.77m\n",
      "         3        1485.1425            7.39m\n",
      "        15         188.9241           17.39m\n",
      "         4        1361.8349            7.28m\n",
      "        12         177.3490           17.30m\n",
      "        22         177.8963           16.27m\n",
      "        10         163.8532           38.44m\n",
      "         5        1254.6385            7.07m\n",
      "         7         235.2494           27.44m\n",
      "         6        1168.7799            6.72m\n",
      "        11         199.1835           39.61m\n",
      "        18         178.0795           44.31m\n",
      "        11         200.7165           36.77m\n",
      "         7        1094.6522            6.44m\n",
      "        15         187.1716           17.75m\n",
      "         8        1032.9658            6.30m\n",
      "        16         187.8355           17.42m\n",
      "         9         975.2144            6.22m\n",
      "[CV 1/5; 25/36] END learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=50;, score=-49.075 total time= 1.4min\n",
      "[CV 2/5; 25/36] START learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        11         155.7847           39.56m\n",
      "[CV 4/5; 24/36] END learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=100;, score=-40.996 total time= 4.9min\n",
      "[CV 3/5; 25/36] START learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        13         167.2950           17.87m\n",
      "         1        2072.2111            5.87m\n",
      "         8         223.4342           31.32m\n",
      "         1        1617.9202            8.69m\n",
      "        23         177.8853           15.98m\n",
      "        12         195.1415           40.25m\n",
      "        12         197.2019           37.65m\n",
      "         2        1868.3153            5.66m\n",
      "         2        1421.0632            7.74m\n",
      "         3        1701.2442            5.48m\n",
      "         3        1261.5846            7.39m\n",
      "         4        1565.1157            5.34m\n",
      "        19         178.0523           45.31m\n",
      "        17         185.6227           17.17m\n",
      "        16         184.2465           17.83m\n",
      "         5        1452.4768            5.25m\n",
      "         4        1132.9141            7.02m\n",
      "         6        1360.0833            5.20m\n",
      "         9         201.7324           33.41m\n",
      "         5        1025.4029            6.88m\n",
      "         7        1257.8562            5.13m\n",
      "        14         162.8648           18.10m\n",
      "        13         191.9437           41.34m\n",
      "         6         937.7620            6.60m\n",
      "        24         177.8641           15.61m\n",
      "         8        1155.9093            5.04m\n",
      "        13         192.5657           39.26m\n",
      "         7         865.8437            6.38m\n",
      "         9        1073.5598            4.94m\n",
      "         8         806.0748            6.13m\n",
      "        10        1006.3320            4.86m\n",
      "         9         749.9951            5.92m\n",
      "        10         194.3286           35.16m\n",
      "        20         177.9961           45.76m\n",
      "        11         946.1892            4.76m\n",
      "        18         183.1879           17.19m\n",
      "        10         705.0285            5.69m\n",
      "        17         183.0286           18.00m\n",
      "        12         904.9733            4.66m\n",
      "        11         664.9040            5.53m\n",
      "        13         860.4601            4.59m\n",
      "        14         190.0546           42.84m\n",
      "        12         628.6515            5.34m\n",
      "        14         189.4684           40.36m\n",
      "        15         160.3793           18.44m\n",
      "        14         823.5749            4.49m\n",
      "        13         600.1634            5.19m\n",
      "        15         790.4088            4.39m\n",
      "        25         177.8330           15.36m\n",
      "        14         576.6894            5.05m\n",
      "        16         750.7993            4.36m\n",
      "        19         181.3598           16.92m\n",
      "        15         550.4764            4.95m\n",
      "        21         177.9453           46.02m\n",
      "        11         184.7980           38.12m\n",
      "        17         717.8062            4.28m\n",
      "        18         181.0998           17.86m\n",
      "        16         518.3836            4.81m\n",
      "        18         647.2574            4.21m\n",
      "        15         188.9241           43.53m\n",
      "        17         491.1014            4.68m\n",
      "        15         187.1716           42.01m\n",
      "        19         588.8608            4.13m\n",
      "        18         469.2126            4.64m\n",
      "        16         159.5170           18.82m\n",
      "        20         541.7309            4.05m\n",
      "[CV 3/5; 25/36] END learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=50;, score=-44.806 total time= 2.7min\n",
      "[CV 4/5; 25/36] START learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "        12         177.3490           38.73m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        19         441.1377            4.58m\n",
      "        20         180.7810           16.60m\n",
      "        26         177.8251           15.03m\n",
      "         1        1990.4531            6.23m\n",
      "        22         177.8963           46.28m\n",
      "         2        1784.2138            6.13m\n",
      "        19         180.2646           17.67m\n",
      "        20         417.7805            4.56m\n",
      "         3        1623.3516            5.97m\n",
      "        16         187.8355           44.27m\n",
      "         4        1488.4214            5.88m\n",
      "        21         397.4285            4.52m\n",
      "        16         184.2465           43.12m\n",
      "         5        1378.1455            5.71m\n",
      "        13         167.2950           40.10m\n",
      "        22         380.0136            4.40m\n",
      "         6        1279.9250            5.60m\n",
      "        21         180.2406           16.33m\n",
      "        27         177.8103           14.50m\n",
      "         7        1196.8266            5.50m\n",
      "        23         365.8802            4.28m\n",
      "        17         158.4805           19.18m\n",
      "         8        1122.2952            5.36m\n",
      "        23         177.8853           46.50m\n",
      "        17         185.6227           44.41m\n",
      "        24         352.9819            4.15m\n",
      "        20         179.7677           17.47m\n",
      "         9        1055.6497            5.33m\n",
      "        25         343.0014            4.04m\n",
      "        10         991.4035            5.42m\n",
      "        14         162.8648           41.51m\n",
      "        26         330.5227            3.91m\n",
      "        11         941.0260            5.49m\n",
      "        17         183.0286           44.70m\n",
      "        22         179.8733           16.05m\n",
      "        28         177.7980           13.99m\n",
      "        12         899.0836            5.49m\n",
      "        27         319.7746            3.80m\n",
      "        18         157.5785           18.98m\n",
      "        21         179.3097           17.09m\n",
      "        24         177.8641           46.50m\n",
      "        18         183.1879           45.20m\n",
      "        13         864.2707            5.46m\n",
      "        28         309.6567            3.66m\n",
      "        14         833.7378            5.43m\n",
      "        29         300.7504            3.56m\n",
      "        15         160.3793           42.46m\n",
      "        18         181.0998           45.08m\n",
      "        23         179.6659           15.58m\n",
      "        15         729.2386            5.37m\n",
      "        30         284.0962            3.40m\n",
      "        29         177.7916           13.50m\n",
      "        16         645.1698            5.32m\n",
      "        31         270.1366            3.25m\n",
      "        19         157.0792           18.73m\n",
      "        19         181.3598           45.29m\n",
      "        22         177.4466           16.80m\n",
      "        17         573.2442            5.24m\n",
      "        32         258.6534            3.12m\n",
      "        25         177.8330           46.93m\n",
      "        18         515.6744            5.13m\n",
      "        16         159.5170           43.52m\n",
      "        33         252.2328            2.98m\n",
      "        24         179.5197           15.25m\n",
      "        19         469.3841            5.05m\n",
      "        19         180.2646           45.72m\n",
      "        20         425.8301            4.93m\n",
      "        20         180.7810           45.30m\n",
      "        34         244.2640            2.84m\n",
      "        30         177.7891           13.05m\n",
      "        20         156.7299           18.47m\n",
      "        21         388.8338            4.84m\n",
      "[CV 4/5; 25/36] END learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=50;, score=-37.800 total time= 3.5min\n",
      "[CV 5/5; 25/36] START learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=50\n",
      "        23         177.0722           16.47m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        35         237.6784            2.69m\n",
      "         1        1937.5716            7.32m\n",
      "        26         177.8251           47.17m\n",
      "        36         231.6175            2.54m\n",
      "         2        1728.4697            7.27m\n",
      "        17         158.4805           44.65m\n",
      "        20         179.7677           46.15m\n",
      "         3        1557.2779            7.05m\n",
      "        25         179.3639           14.97m\n",
      "        37         227.3047            2.38m\n",
      "        21         180.2406           45.49m\n",
      "         4        1398.6495            6.91m\n",
      "         5        1272.2516            6.82m\n",
      "        38         222.0561            2.22m\n",
      "        31         177.7791           12.56m\n",
      "        21         156.5267           18.36m\n",
      "         6        1167.3979            6.64m\n",
      "        27         177.8103           46.79m\n",
      "        24         176.7718           16.28m\n",
      "        39         217.7748            2.05m\n",
      "         7        1073.8631            6.46m\n",
      "        18         157.5785           45.17m\n",
      "        26         179.1656           14.45m\n",
      "        21         179.3097           46.17m\n",
      "         8         997.3520            6.34m\n",
      "        40         214.3847            1.89m\n",
      "        22         179.8733           45.71m\n",
      "         9         933.6176            6.20m\n",
      "        10         879.5620            6.10m\n",
      "        41         211.3560            1.71m\n",
      "        22         156.3507           17.84m\n",
      "        32         177.7723           12.01m\n",
      "        11         838.0421            5.97m\n",
      "        28         177.7980           46.52m\n",
      "        42         208.9737            1.53m\n",
      "        12         799.7196            5.84m\n",
      "        19         157.0792           45.62m\n",
      "        25         176.2915           15.95m\n",
      "        22         177.4466           46.44m\n",
      "        27         178.9401           14.05m\n",
      "        13         769.1583            5.70m\n",
      "        43         206.8673            1.35m\n",
      "        23         179.6659           45.55m\n",
      "        14         743.7160            5.57m\n",
      "        44         204.9667            1.16m\n",
      "        15         648.4521            5.45m\n",
      "        23         156.1376           17.47m\n",
      "        16         571.6955            5.29m\n",
      "        45         203.1697           58.42s\n",
      "        29         177.7916           46.34m\n",
      "        33         177.7703           11.50m\n",
      "        17         507.6590            5.17m\n",
      "        20         156.7299           46.02m\n",
      "        46         201.4534           46.96s\n",
      "        26         176.0461           15.51m\n",
      "        23         177.0722           46.65m\n",
      "        18         454.3430            5.03m\n",
      "        28         178.7883           13.65m\n",
      "[CV 2/5; 23/36] END learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=50;, score=-53.648 total time=17.4min\n",
      "[CV 1/5; 26/36] START learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        24         179.5197           45.93m\n",
      "        19         411.4501            4.91m\n",
      "         1        1812.0077           12.73m\n",
      "        47         199.5285           35.46s\n",
      "         2        1632.9808           12.64m\n",
      "        20         373.3328            4.78m\n",
      "         3        1485.1425           12.24m\n",
      "        48         197.8100           23.82s\n",
      "        21         343.1912            4.66m\n",
      "        24         155.9459           17.10m\n",
      "         4        1361.8349           12.26m\n",
      "        27         175.8800           14.88m\n",
      "         5        1254.6385           11.97m\n",
      "        49         196.4451           11.95s\n",
      "        30         177.7891           46.34m\n",
      "        22         317.9803            4.52m\n",
      "        21         156.5267           46.65m\n",
      "        34         177.7690           10.96m\n",
      "         6        1168.7799           11.65m\n",
      "        24         176.7718           46.98m\n",
      "        23         296.3339            4.39m\n",
      "         7        1094.6522           11.58m\n",
      "        50         195.4021            0.00s\n",
      "[CV 2/5; 25/36] END learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=50;, score=-53.956 total time=10.0min\n",
      "[CV 2/5; 26/36] START learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         8        1032.9658           11.42m\n",
      "        25         179.3639           46.34m\n",
      "        24         278.3595            4.24m\n",
      "         1        1617.9202           13.23m\n",
      "         9         975.2144           11.33m\n",
      "[CV 1/5; 26/36] END learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=100;, score=-49.075 total time= 1.1min\n",
      "[CV 3/5; 26/36] START learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2        1421.0632           12.55m\n",
      "        25         260.6859            4.10m\n",
      "[CV 5/5; 25/36] END learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=50;, score=-38.107 total time= 4.1min\n",
      "[CV 4/5; 26/36] START learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "        28         175.8027           14.26m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        2072.2111           11.32m\n",
      "         3        1261.5846           12.13m\n",
      "         1        1990.4531           13.33m\n",
      "         2        1868.3153           11.82m\n",
      "         4        1132.9141           11.86m\n",
      "        25         155.8763           16.71m\n",
      "        22         156.3507           46.46m\n",
      "         3        1701.2442           11.59m\n",
      "         2        1784.2138           13.36m\n",
      "         5        1025.4029           11.63m\n",
      "        31         177.7791           46.18m\n",
      "         4        1565.1157           11.41m\n",
      "         3        1623.3516           12.88m\n",
      "         6         937.7620           11.50m\n",
      "        35         177.7627           10.40m\n",
      "        26         179.1656           45.88m\n",
      "         5        1452.4768           11.16m\n",
      "        25         176.2915           47.22m\n",
      "         4        1488.4214           12.42m\n",
      "         7         865.8437           11.30m\n",
      "         6        1360.0833           11.17m\n",
      "         5        1378.1455           12.32m\n",
      "         8         806.0748           11.25m\n",
      "         7        1257.8562           11.15m\n",
      "         6        1279.9250           12.21m\n",
      "         9         749.9951           11.25m\n",
      "        29         175.4918           13.74m\n",
      "         8        1155.9093           11.16m\n",
      "         7        1196.8266           12.08m\n",
      "        10         705.0285           11.15m\n",
      "         9        1073.5598           11.10m\n",
      "        26         155.8091           16.18m\n",
      "        23         156.1376           46.64m\n",
      "         8        1122.2952           11.90m\n",
      "        11         664.9040           11.10m\n",
      "        10        1006.3320           11.01m\n",
      "        32         177.7723           45.89m\n",
      "         9        1055.6497           11.83m\n",
      "        12         628.6515           11.04m\n",
      "        11         946.1892           10.92m\n",
      "        10         991.4035           11.86m\n",
      "        13         600.1634           10.99m\n",
      "        27         178.9401           45.96m\n",
      "        12         904.9733           10.81m\n",
      "        36         177.7612            9.81m\n",
      "        14         576.6894           10.88m\n",
      "        11         941.0260           11.84m\n",
      "        26         176.0461           47.57m\n",
      "        13         860.4601           10.74m\n",
      "        15         550.4764           10.86m\n",
      "        12         899.0836           11.83m\n",
      "        14         823.5749           10.74m\n",
      "        30         175.3929           13.22m\n",
      "        16         518.3836           10.85m\n",
      "        13         864.2707           11.79m\n",
      "        15         790.4088           10.68m\n",
      "        27         155.7483           15.63m\n",
      "        24         155.9459           46.82m\n",
      "        17         491.1014           10.81m\n",
      "        14         833.7378           11.75m\n",
      "        16         750.7993           10.75m\n",
      "        18         469.2126           10.88m\n",
      "        33         177.7703           45.76m\n",
      "        15         729.2386           11.92m\n",
      "        17         717.8062           10.79m\n",
      "        19         441.1377           10.98m\n",
      "        27         175.8800           47.32m\n",
      "        18         647.2574           10.80m\n",
      "        16         645.1698           11.97m\n",
      "        28         178.7883           46.19m\n",
      "[CV 2/5; 24/36] END learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=100;, score=-53.648 total time=18.0min\n",
      "[CV 5/5; 26/36] START learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        19         588.8608           10.80m\n",
      "        37         177.7573            9.22m\n",
      "        31         175.2362           12.62m\n",
      "        17         573.2442           11.92m\n",
      "        20         417.7805           11.05m\n",
      "         1        1937.5716           14.00m\n",
      "        28         155.6544           15.01m\n",
      "        18         515.6744           11.77m\n",
      "        20         541.7309           10.77m\n",
      "[CV 3/5; 26/36] END learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=100;, score=-44.806 total time= 2.7min\n",
      "[CV 1/5; 27/36] START learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2        1728.4697           14.15m\n",
      "        21         397.4285           11.13m\n",
      "         1        1829.5595            6.24m\n",
      "        25         155.8763           46.96m\n",
      "        19         469.3841           11.71m\n",
      "         3        1557.2779           14.08m\n",
      "        22         380.0136           11.10m\n",
      "         2        1663.2664            6.13m\n",
      "        20         425.8301           11.58m\n",
      "         4        1398.6495           14.17m\n",
      "        28         175.8027           46.72m\n",
      "        34         177.7690           45.44m\n",
      "         3        1527.2250            5.85m\n",
      "        23         365.8802           11.10m\n",
      "        21         388.8338           11.51m\n",
      "[CV 4/5; 26/36] END learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=100;, score=-37.800 total time= 3.1min\n",
      "[CV 2/5; 27/36] START learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         5        1272.2516           14.19m\n",
      "         4        1403.7845            5.75m\n",
      "        32         174.9727           11.96m\n",
      "        24         352.9819           11.03m\n",
      "         1        1636.5582            6.44m\n",
      "         5        1301.6521            5.53m\n",
      "         6        1167.3979           14.04m\n",
      "         2        1456.1364            5.92m\n",
      "         6        1217.8452            5.35m\n",
      "        25         343.0014           11.03m\n",
      "         7        1073.8631           13.92m\n",
      "        29         155.2606           14.42m\n",
      "         3        1305.0940            5.69m\n",
      "        38         177.7565            8.60m\n",
      "         7        1147.9698            5.25m\n",
      "        26         155.8091           46.73m\n",
      "         8         997.3520           13.71m\n",
      "        26         330.5227           10.95m\n",
      "         8        1083.4512            5.11m\n",
      "         4        1177.3724            5.91m\n",
      "         9         933.6176           13.57m\n",
      "         5        1072.7318            5.74m\n",
      "         9        1024.9468            5.07m\n",
      "        27         319.7746           10.93m\n",
      "        29         175.4918           46.46m\n",
      "        10         879.5620           13.42m\n",
      "        10         968.8903            4.98m\n",
      "         6         984.4019            5.69m\n",
      "        33         174.9294           11.34m\n",
      "        35         177.7627           45.21m\n",
      "        28         309.6567           10.89m\n",
      "        11         917.7634            4.83m\n",
      "        11         838.0421           13.28m\n",
      "         7         910.7365            5.59m\n",
      "        30         155.0353           13.74m\n",
      "[CV 5/5; 23/36] END learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=50;, score=-37.888 total time=20.6min\n",
      "[CV 3/5; 27/36] START learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         8         845.3824            5.45m\n",
      "        29         300.7504           10.86m\n",
      "        12         799.7196           13.22m\n",
      "        12         875.1145            4.88m\n",
      "         1        2085.9322            5.59m\n",
      "         9         788.0412            5.36m\n",
      "        27         155.7483           46.44m\n",
      "        13         769.1583           13.13m\n",
      "        30         284.0962           10.77m\n",
      "        13         840.0794            4.84m\n",
      "        39         177.7557            7.93m\n",
      "         2        1896.5603            5.37m\n",
      "        10         746.4953            5.17m\n",
      "         3        1740.9706            5.34m\n",
      "        14         743.7160           13.03m\n",
      "        14         732.4192            4.76m\n",
      "        31         270.1366           10.71m\n",
      "        11         705.4282            5.15m\n",
      "         4        1615.0394            5.25m\n",
      "        30         175.3929           46.21m\n",
      "        15         638.9144            4.65m\n",
      "[CV 1/5; 27/36] END learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=50;, score=-49.442 total time= 2.0min\n",
      "[CV 4/5; 27/36] START learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        15         648.4521           12.92m\n",
      "        12         667.9809            4.99m\n",
      "        34         174.7641           10.74m\n",
      "         5        1508.9077            5.36m\n",
      "        32         258.6534           10.66m\n",
      "         1        2033.0477            5.92m\n",
      "        16         571.6955           12.75m\n",
      "        13         624.9864            4.88m\n",
      "        36         177.7612           44.88m\n",
      "         6        1415.2696            5.23m\n",
      "         2        1877.1280            6.56m\n",
      "        33         252.2328           10.59m\n",
      "        17         507.6590           12.65m\n",
      "        14         589.5997            4.80m\n",
      "        28         155.6544           45.91m\n",
      "         7        1315.2968            5.14m\n",
      "         3        1738.5421            6.64m\n",
      "         8        1211.7969            5.03m\n",
      "        18         454.3430           12.55m\n",
      "        15         559.1376            4.69m\n",
      "        34         244.2640           10.56m\n",
      "         4        1623.2067            6.46m\n",
      "         9        1128.2863            5.01m\n",
      "        40         177.7542            7.25m\n",
      "        16         534.1023            4.62m\n",
      "        19         411.4501           12.48m\n",
      "        31         175.2362           45.63m\n",
      "         5        1525.4934            6.34m\n",
      "        10        1056.3193            4.93m\n",
      "        35         237.6784           10.51m\n",
      "        17         512.0972            4.51m\n",
      "        20         373.3328           12.42m\n",
      "         6        1443.3317            6.22m\n",
      "        11         994.3076            4.86m\n",
      "        18         491.9188            4.41m\n",
      "        35         174.6947           10.16m\n",
      "         7        1349.4378            6.03m\n",
      "        36         231.6175           10.48m\n",
      "        12         951.3948            4.76m\n",
      "        21         343.1912           12.36m\n",
      "         8        1269.1216            5.84m\n",
      "        19         481.4729            4.31m\n",
      "        29         155.2606           45.66m\n",
      "        37         177.7573           44.64m\n",
      "        13         904.8505            4.80m\n",
      "        22         317.9803           12.29m\n",
      "        37         227.3047           10.43m\n",
      "         9        1188.4223            5.66m\n",
      "        32         174.9727           44.88m\n",
      "        20         470.4361            4.22m\n",
      "        14         868.5426            4.76m\n",
      "        10        1132.2914            5.49m\n",
      "        23         296.3339           12.22m\n",
      "        38         222.0561           10.41m\n",
      "        21         460.8570            4.17m\n",
      "        41         177.7540            6.56m\n",
      "        11        1086.5221            5.39m\n",
      "        15         836.3673            4.71m\n",
      "        24         278.3595           12.17m\n",
      "        12        1048.6606            5.33m\n",
      "        22         438.7308            4.09m\n",
      "        39         217.7748           10.39m\n",
      "        16         794.5086            4.75m\n",
      "        25         260.6859           12.14m\n",
      "[CV 5/5; 26/36] END learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=100;, score=-38.107 total time= 4.1min\n",
      "[CV 5/5; 27/36] START learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        30         155.0353           45.06m\n",
      "[CV 5/5; 24/36] END learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=100;, score=-37.888 total time=19.3min\n",
      "[CV 1/5; 28/36] START learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        13        1006.7666            5.22m\n",
      "         1        1959.9161            5.74m\n",
      "        23         420.2357            4.04m\n",
      "         1        1829.5595           12.24m\n",
      "        17         759.6427            4.77m\n",
      "        36         174.6574            9.61m\n",
      "        40         214.3847           10.34m\n",
      "        14         970.5250            5.11m\n",
      "         2        1780.1389            5.65m\n",
      "        33         174.9294           44.41m\n",
      "         2        1663.2664           12.06m\n",
      "        24         405.7598            3.97m\n",
      "         3        1612.1528            5.50m\n",
      "        15         867.9427            5.00m\n",
      "         3        1527.2250           11.71m\n",
      "        18         686.7023            4.74m\n",
      "        38         177.7565           44.41m\n",
      "        41         211.3560           10.27m\n",
      "         4        1489.7657            5.39m\n",
      "        16         763.4212            4.86m\n",
      "         4        1403.7845           12.18m\n",
      "        25         392.9218            3.86m\n",
      "        19         627.8679            4.67m\n",
      "         5        1376.5136            5.26m\n",
      "        17         678.3275            4.72m\n",
      "         5        1301.6521           12.00m\n",
      "        42         208.9737           10.20m\n",
      "         6        1283.3272            5.18m\n",
      "        42         177.7521            5.88m\n",
      "        26         381.7845            3.75m\n",
      "         6        1217.8452           11.72m\n",
      "        18         609.8893            4.57m\n",
      "        20         579.4710            4.60m\n",
      "[CV 3/5; 27/36] END learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=50;, score=-44.628 total time= 3.1min\n",
      "[CV 2/5; 28/36] START learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         7        1198.6537            5.01m\n",
      "         7        1147.9698           11.79m\n",
      "        37         174.6344            8.94m\n",
      "        43         206.8673           10.10m\n",
      "         1        1636.5582           13.49m\n",
      "         8        1129.3018            4.89m\n",
      "        19         547.2168            4.46m\n",
      "        27         360.6532            3.63m\n",
      "         8        1083.4512           11.55m\n",
      "         9        1071.4590            4.76m\n",
      "         2        1456.1364           12.52m\n",
      "        34         174.7641           44.06m\n",
      "        20         494.3384            4.37m\n",
      "        28         341.3500            3.50m\n",
      "        44         204.9667            9.99m\n",
      "         3        1305.0940           12.03m\n",
      "        10        1022.8413            4.67m\n",
      "         9        1024.9468           11.80m\n",
      "        21         448.9651            4.24m\n",
      "        11         981.5889            4.61m\n",
      "         4        1177.3724           12.26m\n",
      "        10         968.8903           11.72m\n",
      "        29         325.7857            3.38m\n",
      "        45         203.1697            9.86m\n",
      "        39         177.7557           44.12m\n",
      "         5        1072.7318           11.89m\n",
      "        22         409.0214            4.10m\n",
      "        12         946.7288            4.54m\n",
      "[CV 4/5; 27/36] END learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=50;, score=-39.311 total time= 3.2min\n",
      "[CV 3/5; 28/36] START learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        11         917.7634           11.53m\n",
      "         1        2085.9322           12.35m\n",
      "        13         918.4443            4.47m\n",
      "         6         984.4019           12.21m\n",
      "        43         177.7518            5.17m\n",
      "        30         315.2633            3.27m\n",
      "        46         201.4534            9.74m\n",
      "        12         875.1145           11.55m\n",
      "         2        1896.5603           11.80m\n",
      "         7         910.7365           11.98m\n",
      "        14         885.6705            4.39m\n",
      "        13         840.0794           11.52m\n",
      "         3        1740.9706           11.72m\n",
      "        31         310.2961            3.14m\n",
      "         8         845.3824           11.83m\n",
      "        15         789.0705            4.30m\n",
      "        38         174.6157            8.34m\n",
      "        47         199.5285            9.67m\n",
      "        14         732.4192           11.40m\n",
      "         4        1615.0394           11.41m\n",
      "         9         788.0412           11.78m\n",
      "        35         174.6947           43.81m\n",
      "        16         713.8817            4.21m\n",
      "        15         638.9144           11.32m\n",
      "[CV 1/5; 28/36] END learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=100;, score=-49.442 total time= 2.0min\n",
      "[CV 4/5; 28/36] START learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "        32         301.7558            3.00m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         5        1508.9077           11.49m\n",
      "        10         746.4953           11.56m\n",
      "        17         646.8172            4.12m\n",
      "        48         197.8100            9.58m\n",
      "         1        2033.0477           12.42m\n",
      "         6        1415.2696           11.39m\n",
      "        11         705.4282           11.61m\n",
      "        33         290.8471            2.86m\n",
      "        18         595.7442            4.04m\n",
      "         2        1877.1280           12.20m\n",
      "         7        1315.2968           11.60m\n",
      "        12         667.9809           11.43m\n",
      "        49         196.4451            9.44m\n",
      "         3        1738.5421           12.14m\n",
      "        19         552.3046            3.96m\n",
      "        40         177.7542           43.90m\n",
      "         8        1211.7969           11.58m\n",
      "        13         624.9864           11.38m\n",
      "        34         284.5732            2.74m\n",
      "         4        1623.2067           11.98m\n",
      "         9        1128.2863           11.59m\n",
      "        39         174.5600            7.65m\n",
      "        44         177.7517            4.47m\n",
      "        20         502.3379            3.91m\n",
      "        50         195.4021            9.34m\n",
      "         5        1525.4934           11.86m\n",
      "        14         589.5997           11.43m\n",
      "        10        1056.3193           11.56m\n",
      "        35         276.3052            2.61m\n",
      "         6        1443.3317           11.85m\n",
      "        21         460.1705            3.85m\n",
      "        15         559.1376           11.41m\n",
      "        51         194.0927            9.19m\n",
      "        11         994.3076           11.64m\n",
      "         7        1349.4378           11.76m\n",
      "        22         424.9746            3.79m\n",
      "        16         534.1023           11.51m\n",
      "        12         951.3948           11.58m\n",
      "         8        1269.1216           11.67m\n",
      "        36         270.7776            2.47m\n",
      "        36         174.6574           43.87m\n",
      "        52         192.9657            9.07m\n",
      "        17         512.0972           11.47m\n",
      "        23         391.2625            3.72m\n",
      "        13         904.8505           11.64m\n",
      "         9        1188.4223           11.82m\n",
      "        37         263.9795            2.33m\n",
      "        18         491.9188           11.45m\n",
      "        14         868.5426           11.58m\n",
      "        10        1132.2914           11.80m\n",
      "        24         366.5304            3.62m\n",
      "        53         190.9086            8.94m\n",
      "        19         481.4729           11.39m\n",
      "        40         174.5221            7.00m\n",
      "        11        1086.5221           11.78m\n",
      "        15         836.3673           11.62m\n",
      "        41         177.7540           43.66m\n",
      "        25         347.0435            3.54m\n",
      "        45         177.7507            3.75m\n",
      "        38         257.9139            2.18m\n",
      "        12        1048.6606           11.86m\n",
      "        20         470.4361           11.42m\n",
      "        54         189.0102            8.84m\n",
      "        16         794.5086           11.90m\n",
      "        37         174.6344           43.18m\n",
      "        26         334.2867            3.45m\n",
      "        13        1006.7666           11.95m\n",
      "        21         460.8570           11.47m\n",
      "        39         252.8971            2.02m\n",
      "        17         759.6427           12.04m\n",
      "        27         319.4665            3.36m\n",
      "        55         187.4635            8.73m\n",
      "        14         970.5250           12.16m\n",
      "        22         438.7308           11.50m\n",
      "        18         686.7023           12.12m\n",
      "        40         249.3040            1.86m\n",
      "        28         305.2950            3.26m\n",
      "        15         867.9427           12.14m\n",
      "        23         420.2357           11.56m\n",
      "        56         185.9811            8.61m\n",
      "        19         627.8679           12.21m\n",
      "        41         174.5015            6.33m\n",
      "        16         763.4212           12.00m\n",
      "        41         245.4271            1.69m\n",
      "        29         297.2251            3.18m\n",
      "        24         405.7598           11.62m\n",
      "        17         678.3275           11.91m\n",
      "        20         579.4710           12.27m\n",
      "[CV 3/5; 28/36] END learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=100;, score=-44.628 total time= 3.1min\n",
      "[CV 5/5; 28/36] START learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        57         185.0252            8.47m\n",
      "        18         609.8893           11.81m\n",
      "         1        1959.9161           12.12m\n",
      "        30         285.2460            3.09m\n",
      "        42         241.9039            1.52m\n",
      "        25         392.9218           11.63m\n",
      "        38         174.6157           42.95m\n",
      "        46         177.7505            3.03m\n",
      "        42         177.7521           43.58m\n",
      "        19         547.2168           11.66m\n",
      "         2        1780.1389           12.28m\n",
      "        58         183.9720            8.31m\n",
      "        31         274.5967            2.97m\n",
      "        26         381.7845           11.61m\n",
      "         3        1612.1528           12.05m\n",
      "        20         494.3384           11.64m\n",
      "        43         237.6556            1.34m\n",
      "         4        1489.7657           11.80m\n",
      "        59         183.2042            8.16m\n",
      "        27         360.6532           11.62m\n",
      "        21         448.9651           11.57m\n",
      "        32         262.4945            2.87m\n",
      "[CV 5/5; 27/36] END learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=50;, score=-37.985 total time= 5.1min\n",
      "[CV 1/5; 29/36] START learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         5        1376.5136           11.60m\n",
      "         6        1283.3272           11.49m\n",
      "         1        1811.5802            6.79m\n",
      "        44         235.1248            1.17m\n",
      "        22         409.0214           11.51m\n",
      "[CV 4/5; 28/36] END learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=100;, score=-39.311 total time= 3.3min\n",
      "[CV 2/5; 29/36] START learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        28         341.3500           11.57m\n",
      "        42         174.4098            5.69m\n",
      "        60         182.4998            8.02m\n",
      "         7        1198.6537           11.31m\n",
      "         2        1632.2521            6.49m\n",
      "         1        1617.7278            7.20m\n",
      "        39         174.5600           42.32m\n",
      "         8        1129.3018           11.38m\n",
      "        29         325.7857           11.53m\n",
      "         3        1484.0677            6.19m\n",
      "        45         232.0543           58.95s\n",
      "         2        1420.6511            6.56m\n",
      "        61         181.8891            7.85m\n",
      "         9        1071.4590           11.14m\n",
      "         4        1361.1122            6.13m\n",
      "        47         177.7504            2.29m\n",
      "         3        1261.1099            6.29m\n",
      "        10        1022.8413           10.97m\n",
      "        30         315.2633           11.55m\n",
      "         5        1253.8935            5.86m\n",
      "        43         177.7518           43.22m\n",
      "         4        1132.1840            6.02m\n",
      "        62         181.4444            7.68m\n",
      "        46         230.1407           47.86s\n",
      "        11         981.5889           10.94m\n",
      "         6        1168.0296            5.69m\n",
      "         5        1024.5511            5.88m\n",
      "        31         310.2961           11.49m\n",
      "        12         946.7288           10.89m\n",
      "         7        1093.0022            5.71m\n",
      "         6         936.7285            5.74m\n",
      "        63         181.0609            7.53m\n",
      "        13         918.4443           10.83m\n",
      "        47         227.8735           36.30s\n",
      "        32         301.7558           11.42m\n",
      "         8        1032.1696            5.62m\n",
      "         7         864.6912            5.60m\n",
      "        40         174.5221           41.80m\n",
      "        14         885.6705           10.72m\n",
      "        43         174.3978            5.02m\n",
      "         9         974.4625            5.50m\n",
      "         8         804.5005            5.50m\n",
      "        64         180.6593            7.36m\n",
      "        15         789.0705           10.64m\n",
      "        33         290.8471           11.37m\n",
      "         9         748.5070            5.39m\n",
      "        48         224.9493           24.47s\n",
      "        10         918.2215            5.48m\n",
      "        16         713.8817           10.52m\n",
      "        48         177.7500            1.53m\n",
      "        10         703.4679            5.30m\n",
      "        11         862.4374            5.46m\n",
      "[CV 1/5; 29/36] END learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=50;, score=-48.722 total time= 1.5min\n",
      "[CV 3/5; 29/36] START learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "        34         284.5732           11.38m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        17         646.8172           10.48m\n",
      "        65         180.3191            7.21m\n",
      "        49         223.4280           12.33s\n",
      "        11         663.7626            5.27m\n",
      "         1        2071.7588            5.97m\n",
      "        18         595.7442           10.41m\n",
      "         2        1867.5887            5.92m\n",
      "        35         276.3052           11.37m\n",
      "        44         177.7517           42.96m\n",
      "        12         627.7536            5.25m\n",
      "        19         552.3046           10.36m\n",
      "        66         180.0477            7.04m\n",
      "        41         174.5015           41.23m\n",
      "         3        1699.9668            5.81m\n",
      "        50         221.0650            0.00s\n",
      "        44         174.3727            4.31m\n",
      "[CV 2/5; 27/36] END learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=50;, score=-53.641 total time=10.4min\n",
      "[CV 4/5; 29/36] START learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        13         599.1987            5.14m\n",
      "        20         502.3379           10.51m\n",
      "         4        1563.5476            5.73m\n",
      "        36         270.7776           11.34m\n",
      "         1        1990.1145            6.41m\n",
      "        14         575.9147            5.04m\n",
      "        67         179.8842            6.88m\n",
      "         5        1450.8246            5.61m\n",
      "        21         460.1705           10.51m\n",
      "         2        1783.6392            6.25m\n",
      "        15         550.3165            4.94m\n",
      "         6        1358.2472            5.55m\n",
      "        37         263.9795           11.30m\n",
      "        22         424.9746           10.53m\n",
      "         3        1622.5961            6.59m\n",
      "         7        1256.2819            5.45m\n",
      "        16         517.8754            4.84m\n",
      "        49         177.7498           46.25s\n",
      "        68         179.7228            6.73m\n",
      "         4        1487.5760            6.37m\n",
      "        23         391.2625           10.61m\n",
      "         8        1154.3691            5.40m\n",
      "        38         257.9139           11.26m\n",
      "        17         492.1562            4.73m\n",
      "         5        1378.2994            6.12m\n",
      "         9        1072.1748            5.39m\n",
      "        24         366.5304           10.56m\n",
      "        18         465.7647            4.63m\n",
      "         6        1280.2284            5.96m\n",
      "        42         174.4098           40.77m\n",
      "        45         174.3624            3.60m\n",
      "        69         179.5513            6.56m\n",
      "        39         252.8971           11.19m\n",
      "        10        1005.1358            5.27m\n",
      "        45         177.7507           42.57m\n",
      "         7        1196.8378            5.80m\n",
      "        25         347.0435           10.63m\n",
      "        19         435.3768            4.55m\n",
      "        11         945.1951            5.14m\n",
      "         8        1122.2199            5.66m\n",
      "        40         249.3040           11.11m\n",
      "        70         179.4453            6.40m\n",
      "        12         904.1382            5.05m\n",
      "        26         334.2867           10.67m\n",
      "        20         410.0037            4.50m\n",
      "         9        1045.4609            5.61m\n",
      "        13         859.6268            4.97m\n",
      "        41         245.4271           11.04m\n",
      "        10         983.2659            5.57m\n",
      "[CV 4/5; 29/36] END learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=50;, score=-35.787 total time= 1.4min\n",
      "[CV 5/5; 29/36] START learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        27         319.4665           10.76m\n",
      "        21         389.1644            4.43m\n",
      "        50         177.7497            0.00s\n",
      "[CV 2/5; 21/36] END learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=50;, score=-54.361 total time=38.7min\n",
      "[CV 1/5; 30/36] START learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        71         179.2734            6.21m\n",
      "        14         823.0269            4.91m\n",
      "         1        1937.3951            6.13m\n",
      "         1        1811.5802           13.65m\n",
      "        28         305.2950           10.75m\n",
      "        22         372.1728            4.35m\n",
      "         2        1728.1373            5.82m\n",
      "        42         241.9039           10.96m\n",
      "        15         791.4050            4.89m\n",
      "         2        1632.2521           12.82m\n",
      "        72         179.1654            6.02m\n",
      "        46         174.3530            2.90m\n",
      "         3        1556.8509            5.82m\n",
      "        43         174.3978           40.40m\n",
      "         3        1484.0677           12.43m\n",
      "        29         297.2251           10.77m\n",
      "        23         358.0342            4.27m\n",
      "         4        1398.1413            5.63m\n",
      "        16         751.6601            4.91m\n",
      "        43         237.6556           10.87m\n",
      "         4        1361.1122           12.37m\n",
      "        73         179.0789            5.84m\n",
      "         5        1271.6604            5.64m\n",
      "         5        1253.8935           12.13m\n",
      "        30         285.2460           10.81m\n",
      "        24         344.8836            4.18m\n",
      "        46         177.7505           42.27m\n",
      "        17         718.5733            4.92m\n",
      "         6        1166.7956            5.48m\n",
      "         6        1168.0296           11.95m\n",
      "        44         235.1248           10.80m\n",
      "         7        1073.2455            5.37m\n",
      "        74         178.9627            5.64m\n",
      "        31         274.5967           10.78m\n",
      "        25         334.9163            4.09m\n",
      "         7        1093.0022           11.70m\n",
      "        18         647.4462            4.92m\n",
      "         8         996.7543            5.30m\n",
      "         8        1032.1696           11.76m\n",
      "        45         232.0543           10.69m\n",
      "        32         262.4945           10.80m\n",
      "[CV 5/5; 28/36] END learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=100;, score=-37.985 total time= 5.1min\n",
      "[CV 2/5; 30/36] START learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        26         322.0980            3.99m\n",
      "         9         933.0193            5.18m\n",
      "        75         178.8706            5.44m\n",
      "        47         174.3436            2.18m\n",
      "        44         174.3727           39.78m\n",
      "        19         589.3216            4.88m\n",
      "         9         974.4625           11.64m\n",
      "         1        1617.7278           13.87m\n",
      "        10         878.9413            5.08m\n",
      "        10         918.2215           11.54m\n",
      "        46         230.1407           10.60m\n",
      "        27         310.5810            3.87m\n",
      "        20         542.2950            4.79m\n",
      "[CV 3/5; 29/36] END learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=50;, score=-45.078 total time= 3.2min\n",
      "[CV 3/5; 30/36] START learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2        1420.6511           12.67m\n",
      "        11         837.4683            5.01m\n",
      "        76         178.7826            5.25m\n",
      "        11         862.4374           11.69m\n",
      "[CV 1/5; 30/36] END learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=100;, score=-48.722 total time= 1.5min\n",
      "[CV 4/5; 30/36] START learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        2071.7588           11.70m\n",
      "         3        1261.1099           12.30m\n",
      "        12         799.2576            4.86m\n",
      "        28         301.3700            3.75m\n",
      "        47         227.8735           10.48m\n",
      "         2        1867.5887           11.80m\n",
      "         4        1132.1840           12.05m\n",
      "         1        1990.1145           15.73m\n",
      "        13         768.9364            4.74m\n",
      "        47         177.7504           41.76m\n",
      "        77         178.7342            5.05m\n",
      "         5        1024.5511           11.87m\n",
      "         3        1699.9668           12.45m\n",
      "         2        1783.6392           14.20m\n",
      "        29         295.9108            3.61m\n",
      "        14         742.2486            4.64m\n",
      "[CV 5/5; 29/36] END learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=50;, score=-42.519 total time= 1.8min\n",
      "[CV 5/5; 30/36] START learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         6         936.7285           11.79m\n",
      "        48         224.9493           10.37m\n",
      "         4        1563.5476           12.42m\n",
      "         3        1622.5961           13.65m\n",
      "        48         174.3362            1.46m\n",
      "         1        1937.3951           12.23m\n",
      "        45         174.3624           39.18m\n",
      "         7         864.6912           11.70m\n",
      "         5        1450.8246           12.27m\n",
      "         4        1487.5760           13.35m\n",
      "         2        1728.1373           11.95m\n",
      "        30         288.9831            3.50m\n",
      "        78         178.6638            4.87m\n",
      "         8         804.5005           11.67m\n",
      "        49         223.4280           10.23m\n",
      "         6        1358.2472           12.12m\n",
      "         5        1378.2994           13.03m\n",
      "         3        1556.8509           11.78m\n",
      "         9         748.5070           11.60m\n",
      "         7        1256.2819           12.00m\n",
      "         6        1280.2284           12.78m\n",
      "         4        1398.1413           11.71m\n",
      "        31         283.1025            3.37m\n",
      "        79         178.6015            4.66m\n",
      "        10         703.4679           11.50m\n",
      "         5        1271.6604           11.54m\n",
      "         8        1154.3691           11.97m\n",
      "         7        1196.8378           12.73m\n",
      "        50         221.0650           10.13m\n",
      "        11         663.7626           11.45m\n",
      "         6        1166.7956           11.45m\n",
      "        32         269.9869            3.23m\n",
      "         8        1122.2199           12.47m\n",
      "         9        1072.1748           12.22m\n",
      "        80         178.5441            4.45m\n",
      "         7        1073.2455           11.36m\n",
      "        12         627.7536           11.37m\n",
      "        48         177.7500           41.18m\n",
      "         9        1045.4609           12.47m\n",
      "        49         174.3197           43.76s\n",
      "        51         218.9462           10.01m\n",
      "        10        1005.1358           12.17m\n",
      "         8         996.7543           11.33m\n",
      "        46         174.3530           38.64m\n",
      "        33         259.2990            3.10m\n",
      "        13         599.1987           11.36m\n",
      "        11         945.1951           12.05m\n",
      "        10         983.2659           12.50m\n",
      "[CV 4/5; 30/36] END learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=100;, score=-35.787 total time= 1.4min\n",
      "[CV 1/5; 31/36] START learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        81         178.4872            4.24m\n",
      "         9         933.0193           11.21m\n",
      "        14         575.9147           11.38m\n",
      "        12         904.1382           11.98m\n",
      "         1        1829.2434            6.73m\n",
      "        52         217.1795            9.90m\n",
      "        10         878.9413           11.22m\n",
      "        34         249.1518            2.97m\n",
      "        15         550.3165           11.37m\n",
      "        13         859.6268           11.88m\n",
      "         2        1662.6301            6.91m\n",
      "        82         178.4425            4.03m\n",
      "        11         837.4683           11.16m\n",
      "[CV 2/5; 26/36] END learning_rate=0.1, max_depth=50, min_samples_split=10, n_estimators=100;, score=-53.776 total time=18.4min\n",
      "[CV 2/5; 31/36] START learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        16         517.8754           11.36m\n",
      "         3        1526.4762            6.46m\n",
      "        14         823.0269           11.78m\n",
      "        53         214.5668            9.75m\n",
      "        12         799.2576           11.05m\n",
      "         1        1636.4173            8.49m\n",
      "        35         241.1104            2.82m\n",
      "         4        1403.4179            6.20m\n",
      "        15         791.4050           11.65m\n",
      "        17         492.1562           11.31m\n",
      "        13         768.9364           10.96m\n",
      "         2        1455.8066            7.73m\n",
      "         5        1301.0519            5.91m\n",
      "        50         174.3140            0.00s\n",
      "[CV 3/5; 23/36] END learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=50;, score=-36.952 total time=36.6min\n",
      "[CV 3/5; 31/36] START learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        14         742.2486           10.99m\n",
      "[CV 5/5; 30/36] END learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=100;, score=-42.519 total time= 1.8min\n",
      "[CV 4/5; 31/36] START learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        18         465.7647           11.31m\n",
      "        16         751.6601           11.73m\n",
      "        54         212.5585            9.62m\n",
      "        47         174.3436           38.03m\n",
      "         1        2085.5331            5.44m\n",
      "        36         235.7033            2.67m\n",
      "         3        1304.6659            7.45m\n",
      "         6        1216.7655            5.91m\n",
      "         1        2032.7967            5.89m\n",
      "        49         177.7498           40.67m\n",
      "        19         435.3768           11.33m\n",
      "        17         718.5733           11.74m\n",
      "         2        1895.8597            5.48m\n",
      "         2        1876.5846            5.70m\n",
      "         4        1176.7755            7.41m\n",
      "         7        1146.9428            5.98m\n",
      "        55         209.7390            9.47m\n",
      "         3        1740.0277            5.52m\n",
      "        37         229.8616            2.51m\n",
      "        18         647.4462           11.73m\n",
      "         3        1738.0191            5.65m\n",
      "        20         410.0037           11.39m\n",
      "         5        1072.1287            7.06m\n",
      "         8        1082.6299            5.91m\n",
      "         4        1613.9853            5.30m\n",
      "         4        1622.6636            5.61m\n",
      "        19         589.3216           11.69m\n",
      "         5        1507.5057            5.21m\n",
      "        56         207.8240            9.30m\n",
      "        21         389.1644           11.42m\n",
      "         6         983.8764            7.01m\n",
      "         9        1023.6422            5.92m\n",
      "         5        1525.0675            5.47m\n",
      "        38         225.0511            2.35m\n",
      "         6        1414.1063            5.11m\n",
      "        20         542.2950           11.61m\n",
      "[CV 3/5; 30/36] END learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=100;, score=-45.078 total time= 2.9min\n",
      "[CV 5/5; 31/36] START learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         7         909.9357            6.75m\n",
      "         6        1444.6380            5.39m\n",
      "        22         372.1728           11.44m\n",
      "        10         967.9513            5.91m\n",
      "        48         174.3362           37.33m\n",
      "         1        1959.7734            5.34m\n",
      "         7        1314.1630            5.18m\n",
      "        57         205.5449            9.13m\n",
      "         8         844.3691            6.42m\n",
      "         7        1350.2272            5.29m\n",
      "         2        1779.8315            5.40m\n",
      "        11         916.9127            5.80m\n",
      "        39         220.0916            2.18m\n",
      "         9         787.2662            6.12m\n",
      "         8        1210.9450            5.25m\n",
      "        23         358.0342           11.50m\n",
      "        50         177.7497           39.89m\n",
      "         8        1269.5816            5.17m\n",
      "         3        1611.7755            5.31m\n",
      "        10         744.4213            5.90m\n",
      "        58         203.8695            8.96m\n",
      "         9        1127.6293            5.20m\n",
      "        12         874.5244            5.77m\n",
      "         9        1188.6305            5.11m\n",
      "        24         344.8836           11.54m\n",
      "         4        1489.2212            5.63m\n",
      "        40         215.5976            2.00m\n",
      "        11         703.9506            5.76m\n",
      "        10        1056.2542            5.11m\n",
      "        13         839.5016            5.69m\n",
      "        10        1132.5125            5.09m\n",
      "         5        1375.8173            5.44m\n",
      "        59         202.6355            8.76m\n",
      "        12         666.2837            5.56m\n",
      "        11         995.7395            5.04m\n",
      "        25         334.9163           11.58m\n",
      "         6        1285.6548            5.30m\n",
      "        11        1087.0026            5.11m\n",
      "        14         731.9140            5.64m\n",
      "        41         211.9950            1.82m\n",
      "        13         636.7439            5.43m\n",
      "        49         174.3197           36.66m\n",
      "        12         950.8656            4.99m\n",
      "         7        1201.6051            5.19m\n",
      "        60         201.4326            8.56m\n",
      "        12        1049.0981            5.03m\n",
      "        26         322.0980           11.65m\n",
      "         8        1132.1169            5.06m\n",
      "        14         597.9099            5.30m\n",
      "        15         638.4227            5.57m\n",
      "[CV 1/5; 31/36] END learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=50;, score=-49.514 total time= 2.4min\n",
      "[CV 1/5; 32/36] START learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "        13         903.7025            4.88m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        13        1007.0109            4.93m\n",
      "        51         177.7492           39.06m\n",
      "         9        1077.5568            4.98m\n",
      "        42         209.1813            1.64m\n",
      "        61         199.7406            8.38m\n",
      "        14         867.1553            4.82m\n",
      "        15         565.5165            5.19m\n",
      "        27         310.5810           11.65m\n",
      "         1        1829.2434           16.11m\n",
      "        14         966.6704            4.83m\n",
      "        10        1029.6801            4.92m\n",
      "        15         834.9006            4.72m\n",
      "        16         538.8846            5.09m\n",
      "         2        1662.6301           16.13m\n",
      "        15         864.3442            4.77m\n",
      "        28         301.3700           11.65m\n",
      "        11         988.2009            4.93m\n",
      "        62         198.4242            8.20m\n",
      "        43         206.4707            1.45m\n",
      "        16         794.0859            4.65m\n",
      "         3        1526.4762           15.48m\n",
      "        17         515.7903            4.98m\n",
      "        16         759.7629            4.67m\n",
      "        12         955.9998            4.82m\n",
      "[CV 5/5; 31/36] END learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=50;, score=-42.011 total time= 1.5min\n",
      "[CV 2/5; 32/36] START learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        29         295.9108           11.61m\n",
      "        50         174.3140           35.97m\n",
      "         4        1403.4179           15.44m\n",
      "        18         495.7221            4.87m\n",
      "         1        1636.4173           13.33m\n",
      "        63         197.2677            8.01m\n",
      "        17         674.5571            4.57m\n",
      "        17         759.8145            4.69m\n",
      "        44         204.4112            1.26m\n",
      "         5        1301.0519           14.89m\n",
      "         2        1455.8066           12.45m\n",
      "        18         601.3963            4.46m\n",
      "        30         288.9831           11.65m\n",
      "        19         485.5504            4.76m\n",
      "         3        1304.6659           12.05m\n",
      "        18         686.6341            4.71m\n",
      "        64         196.2735            7.82m\n",
      "         6        1216.7655           14.62m\n",
      "        19         534.5333            4.36m\n",
      "        52         177.7491           38.44m\n",
      "        20         473.7701            4.69m\n",
      "         4        1176.7755           12.43m\n",
      "        45         202.4321            1.06m\n",
      "        31         283.1025           11.64m\n",
      "         7        1146.9428           14.65m\n",
      "        19         627.6220            4.66m\n",
      "        20         481.9018            4.24m\n",
      "         5        1072.1287           12.13m\n",
      "        65         194.8613            7.63m\n",
      "        21         464.2891            4.56m\n",
      "         8        1082.6299           14.57m\n",
      "         6         983.8764           12.08m\n",
      "        32         269.9869           11.63m\n",
      "        21         436.4542            4.14m\n",
      "[CV 4/5; 31/36] END learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=50;, score=-39.298 total time= 3.0min\n",
      "[CV 3/5; 32/36] START learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        20         579.1678            4.61m\n",
      "[CV 3/5; 31/36] END learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=50;, score=-44.499 total time= 3.1min\n",
      "[CV 4/5; 32/36] START learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        46         200.8821           51.46s\n",
      "        22         442.4436            4.45m\n",
      "         7         909.9357           11.84m\n",
      "         9        1023.6422           14.51m\n",
      "        66         193.8450            7.43m\n",
      "         1        2085.5331           14.20m\n",
      "         1        2032.7967           12.08m\n",
      "        51         174.3085           35.44m\n",
      "         8         844.3691           11.79m\n",
      "        33         259.2990           11.64m\n",
      "         2        1895.8597           13.57m\n",
      "         2        1876.5846           12.44m\n",
      "        10         967.9513           14.58m\n",
      "        23         424.1025            4.37m\n",
      "        47         199.6754           38.89s\n",
      "         9         787.2662           11.69m\n",
      "         3        1740.0277           13.03m\n",
      "         3        1738.0191           12.45m\n",
      "        67         192.7720            7.25m\n",
      "        11         916.9127           14.48m\n",
      "        10         744.4213           11.73m\n",
      "         4        1613.9853           12.58m\n",
      "        34         249.1518           11.67m\n",
      "        24         408.7069            4.27m\n",
      "        53         177.7490           37.65m\n",
      "         4        1622.6636           12.60m\n",
      "         5        1507.5057           12.16m\n",
      "        12         874.5244           14.47m\n",
      "        68         192.2567            7.03m\n",
      "         5        1525.0675           12.43m\n",
      "        11         703.9506           11.97m\n",
      "        48         198.4985           26.23s\n",
      "        25         395.9682            4.16m\n",
      "         6        1414.1063           12.11m\n",
      "        35         241.1104           11.64m\n",
      "         6        1444.6380           12.27m\n",
      "        12         666.2837           11.76m\n",
      "        13         839.5016           14.43m\n",
      "        52         174.2978           34.72m\n",
      "         7        1314.1630           12.13m\n",
      "        69         191.8603            6.84m\n",
      "         7        1350.2272           12.16m\n",
      "        26         385.0496            4.04m\n",
      "        13         636.7439           11.93m\n",
      "        49         197.1909           13.22s\n",
      "        14         731.9140           14.44m\n",
      "         8        1210.9450           11.98m\n",
      "        36         235.7033           11.64m\n",
      "         8        1269.5816           12.33m\n",
      "        14         597.9099           11.88m\n",
      "         9        1127.6293           11.99m\n",
      "        70         191.3031            6.64m\n",
      "        27         361.9258            3.92m\n",
      "        15         638.4227           14.42m\n",
      "[CV 1/5; 32/36] END learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=100;, score=-49.514 total time= 2.6min\n",
      "[CV 5/5; 32/36] START learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         9        1188.6305           12.43m\n",
      "        15         565.5165           11.90m\n",
      "        50         195.6765            0.00s\n",
      "[CV 2/5; 29/36] END learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=50;, score=-54.101 total time=11.1min\n",
      "[CV 1/5; 33/36] START learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        10        1056.2542           12.07m\n",
      "        37         229.8616           11.63m\n",
      "         1        1959.7734           14.20m\n",
      "        10        1132.5125           12.47m\n",
      "        54         177.7490           36.91m\n",
      "        28         352.6756            3.81m\n",
      "         1        1811.5801            6.18m\n",
      "        71         190.7600            6.44m\n",
      "        11         995.7395           11.93m\n",
      "        16         538.8846           12.08m\n",
      "         2        1779.8315           14.12m\n",
      "        11        1087.0026           12.43m\n",
      "         2        1632.2519            6.06m\n",
      "        12         950.8656           11.94m\n",
      "        38         225.0511           11.62m\n",
      "        53         174.2847           34.01m\n",
      "         3        1611.7755           14.02m\n",
      "         3        1484.0677            5.80m\n",
      "        17         515.7903           12.20m\n",
      "        29         335.1485            3.70m\n",
      "        12        1049.0981           12.50m\n",
      "        13         903.7025           11.81m\n",
      "        72         189.9888            6.25m\n",
      "         4        1489.2212           13.88m\n",
      "         4        1361.1116            5.72m\n",
      "        18         495.7221           12.17m\n",
      "        13        1007.0109           12.63m\n",
      "        14         867.1553           11.79m\n",
      "        39         220.0916           11.60m\n",
      "         5        1253.8930            5.50m\n",
      "         5        1375.8173           13.75m\n",
      "        30         320.1507            3.58m\n",
      "        73         189.5150            6.03m\n",
      "         6        1168.0293            5.38m\n",
      "        14         966.6704           12.57m\n",
      "        15         834.9006           11.74m\n",
      "        19         485.5504           12.21m\n",
      "         6        1285.6548           13.63m\n",
      "         7        1093.0025            5.37m\n",
      "        40         215.5976           11.54m\n",
      "        55         177.7489           36.12m\n",
      "        15         864.3442           12.58m\n",
      "        31         307.7326            3.46m\n",
      "        16         794.0859           11.84m\n",
      "         7        1201.6051           13.69m\n",
      "        20         473.7701           12.29m\n",
      "        74         189.1077            5.83m\n",
      "         8        1032.1599            5.29m\n",
      "        16         759.7629           12.48m\n",
      "         8        1132.1169           13.65m\n",
      "        54         174.2812           33.34m\n",
      "         9         974.4548            5.20m\n",
      "        21         464.2891           12.26m\n",
      "        17         759.8145           12.08m\n",
      "        41         211.9950           11.50m\n",
      "        32         299.1291            3.35m\n",
      "         9        1077.5568           13.49m\n",
      "        17         674.5571           12.39m\n",
      "        75         188.6703            5.63m\n",
      "        10         918.2127            5.04m\n",
      "        18         686.6341           12.19m\n",
      "        22         442.4436           12.31m\n",
      "        18         601.3963           12.30m\n",
      "        10        1029.6801           13.48m\n",
      "        11         862.4264            4.97m\n",
      "[CV 1/5; 33/36] END learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=50;, score=-48.766 total time= 1.4min\n",
      "[CV 2/5; 33/36] START learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        42         209.1813           11.45m\n",
      "        33         290.8341            3.21m\n",
      "         1        1617.7277            6.80m\n",
      "        19         627.6220           12.25m\n",
      "        76         188.3119            5.42m\n",
      "        19         534.5333           12.17m\n",
      "        11         988.2009           13.42m\n",
      "        23         424.1025           12.55m\n",
      "         2        1420.6509            6.30m\n",
      "        20         481.9018           12.03m\n",
      "        56         177.7486           35.32m\n",
      "        12         955.9998           13.40m\n",
      "[CV 5/5; 32/36] END learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=100;, score=-42.011 total time= 1.8min\n",
      "[CV 3/5; 33/36] START learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        20         579.1678           12.39m\n",
      "[CV 3/5; 32/36] END learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=100;, score=-44.499 total time= 3.1min\n",
      "[CV 4/5; 33/36] START learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         3        1261.1080            6.05m\n",
      "        34         285.3780            3.07m\n",
      "        43         206.4707           11.42m\n",
      "        77         187.8989            5.21m\n",
      "        55         174.2773           32.62m\n",
      "        21         436.4542           11.93m\n",
      "[CV 4/5; 32/36] END learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=100;, score=-39.298 total time= 3.2min\n",
      "[CV 5/5; 33/36] START learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        2071.7588            6.93m\n",
      "        24         408.7069           12.67m\n",
      "         1        1990.1140            5.52m\n",
      "         4        1132.1979            5.72m\n",
      "         1        1937.3950            5.82m\n",
      "         2        1783.6388            5.80m\n",
      "         5        1024.5624            5.64m\n",
      "         2        1867.5887            7.31m\n",
      "        35         275.2874            2.91m\n",
      "        78         187.5739            4.99m\n",
      "        25         395.9682           12.68m\n",
      "         2        1728.1372            5.67m\n",
      "        44         204.4112           11.37m\n",
      "         6         936.7404            5.48m\n",
      "         3        1622.5956            6.09m\n",
      "         3        1699.9660            6.94m\n",
      "         3        1556.8507            5.45m\n",
      "         7         864.7099            5.38m\n",
      "        26         385.0496           12.64m\n",
      "         4        1487.5753            6.38m\n",
      "         4        1398.1410            5.28m\n",
      "         4        1563.5471            6.92m\n",
      "        79         187.3460            4.79m\n",
      "         8         804.4902            5.27m\n",
      "        36         268.2611            2.78m\n",
      "        45         202.4321           11.29m\n",
      "         5        1271.6599            5.20m\n",
      "         5        1378.2973            6.47m\n",
      "         5        1450.8240            6.83m\n",
      "        57         177.7486           34.52m\n",
      "         9         748.4951            5.12m\n",
      "        27         361.9258           12.61m\n",
      "         6        1166.7950            5.12m\n",
      "         6        1280.2275            6.28m\n",
      "         6        1358.2461            6.71m\n",
      "        10         703.4372            5.02m\n",
      "        80         187.1240            4.57m\n",
      "        56         174.2737           31.99m\n",
      "         7        1073.2469            5.20m\n",
      "        37         260.2058            2.62m\n",
      "        46         200.8821           11.22m\n",
      "         7        1196.8281            5.99m\n",
      "        11         663.6974            4.91m\n",
      "         7        1256.2808            6.53m\n",
      "        28         352.6756           12.68m\n",
      "         8         996.7553            5.07m\n",
      "         8        1122.2238            5.80m\n",
      "        12         627.7032            4.80m\n",
      "         9         933.0346            4.95m\n",
      "        81         186.7960            4.36m\n",
      "         8        1154.3691            6.45m\n",
      "        38         254.4806            2.45m\n",
      "         9        1045.4662            5.71m\n",
      "        47         199.6754           11.11m\n",
      "        13         599.1643            4.71m\n",
      "        10         878.9572            4.88m\n",
      "         9        1072.1757            6.32m\n",
      "        29         335.1485           12.87m\n",
      "        10         983.2791            5.60m\n",
      "        14         575.8698            4.65m\n",
      "        11         837.4894            4.79m\n",
      "        82         186.4001            4.14m\n",
      "[CV 2/5; 28/36] END learning_rate=0.1, max_depth=50, min_samples_split=20, n_estimators=100;, score=-53.548 total time=18.9min\n",
      "[CV 1/5; 34/36] START learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        10        1005.1217            6.15m\n",
      "        11         936.8289            5.57m\n",
      "        12         799.2721            4.67m\n",
      "        15         550.2810            4.57m\n",
      "        39         249.2253            2.28m\n",
      "         1        1811.5801           12.61m\n",
      "        48         198.4985           11.04m\n",
      "        11         945.1612            5.90m\n",
      "        30         320.1507           12.92m\n",
      "        13         768.9396            4.58m\n",
      "        57         174.2708           31.32m\n",
      "         2        1632.2519           12.43m\n",
      "        58         177.7486           33.86m\n",
      "        16         517.8840            4.52m\n",
      "        12         904.1123            5.68m\n",
      "        12         894.7486            5.60m\n",
      "[CV 4/5; 33/36] END learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=50;, score=-35.687 total time= 1.8min\n",
      "[CV 2/5; 34/36] START learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        14         742.2477            4.50m\n",
      "         3        1484.0677           12.86m\n",
      "        13         859.5845            5.52m\n",
      "        49         197.1909           10.92m\n",
      "        17         492.1818            4.43m\n",
      "        31         307.7326           12.92m\n",
      "        40         245.7673            2.11m\n",
      "         1        1617.7277           16.46m\n",
      "        15         649.4176            4.38m\n",
      "         4        1361.1116           12.94m\n",
      "        14         822.7450            5.32m\n",
      "        18         465.7397            4.35m\n",
      "         2        1420.6509           15.50m\n",
      "         5        1253.8930           12.36m\n",
      "        16         572.3797            4.37m\n",
      "        15         791.2214            5.15m\n",
      "        32         299.1291           12.96m\n",
      "        50         195.6765           10.81m\n",
      "         6        1168.0293           12.10m\n",
      "         3        1261.1080           15.02m\n",
      "        19         435.3620            4.27m\n",
      "        17         507.3181            4.27m\n",
      "        41         241.5929            1.94m\n",
      "        16         751.6543            5.05m\n",
      "         7        1093.0025           11.88m\n",
      "         4        1132.1979           14.83m\n",
      "        20         410.0145            4.20m\n",
      "        18         454.8736            4.21m\n",
      "        33         290.8341           12.93m\n",
      "         8        1032.1599           11.75m\n",
      "        58         174.2684           30.63m\n",
      "        59         177.7485           33.02m\n",
      "        17         718.5332            4.96m\n",
      "         5        1024.5624           14.63m\n",
      "        51         194.3105           10.74m\n",
      "        21         389.2620            4.12m\n",
      "         9         974.4548           11.60m\n",
      "        19         412.6680            4.12m\n",
      "        42         237.9584            1.75m\n",
      "        18         647.4476            4.88m\n",
      "         6         936.7404           14.55m\n",
      "        10         918.2127           11.37m\n",
      "        34         285.3780           12.90m\n",
      "        20         374.2802            4.01m\n",
      "        22         372.7802            4.04m\n",
      "         7         864.7099           14.47m\n",
      "        11         862.4264           11.36m\n",
      "[CV 1/5; 34/36] END learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=100;, score=-48.766 total time= 1.4min\n",
      "[CV 3/5; 34/36] START learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        19         589.3430            4.79m\n",
      "        52         191.9042           10.64m\n",
      "        21         342.2589            3.93m\n",
      "        23         353.5371            3.95m\n",
      "         1        2071.7588           11.42m\n",
      "        35         275.2874           12.83m\n",
      "         8         804.4902           14.37m\n",
      "        20         542.1754            4.66m\n",
      "[CV 3/5; 33/36] END learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=50;, score=-44.983 total time= 3.1min\n",
      "[CV 4/5; 34/36] START learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2        1867.5887           11.41m\n",
      "        43         234.7782            1.57m\n",
      "        22         317.5703            3.87m\n",
      "         9         748.4951           14.24m\n",
      "        24         337.4350            3.87m\n",
      "         1        1990.1140           12.43m\n",
      "         3        1699.9660           11.12m\n",
      "        23         297.2018            3.77m\n",
      "        53         189.9965           10.56m\n",
      "         4        1563.5471           11.27m\n",
      "         2        1783.6388           13.73m\n",
      "        10         703.4372           14.24m\n",
      "        60         177.7485           32.26m\n",
      "        36         268.2611           12.92m\n",
      "        59         174.2633           30.02m\n",
      "        25         327.1189            3.80m\n",
      "         5        1450.8240           11.16m\n",
      "        24         279.3918            3.67m\n",
      "[CV 5/5; 33/36] END learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=50;, score=-37.890 total time= 3.4min\n",
      "[CV 5/5; 34/36] START learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=100\n",
      "        44         231.6814            1.36m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         3        1622.5956           13.97m\n",
      "        11         663.6974           14.14m\n",
      "         6        1358.2461           11.17m\n",
      "         1        1937.3950           11.55m\n",
      "        26         318.0424            3.71m\n",
      "         4        1487.5753           14.34m\n",
      "        37         260.2058           12.89m\n",
      "        12         627.7032           14.01m\n",
      "         7        1256.2808           11.03m\n",
      "        54         188.3562           10.48m\n",
      "         2        1728.1372           11.21m\n",
      "        45         228.1028            1.15m\n",
      "         5        1378.2973           13.83m\n",
      "         3        1556.8507           11.23m\n",
      "        13         599.1643           13.94m\n",
      "        27         310.9255            3.61m\n",
      "         8        1154.3691           11.44m\n",
      "         6        1280.2275           13.41m\n",
      "        38         254.4806           12.81m\n",
      "         4        1398.1410           11.34m\n",
      "         9        1072.1757           11.59m\n",
      "        14         575.8698           13.92m\n",
      "         7        1196.8281           13.00m\n",
      "        60         174.2610           29.25m\n",
      "         5        1271.6599           11.17m\n",
      "        28         300.5596            3.50m\n",
      "        55         187.5595           10.38m\n",
      "        61         177.7484           31.40m\n",
      "        46         224.9301           55.49s\n",
      "        10        1005.1217           11.71m\n",
      "         8        1122.2238           12.67m\n",
      "         6        1166.7950           10.96m\n",
      "        15         550.2810           13.87m\n",
      "        39         249.2253           12.80m\n",
      "        29         293.1668            3.39m\n",
      "         7        1073.2469           11.09m\n",
      "         9        1045.4662           12.62m\n",
      "        11         945.1612           11.74m\n",
      "        16         517.8840           13.88m\n",
      "         8         996.7553           11.21m\n",
      "        56         187.0041           10.26m\n",
      "        10         983.2791           12.57m\n",
      "        12         904.1123           11.82m\n",
      "        30         288.2486            3.28m\n",
      "         9         933.0346           11.10m\n",
      "        47         222.1775           42.52s\n",
      "        11         936.8289           12.42m\n",
      "        17         492.1818           13.85m\n",
      "        13         859.5845           11.83m\n",
      "        40         245.7673           12.82m\n",
      "        10         878.9572           11.37m\n",
      "        12         894.7486           12.43m\n",
      "[CV 4/5; 34/36] END learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=100;, score=-35.687 total time= 1.7min\n",
      "[CV 1/5; 35/36] START learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        14         822.7450           11.72m\n",
      "        31         273.3476            3.17m\n",
      "        18         465.7397           13.91m\n",
      "        57         185.7264           10.14m\n",
      "        61         174.2587           28.56m\n",
      "        11         837.4894           11.31m\n",
      "        62         177.7484           30.60m\n",
      "         1        1829.2434            6.23m\n",
      "        15         791.2214           11.86m\n",
      "        12         799.2721           11.15m\n",
      "        48         219.8878           28.80s\n",
      "         2        1662.6300            6.03m\n",
      "        32         265.5391            3.04m\n",
      "        41         241.5929           12.85m\n",
      "        19         435.3620           13.94m\n",
      "        16         751.6543           11.89m\n",
      "        13         768.9396           11.06m\n",
      "         3        1526.4757            6.53m\n",
      "        58         185.1496           10.03m\n",
      "        20         410.0145           13.99m\n",
      "        14         742.2477           11.13m\n",
      "        17         718.5332           11.97m\n",
      "         4        1403.4157            6.39m\n",
      "        33         254.6123            2.94m\n",
      "        49         216.1546           14.61s\n",
      "        42         237.9584           12.87m\n",
      "        15         649.4176           11.05m\n",
      "         5        1301.0499            6.10m\n",
      "        18         647.4476           12.03m\n",
      "        21         389.2620           14.07m\n",
      "         6        1216.7633            5.90m\n",
      "        34         244.4534            2.81m\n",
      "        16         572.3797           11.11m\n",
      "        62         174.2571           27.84m\n",
      "[CV 3/5; 24/36] END learning_rate=0.5, max_depth=200, min_samples_split=20, n_estimators=100;, score=-36.963 total time=45.4min\n",
      "[CV 2/5; 35/36] START learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        59         184.1935            9.91m\n",
      "        19         589.3430           12.02m\n",
      "         7        1146.9396            5.88m\n",
      "        22         372.7802           14.05m\n",
      "        17         507.3181           11.00m\n",
      "         1        1636.4172            6.32m\n",
      "        50         213.6892            0.00s\n",
      "[CV 2/5; 31/36] END learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=50;, score=-53.534 total time=12.3min\n",
      "[CV 3/5; 35/36] START learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        63         177.7484           29.87m\n",
      "        43         234.7782           12.93m\n",
      "         8        1082.6262            5.70m\n",
      "         2        1455.8065            6.06m\n",
      "        18         454.8736           10.93m\n",
      "        20         542.1754           12.06m\n",
      "[CV 3/5; 34/36] END learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=100;, score=-44.983 total time= 3.0min\n",
      "[CV 4/5; 35/36] START learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        35         238.5792            2.69m\n",
      "        23         353.5371           13.93m\n",
      "         1        2085.5330            5.53m\n",
      "         9        1023.6301            5.53m\n",
      "         3        1304.6655            6.25m\n",
      "        19         412.6680           10.84m\n",
      "         1        2032.7963            6.77m\n",
      "        60         183.3352            9.76m\n",
      "         2        1895.8597            5.40m\n",
      "        10         967.9317            5.46m\n",
      "        24         337.4350           13.84m\n",
      "        36         231.9577            2.54m\n",
      "         3        1740.0254            5.27m\n",
      "        20         374.2802           10.73m\n",
      "         2        1876.5837            6.85m\n",
      "         4        1176.7757            6.51m\n",
      "        44         231.6814           12.88m\n",
      "        11         916.9232            5.29m\n",
      "         4        1613.9833            5.28m\n",
      "         3        1738.0182            6.69m\n",
      "        21         342.2589           10.67m\n",
      "         5        1072.1289            6.50m\n",
      "        25         327.1189           13.76m\n",
      "         5        1507.5033            5.19m\n",
      "        61         182.6307            9.61m\n",
      "        37         227.7825            2.39m\n",
      "         4        1622.6640            6.28m\n",
      "        12         874.4966            5.29m\n",
      "        22         317.5703           10.59m\n",
      "         6         983.8738            6.48m\n",
      "         6        1414.1059            5.09m\n",
      "        45         228.1028           12.74m\n",
      "         5        1525.0663            6.07m\n",
      "        26         318.0424           13.68m\n",
      "        13         839.4761            5.22m\n",
      "        23         297.2018           10.53m\n",
      "         7        1314.1593            5.02m\n",
      "         7         909.9331            6.34m\n",
      "         6        1444.6463            5.86m\n",
      "        38         222.5428            2.24m\n",
      "        14         731.9300            5.11m\n",
      "         8        1210.9630            5.01m\n",
      "        64         177.7483           29.15m\n",
      "        24         279.3918           10.50m\n",
      "[CV 5/5; 34/36] END learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=100;, score=-37.890 total time= 3.3min\n",
      "[CV 5/5; 35/36] START learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=50\n",
      "        27         310.9255           13.59m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         7        1350.2382            5.66m\n",
      "         8         844.3093            6.30m\n",
      "        62         181.9400            9.45m\n",
      "        46         224.9301           12.63m\n",
      "        15         638.4074            5.00m\n",
      "[CV 1/5; 35/36] END learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=50;, score=-49.755 total time= 2.2min\n",
      "[CV 1/5; 36/36] START learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "         1        1959.7732            5.53m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         9        1127.6277            5.01m\n",
      "         8        1269.5944            5.49m\n",
      "         9         787.2004            6.13m\n",
      "        39         217.9079            2.08m\n",
      "         2        1779.8312            5.47m\n",
      "         1        1829.2434           12.17m\n",
      "        28         300.5596           13.57m\n",
      "        10        1056.2594            4.93m\n",
      "         9        1188.6413            5.36m\n",
      "        10         744.3720            6.03m\n",
      "         3        1611.7698            5.26m\n",
      "         2        1662.6300           13.34m\n",
      "        11         996.2074            4.87m\n",
      "        10        1132.5618            5.28m\n",
      "        63         181.4892            9.30m\n",
      "         4        1489.2156            5.22m\n",
      "        11         703.7140            5.87m\n",
      "        40         214.7517            1.92m\n",
      "        47         222.1775           12.64m\n",
      "         3        1526.4757           13.40m\n",
      "        12         951.2674            4.82m\n",
      "        29         293.1668           13.65m\n",
      "        11        1087.0386            5.20m\n",
      "         5        1375.8067            5.06m\n",
      "        12         666.1238            5.62m\n",
      "         6        1285.6532            5.02m\n",
      "         4        1403.4157           13.73m\n",
      "        12        1049.1190            5.14m\n",
      "        13         903.7812            4.81m\n",
      "        13         636.8024            5.51m\n",
      "        41         211.4894            1.75m\n",
      "         7        1201.6102            4.91m\n",
      "         5        1301.0499           12.99m\n",
      "        64         181.0777            9.11m\n",
      "        30         288.2486           13.66m\n",
      "        13        1007.0257            5.02m\n",
      "        14         867.1779            4.72m\n",
      "        65         177.7483           28.38m\n",
      "        14         597.9344            5.38m\n",
      "         8        1132.1215            4.82m\n",
      "         6        1216.7633           12.60m\n",
      "        48         219.8878           12.60m\n",
      "        14         966.6850            4.92m\n",
      "        15         835.2937            4.66m\n",
      "         9        1077.5586            4.72m\n",
      "         7        1146.9396           12.46m\n",
      "        15         565.5543            5.27m\n",
      "        42         208.6427            1.58m\n",
      "        31         273.3476           13.72m\n",
      "        15         864.3404            4.83m\n",
      "        10        1029.6863            4.65m\n",
      "        16         794.3297            4.61m\n",
      "         8        1082.6262           12.51m\n",
      "        65         180.5788            8.95m\n",
      "        16         538.9507            5.16m\n",
      "        11         988.2032            4.57m\n",
      "        16         759.7702            4.72m\n",
      "        49         216.1546           12.53m\n",
      "         9        1023.6301           12.73m\n",
      "        32         265.5391           13.65m\n",
      "        17         759.9318            4.65m\n",
      "        12         956.0206            4.49m\n",
      "[CV 5/5; 35/36] END learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=50;, score=-42.009 total time= 1.4min\n",
      "[CV 2/5; 36/36] START learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "        43         206.5438            1.40m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        17         515.7243            5.12m\n",
      "        17         674.5723            4.61m\n",
      "        10         967.9317           12.88m\n",
      "         1        1636.4172           13.31m\n",
      "        18         687.0741            4.59m\n",
      "        18         601.5300            4.51m\n",
      "        66         180.2040            8.77m\n",
      "        18         495.6251            5.05m\n",
      "         2        1455.8065           12.34m\n",
      "        33         254.6123           13.63m\n",
      "        11         916.9232           12.86m\n",
      "        50         213.6892           12.41m\n",
      "        44         204.2614            1.22m\n",
      "        19         534.6075            4.38m\n",
      "        19         627.8862            4.51m\n",
      "         3        1304.6655           12.12m\n",
      "        66         177.7483           27.64m\n",
      "        19         485.5348            4.96m\n",
      "        12         874.4966           13.05m\n",
      "        20         481.9641            4.26m\n",
      "         4        1176.7757           12.31m\n",
      "        34         244.4534           13.54m\n",
      "        20         579.3118            4.45m\n",
      "[CV 3/5; 35/36] END learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=50;, score=-44.507 total time= 3.0min\n",
      "[CV 3/5; 36/36] START learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        67         179.9508            8.57m\n",
      "         5        1072.1289           11.99m\n",
      "        20         473.8246            4.89m\n",
      "        13         839.4761           13.17m\n",
      "        21         436.8005            4.14m\n",
      "[CV 4/5; 35/36] END learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=50;, score=-39.086 total time= 3.0min\n",
      "[CV 4/5; 36/36] START learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "        45         202.7073            1.03m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        2085.5330           11.10m\n",
      "        51         212.2418           12.35m\n",
      "         6         983.8738           11.92m\n",
      "        35         238.5792           13.45m\n",
      "         1        2032.7963           12.43m\n",
      "         2        1895.8597           11.15m\n",
      "        14         731.9300           13.29m\n",
      "        21         464.2504            4.81m\n",
      "         7         909.9331           11.61m\n",
      "         2        1876.5837           12.34m\n",
      "         3        1740.0254           11.27m\n",
      "         8         844.3093           11.57m\n",
      "        46         200.8083           50.19s\n",
      "        68         179.7609            8.39m\n",
      "         4        1613.9833           11.20m\n",
      "         3        1738.0182           12.18m\n",
      "        15         638.4074           13.38m\n",
      "[CV 1/5; 36/36] END learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=100;, score=-49.755 total time= 2.4min\n",
      "[CV 5/5; 36/36] START learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=100\n",
      "        22         442.1734            4.67m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        36         231.9577           13.39m\n",
      "         9         787.2004           11.64m\n",
      "         5        1507.5033           11.05m\n",
      "         4        1622.6640           11.99m\n",
      "        52         210.3464           12.28m\n",
      "         1        1959.7732           14.48m\n",
      "        23         423.3980            4.55m\n",
      "         6        1414.1059           11.03m\n",
      "         5        1525.0663           11.81m\n",
      "        10         744.3720           11.91m\n",
      "         2        1779.8312           12.61m\n",
      "        37         227.7825           13.30m\n",
      "        67         177.7483           26.91m\n",
      "        47         199.3588           38.24s\n",
      "         7        1314.1593           11.04m\n",
      "         6        1444.6463           11.72m\n",
      "         3        1611.7698           11.99m\n",
      "        69         179.4866            8.21m\n",
      "        24         408.5319            4.44m\n",
      "        11         703.7140           12.23m\n",
      "         8        1210.9630           11.05m\n",
      "         7        1350.2382           11.65m\n",
      "         4        1489.2156           11.81m\n",
      "        53         208.9888           12.17m\n",
      "        38         222.5428           13.20m\n",
      "        12         666.1238           12.24m\n",
      "         8        1269.5944           11.55m\n",
      "         5        1375.8067           11.65m\n",
      "         9        1127.6277           11.17m\n",
      "        25         395.8060            4.32m\n",
      "        48         197.7791           25.94s\n",
      "         6        1285.6532           11.58m\n",
      "         9        1188.6413           11.61m\n",
      "        10        1056.2594           11.24m\n",
      "        70         179.2648            8.00m\n",
      "        13         636.8024           12.54m\n",
      "        39         217.9079           13.10m\n",
      "         7        1201.6102           11.41m\n",
      "        10        1132.5618           11.61m\n",
      "        11         996.2074           11.23m\n",
      "        26         385.1841            4.23m\n",
      "        54         207.7623           12.04m\n",
      "         8        1132.1215           11.40m\n",
      "        14         597.9344           12.66m\n",
      "        11        1087.0386           11.74m\n",
      "        12         951.2674           11.37m\n",
      "         9        1077.5586           11.24m\n",
      "        49         196.2088           13.21s\n",
      "        27         363.5207            4.10m\n",
      "        71         179.1441            7.78m\n",
      "        68         177.7482           26.11m\n",
      "        40         214.7517           13.03m\n",
      "        12        1049.1190           11.67m\n",
      "        15         565.5543           12.86m\n",
      "        13         903.7812           11.39m\n",
      "        10        1029.6863           11.32m\n",
      "        13        1007.0257           11.69m\n",
      "        14         867.1779           11.40m\n",
      "        55         205.5633           11.93m\n",
      "        11         988.2032           11.47m\n",
      "        28         343.7426            4.01m\n",
      "        16         538.9507           13.09m\n",
      "        50         194.9368            0.00s\n",
      "[CV 2/5; 33/36] END learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=50;, score=-54.199 total time=11.2min\n",
      "        41         211.4894           12.95m\n",
      "        14         966.6850           11.65m\n",
      "        15         835.2937           11.35m\n",
      "        12         956.0206           11.28m\n",
      "[CV 5/5; 36/36] END learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=100;, score=-42.009 total time= 1.5min\n",
      "        72         178.9601            7.57m\n",
      "        17         515.7243           12.96m\n",
      "        15         864.3404           11.47m\n",
      "        29         327.4251            3.85m\n",
      "        16         794.3297           11.25m\n",
      "        56         204.4452           11.68m\n",
      "        18         495.6251           12.65m\n",
      "        42         208.6427           12.75m\n",
      "        16         759.7702           11.21m\n",
      "        17         759.9318           11.28m\n",
      "        30         313.7940            3.68m\n",
      "        19         485.5348           12.47m\n",
      "        17         674.5723           11.03m\n",
      "        73         178.8490            7.32m\n",
      "        69         177.7482           25.28m\n",
      "        18         687.0741           11.20m\n",
      "        43         206.5438           12.60m\n",
      "        18         601.5300           10.86m\n",
      "        20         473.8246           12.28m\n",
      "        57         203.4435           11.49m\n",
      "        31         302.9044            3.51m\n",
      "        19         534.6075           10.63m\n",
      "        19         627.8862           11.08m\n",
      "        21         464.2504           12.09m\n",
      "        20         481.9641           10.47m\n",
      "        74         178.7767            7.07m\n",
      "        44         204.2614           12.43m\n",
      "        20         579.3118           11.03m\n",
      "[CV 3/5; 36/36] END learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=100;, score=-44.507 total time= 2.8min\n",
      "        32         294.4468            3.35m\n",
      "        58         201.9063           11.20m\n",
      "        22         442.1734           11.91m\n",
      "        21         436.8005           10.27m\n",
      "[CV 4/5; 36/36] END learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=100;, score=-39.086 total time= 2.7min\n",
      "        23         423.3980           11.74m\n",
      "        33         287.6533            3.17m\n",
      "        45         202.7073           12.23m\n",
      "        75         178.6538            6.81m\n",
      "        70         177.7482           24.37m\n",
      "        24         408.5319           11.56m\n",
      "        59         201.0322           10.97m\n",
      "        34         282.1087            2.98m\n",
      "        46         200.8083           12.01m\n",
      "        25         395.8060           11.39m\n",
      "        35         274.6221            2.79m\n",
      "        60         198.9924           10.72m\n",
      "        76         178.5844            6.56m\n",
      "        26         385.1841           11.26m\n",
      "        47         199.3588           11.79m\n",
      "        36         269.3055            2.60m\n",
      "        27         363.5207           11.09m\n",
      "        61         198.3359           10.44m\n",
      "        71         177.7482           23.48m\n",
      "        48         197.7791           11.59m\n",
      "        77         178.5314            6.30m\n",
      "        28         343.7426           11.04m\n",
      "        37         263.9154            2.43m\n",
      "        29         327.4251           10.92m\n",
      "        62         197.6703           10.18m\n",
      "        49         196.2088           11.39m\n",
      "        38         259.5336            2.26m\n",
      "        78         178.4662            6.03m\n",
      "[CV 2/5; 30/36] END learning_rate=0.1, max_depth=100, min_samples_split=10, n_estimators=100;, score=-53.856 total time=21.4min\n",
      "        30         313.7940           10.79m\n",
      "        50         194.9368           11.18m\n",
      "        63         196.6913            9.90m\n",
      "        39         255.5725            2.08m\n",
      "        31         302.9044           10.66m\n",
      "        72         177.7482           22.60m\n",
      "        32         294.4468           10.54m\n",
      "        51         193.7481           10.97m\n",
      "        40         252.5241            1.90m\n",
      "        64         195.8639            9.64m\n",
      "        33         287.6533           10.43m\n",
      "        52         192.4981           10.77m\n",
      "        41         248.0785            1.72m\n",
      "        65         194.4042            9.37m\n",
      "        34         282.1087           10.28m\n",
      "        42         244.6748            1.54m\n",
      "        35         274.6221           10.15m\n",
      "        53         190.4524           10.60m\n",
      "        66         193.1546            9.08m\n",
      "        73         177.7482           21.74m\n",
      "        36         269.3055           10.02m\n",
      "        43         241.2646            1.36m\n",
      "        67         192.1628            8.82m\n",
      "        54         189.5650           10.44m\n",
      "        37         263.9154            9.93m\n",
      "        44         238.6602            1.16m\n",
      "        55         188.6852           10.24m\n",
      "        68         191.3539            8.56m\n",
      "        38         259.5336            9.92m\n",
      "        74         177.7482           20.86m\n",
      "        45         235.5272           58.60s\n",
      "        39         255.5725            9.83m\n",
      "        69         190.2540            8.28m\n",
      "        56         187.1261           10.05m\n",
      "        46         233.1072           47.08s\n",
      "        40         252.5241            9.74m\n",
      "        70         189.4281            8.03m\n",
      "        47         230.6610           35.54s\n",
      "        57         185.7197            9.91m\n",
      "        41         248.0785            9.68m\n",
      "        75         177.7482           20.00m\n",
      "        71         188.8853            7.77m\n",
      "        48         228.1528           23.86s\n",
      "        58         184.6561            9.70m\n",
      "        42         244.6748            9.62m\n",
      "        72         188.0595            7.48m\n",
      "        49         226.8002           12.02s\n",
      "        59         183.7704            9.50m\n",
      "        43         241.2646            9.59m\n",
      "        76         177.7482           19.13m\n",
      "        73         187.5384            7.21m\n",
      "        44         238.6602            9.49m\n",
      "        50         223.7307            0.00s\n",
      "[CV 2/5; 35/36] END learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=50;, score=-53.466 total time=10.1min\n",
      "        60         183.0668            9.30m\n",
      "        74         186.9700            6.93m\n",
      "        45         235.5272            9.39m\n",
      "        61         182.4722            9.10m\n",
      "        75         186.6335            6.64m\n",
      "        77         177.7482           18.26m\n",
      "        46         233.1072            9.29m\n",
      "        76         186.2543            6.36m\n",
      "        62         182.0007            8.89m\n",
      "        47         230.6610            9.18m\n",
      "        77         186.0353            6.10m\n",
      "        63         181.3990            8.71m\n",
      "        48         228.1528            9.10m\n",
      "        78         177.7482           17.39m\n",
      "        78         185.7747            5.83m\n",
      "        49         226.8002            8.99m\n",
      "        64         180.8770            8.53m\n",
      "        79         185.5252            5.55m\n",
      "        50         223.7307            8.88m\n",
      "        79         177.7482           16.55m\n",
      "        65         180.5080            8.34m\n",
      "        80         185.1978            5.29m\n",
      "        51         222.0599            8.76m\n",
      "        66         180.1528            8.13m\n",
      "        81         184.9857            5.03m\n",
      "        52         219.4509            8.65m\n",
      "        80         177.7482           15.71m\n",
      "        67         179.9127            7.91m\n",
      "        82         184.7708            4.77m\n",
      "        53         216.3724            8.56m\n",
      "        83         184.5457            4.50m\n",
      "        68         179.6285            7.69m\n",
      "        54         213.4228            8.49m\n",
      "        81         177.7482           14.85m\n",
      "        84         184.2785            4.23m\n",
      "        69         179.3907            7.48m\n",
      "        55         211.3064            8.36m\n",
      "        85         184.0724            3.96m\n",
      "        70         179.2555            7.26m\n",
      "        56         208.4857            8.24m\n",
      "        82         177.7482           14.01m\n",
      "        86         183.8279            3.70m\n",
      "        57         207.6261            8.11m\n",
      "        71         179.1333            7.04m\n",
      "        87         183.6904            3.44m\n",
      "        58         205.9488            8.01m\n",
      "        83         177.7482           13.18m\n",
      "        88         183.5845            3.17m\n",
      "        72         178.9444            6.83m\n",
      "        59         204.7003            7.91m\n",
      "        89         183.4589            2.91m\n",
      "        73         178.8141            6.62m\n",
      "        84         177.7482           12.34m\n",
      "        90         183.2931            2.64m\n",
      "        60         203.9815            7.79m\n",
      "        74         178.7333            6.39m\n",
      "        85         177.7482           11.50m\n",
      "        91         183.1303            2.38m\n",
      "        61         202.8839            7.65m\n",
      "        75         178.6197            6.17m\n",
      "        92         183.0204            2.11m\n",
      "        62         201.3219            7.49m\n",
      "        76         178.5299            5.95m\n",
      "        93         182.8700            1.85m\n",
      "        86         177.7482           10.70m\n",
      "        63         200.9000            7.35m\n",
      "        77         178.4520            5.71m\n",
      "        94         182.7129            1.59m\n",
      "        64         199.4840            7.20m\n",
      "        87         177.7482            9.90m\n",
      "        95         182.5478            1.32m\n",
      "[CV 2/5; 32/36] END learning_rate=0.1, max_depth=100, min_samples_split=20, n_estimators=100;, score=-53.304 total time=25.1min\n",
      "        78         178.3890            5.48m\n",
      "[CV 2/5; 34/36] END learning_rate=0.1, max_depth=200, min_samples_split=10, n_estimators=100;, score=-53.519 total time=19.5min\n",
      "        65         198.1734            7.05m\n",
      "        66         197.6662            6.88m\n",
      "        88         177.7482            9.09m\n",
      "        67         197.3648            6.72m\n",
      "        89         177.7482            8.29m\n",
      "        68         196.1115            6.53m\n",
      "        69         195.2081            6.36m\n",
      "        90         177.7482            7.50m\n",
      "        70         194.4575            6.20m\n",
      "        91         177.7482            6.72m\n",
      "        71         193.2391            6.03m\n",
      "        72         192.3157            5.84m\n",
      "        92         177.7482            5.95m\n",
      "        73         191.6323            5.67m\n",
      "        74         191.1699            5.48m\n",
      "        93         177.7482            5.19m\n",
      "        75         190.4369            5.30m\n",
      "        76         189.9974            5.11m\n",
      "        94         177.7482            4.43m\n",
      "        77         189.4332            4.91m\n",
      "        78         188.6654            4.71m\n",
      "        95         177.7482            3.68m\n",
      "        79         188.2408            4.51m\n",
      "        96         177.7482            2.93m\n",
      "        80         187.6932            4.30m\n",
      "        81         187.2413            4.10m\n",
      "        97         177.7482            2.19m\n",
      "        82         187.0025            3.90m\n",
      "        98         177.7482            1.45m\n",
      "        83         186.7204            3.70m\n",
      "        84         186.4531            3.50m\n",
      "        99         177.7482           43.51s\n",
      "        85         186.1563            3.28m\n",
      "        86         185.8676            3.08m\n",
      "       100         177.7482            0.00s\n",
      "[CV 2/5; 22/36] END learning_rate=0.5, max_depth=200, min_samples_split=10, n_estimators=100;, score=-54.360 total time=72.2min\n",
      "        87         185.7062            2.87m\n",
      "        88         185.2589            2.65m\n",
      "        89         184.8052            2.44m\n",
      "        90         184.4841            2.22m\n",
      "        91         184.2254            2.00m\n",
      "        92         184.0032            1.78m\n",
      "        93         183.8075            1.56m\n",
      "        94         183.4715            1.34m\n",
      "        95         183.3212            1.12m\n",
      "        96         183.0965           53.83s\n",
      "        97         182.9505           40.46s\n",
      "[CV 2/5; 36/36] END learning_rate=0.1, max_depth=200, min_samples_split=20, n_estimators=100;, score=-52.988 total time=21.8min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1310.7835            6.92m\n",
      "         2        1044.0607            7.10m\n",
      "         3         902.5346            7.90m\n",
      "         4         494.6454            8.41m\n",
      "         5         375.6434            8.94m\n",
      "         6         286.4035            9.15m\n",
      "         7         239.6305            9.94m\n",
      "         8         223.0670           10.32m\n",
      "         9         211.7757           10.59m\n",
      "        10         206.2911           10.85m\n",
      "        11         200.8614           10.94m\n",
      "        12         195.8051           11.31m\n",
      "        13         188.4085           11.65m\n",
      "        14         185.0981           11.74m\n",
      "        15         183.3992           11.99m\n",
      "        16         182.0495           11.93m\n",
      "        17         180.9652           11.84m\n",
      "        18         179.9899           11.63m\n",
      "        19         179.2001           11.53m\n",
      "        20         178.7111           11.38m\n",
      "        21         178.3955           11.35m\n",
      "        22         178.1008           11.28m\n",
      "        23         177.6846           11.20m\n",
      "        24         177.3440           11.01m\n",
      "        25         177.1564           10.97m\n",
      "        26         176.9954           10.98m\n",
      "        27         176.7238           10.89m\n",
      "        28         176.5431           10.71m\n"
     ]
    }
   ],
   "source": [
    "# X = train_foil.drop(columns=['median_foil','id']).to_numpy()\n",
    "# y = train_foil['median_foil'].to_numpy()\n",
    "\n",
    "# with parallel_backend('threading',n_jobs=10):\n",
    "#     foil_gs = GridSearchCV(\n",
    "#         gbr['foil'],params,\n",
    "#         scoring='neg_root_mean_squared_error',\n",
    "#         verbose=10\n",
    "#     ).fit(\n",
    "#         X,y\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mGradientBoostingRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'squared_error'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msubsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'friedman_mse'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_impurity_decrease\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_leaf_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvalidation_fraction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mn_iter_no_change\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mccp_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Gradient Boosting for regression.\n",
      "\n",
      "This estimator builds an additive model in a forward stage-wise fashion; it\n",
      "allows for the optimization of arbitrary differentiable loss functions. In\n",
      "each stage a regression tree is fit on the negative gradient of the given\n",
      "loss function.\n",
      "\n",
      ":class:`sklearn.ensemble.HistGradientBoostingRegressor` is a much faster\n",
      "variant of this algorithm for intermediate datasets (`n_samples >= 10_000`).\n",
      "\n",
      "Read more in the :ref:`User Guide <gradient_boosting>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "loss : {'squared_error', 'absolute_error', 'huber', 'quantile'},             default='squared_error'\n",
      "    Loss function to be optimized. 'squared_error' refers to the squared\n",
      "    error for regression. 'absolute_error' refers to the absolute error of\n",
      "    regression and is a robust loss function. 'huber' is a\n",
      "    combination of the two. 'quantile' allows quantile regression (use\n",
      "    `alpha` to specify the quantile).\n",
      "\n",
      "learning_rate : float, default=0.1\n",
      "    Learning rate shrinks the contribution of each tree by `learning_rate`.\n",
      "    There is a trade-off between learning_rate and n_estimators.\n",
      "    Values must be in the range `[0.0, inf)`.\n",
      "\n",
      "n_estimators : int, default=100\n",
      "    The number of boosting stages to perform. Gradient boosting\n",
      "    is fairly robust to over-fitting so a large number usually\n",
      "    results in better performance.\n",
      "    Values must be in the range `[1, inf)`.\n",
      "\n",
      "subsample : float, default=1.0\n",
      "    The fraction of samples to be used for fitting the individual base\n",
      "    learners. If smaller than 1.0 this results in Stochastic Gradient\n",
      "    Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
      "    Choosing `subsample < 1.0` leads to a reduction of variance\n",
      "    and an increase in bias.\n",
      "    Values must be in the range `(0.0, 1.0]`.\n",
      "\n",
      "criterion : {'friedman_mse', 'squared_error'}, default='friedman_mse'\n",
      "    The function to measure the quality of a split. Supported criteria are\n",
      "    \"friedman_mse\" for the mean squared error with improvement score by\n",
      "    Friedman, \"squared_error\" for mean squared error. The default value of\n",
      "    \"friedman_mse\" is generally the best as it can provide a better\n",
      "    approximation in some cases.\n",
      "\n",
      "    .. versionadded:: 0.18\n",
      "\n",
      "min_samples_split : int or float, default=2\n",
      "    The minimum number of samples required to split an internal node:\n",
      "\n",
      "    - If int, values must be in the range `[2, inf)`.\n",
      "    - If float, values must be in the range `(0.0, 1.0]` and `min_samples_split`\n",
      "      will be `ceil(min_samples_split * n_samples)`.\n",
      "\n",
      "    .. versionchanged:: 0.18\n",
      "       Added float values for fractions.\n",
      "\n",
      "min_samples_leaf : int or float, default=1\n",
      "    The minimum number of samples required to be at a leaf node.\n",
      "    A split point at any depth will only be considered if it leaves at\n",
      "    least ``min_samples_leaf`` training samples in each of the left and\n",
      "    right branches.  This may have the effect of smoothing the model,\n",
      "    especially in regression.\n",
      "\n",
      "    - If int, values must be in the range `[1, inf)`.\n",
      "    - If float, values must be in the range `(0.0, 1.0)` and `min_samples_leaf`\n",
      "      will be `ceil(min_samples_leaf * n_samples)`.\n",
      "\n",
      "    .. versionchanged:: 0.18\n",
      "       Added float values for fractions.\n",
      "\n",
      "min_weight_fraction_leaf : float, default=0.0\n",
      "    The minimum weighted fraction of the sum total of weights (of all\n",
      "    the input samples) required to be at a leaf node. Samples have\n",
      "    equal weight when sample_weight is not provided.\n",
      "    Values must be in the range `[0.0, 0.5]`.\n",
      "\n",
      "max_depth : int or None, default=3\n",
      "    Maximum depth of the individual regression estimators. The maximum\n",
      "    depth limits the number of nodes in the tree. Tune this parameter\n",
      "    for best performance; the best value depends on the interaction\n",
      "    of the input variables. If None, then nodes are expanded until\n",
      "    all leaves are pure or until all leaves contain less than\n",
      "    min_samples_split samples.\n",
      "    If int, values must be in the range `[1, inf)`.\n",
      "\n",
      "min_impurity_decrease : float, default=0.0\n",
      "    A node will be split if this split induces a decrease of the impurity\n",
      "    greater than or equal to this value.\n",
      "    Values must be in the range `[0.0, inf)`.\n",
      "\n",
      "    The weighted impurity decrease equation is the following::\n",
      "\n",
      "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "                            - N_t_L / N_t * left_impurity)\n",
      "\n",
      "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "\n",
      "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "    if ``sample_weight`` is passed.\n",
      "\n",
      "    .. versionadded:: 0.19\n",
      "\n",
      "init : estimator or 'zero', default=None\n",
      "    An estimator object that is used to compute the initial predictions.\n",
      "    ``init`` has to provide :term:`fit` and :term:`predict`. If 'zero', the\n",
      "    initial raw predictions are set to zero. By default a\n",
      "    ``DummyEstimator`` is used, predicting either the average target value\n",
      "    (for loss='squared_error'), or a quantile for the other losses.\n",
      "\n",
      "random_state : int, RandomState instance or None, default=None\n",
      "    Controls the random seed given to each Tree estimator at each\n",
      "    boosting iteration.\n",
      "    In addition, it controls the random permutation of the features at\n",
      "    each split (see Notes for more details).\n",
      "    It also controls the random splitting of the training data to obtain a\n",
      "    validation set if `n_iter_no_change` is not None.\n",
      "    Pass an int for reproducible output across multiple function calls.\n",
      "    See :term:`Glossary <random_state>`.\n",
      "\n",
      "max_features : {'auto', 'sqrt', 'log2'}, int or float, default=None\n",
      "    The number of features to consider when looking for the best split:\n",
      "\n",
      "    - If int, values must be in the range `[1, inf)`.\n",
      "    - If float, values must be in the range `(0.0, 1.0]` and the features\n",
      "      considered at each split will be `max(1, int(max_features * n_features_in_))`.\n",
      "    - If \"auto\", then `max_features=n_features`.\n",
      "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "    - If \"log2\", then `max_features=log2(n_features)`.\n",
      "    - If None, then `max_features=n_features`.\n",
      "\n",
      "    Choosing `max_features < n_features` leads to a reduction of variance\n",
      "    and an increase in bias.\n",
      "\n",
      "    Note: the search for a split does not stop until at least one\n",
      "    valid partition of the node samples is found, even if it requires to\n",
      "    effectively inspect more than ``max_features`` features.\n",
      "\n",
      "alpha : float, default=0.9\n",
      "    The alpha-quantile of the huber loss function and the quantile\n",
      "    loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n",
      "    Values must be in the range `(0.0, 1.0)`.\n",
      "\n",
      "verbose : int, default=0\n",
      "    Enable verbose output. If 1 then it prints progress and performance\n",
      "    once in a while (the more trees the lower the frequency). If greater\n",
      "    than 1 then it prints progress and performance for every tree.\n",
      "    Values must be in the range `[0, inf)`.\n",
      "\n",
      "max_leaf_nodes : int, default=None\n",
      "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "    Best nodes are defined as relative reduction in impurity.\n",
      "    Values must be in the range `[2, inf)`.\n",
      "    If None, then unlimited number of leaf nodes.\n",
      "\n",
      "warm_start : bool, default=False\n",
      "    When set to ``True``, reuse the solution of the previous call to fit\n",
      "    and add more estimators to the ensemble, otherwise, just erase the\n",
      "    previous solution. See :term:`the Glossary <warm_start>`.\n",
      "\n",
      "validation_fraction : float, default=0.1\n",
      "    The proportion of training data to set aside as validation set for\n",
      "    early stopping. Values must be in the range `(0.0, 1.0)`.\n",
      "    Only used if ``n_iter_no_change`` is set to an integer.\n",
      "\n",
      "    .. versionadded:: 0.20\n",
      "\n",
      "n_iter_no_change : int, default=None\n",
      "    ``n_iter_no_change`` is used to decide if early stopping will be used\n",
      "    to terminate training when validation score is not improving. By\n",
      "    default it is set to None to disable early stopping. If set to a\n",
      "    number, it will set aside ``validation_fraction`` size of the training\n",
      "    data as validation and terminate training when validation score is not\n",
      "    improving in all of the previous ``n_iter_no_change`` numbers of\n",
      "    iterations.\n",
      "    Values must be in the range `[1, inf)`.\n",
      "\n",
      "    .. versionadded:: 0.20\n",
      "\n",
      "tol : float, default=1e-4\n",
      "    Tolerance for the early stopping. When the loss is not improving\n",
      "    by at least tol for ``n_iter_no_change`` iterations (if set to a\n",
      "    number), the training stops.\n",
      "    Values must be in the range `[0.0, inf)`.\n",
      "\n",
      "    .. versionadded:: 0.20\n",
      "\n",
      "ccp_alpha : non-negative float, default=0.0\n",
      "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "    subtree with the largest cost complexity that is smaller than\n",
      "    ``ccp_alpha`` will be chosen. By default, no pruning is performed.\n",
      "    Values must be in the range `[0.0, inf)`.\n",
      "    See :ref:`minimal_cost_complexity_pruning` for details.\n",
      "\n",
      "    .. versionadded:: 0.22\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "feature_importances_ : ndarray of shape (n_features,)\n",
      "    The impurity-based feature importances.\n",
      "    The higher, the more important the feature.\n",
      "    The importance of a feature is computed as the (normalized)\n",
      "    total reduction of the criterion brought by that feature.  It is also\n",
      "    known as the Gini importance.\n",
      "\n",
      "    Warning: impurity-based feature importances can be misleading for\n",
      "    high cardinality features (many unique values). See\n",
      "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "\n",
      "oob_improvement_ : ndarray of shape (n_estimators,)\n",
      "    The improvement in loss (= deviance) on the out-of-bag samples\n",
      "    relative to the previous iteration.\n",
      "    ``oob_improvement_[0]`` is the improvement in\n",
      "    loss of the first stage over the ``init`` estimator.\n",
      "    Only available if ``subsample < 1.0``\n",
      "\n",
      "train_score_ : ndarray of shape (n_estimators,)\n",
      "    The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
      "    model at iteration ``i`` on the in-bag sample.\n",
      "    If ``subsample == 1`` this is the deviance on the training data.\n",
      "\n",
      "loss_ : LossFunction\n",
      "    The concrete ``LossFunction`` object.\n",
      "\n",
      "    .. deprecated:: 1.1\n",
      "         Attribute `loss_` was deprecated in version 1.1 and will be\n",
      "        removed in 1.3.\n",
      "\n",
      "init_ : estimator\n",
      "    The estimator that provides the initial predictions.\n",
      "    Set via the ``init`` argument or ``loss.init_estimator``.\n",
      "\n",
      "estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, 1)\n",
      "    The collection of fitted sub-estimators.\n",
      "\n",
      "n_estimators_ : int\n",
      "    The number of estimators as selected by early stopping (if\n",
      "    ``n_iter_no_change`` is specified). Otherwise it is set to\n",
      "    ``n_estimators``.\n",
      "\n",
      "n_features_in_ : int\n",
      "    Number of features seen during :term:`fit`.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "\n",
      "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "    Names of features seen during :term:`fit`. Defined only when `X`\n",
      "    has feature names that are all strings.\n",
      "\n",
      "    .. versionadded:: 1.0\n",
      "\n",
      "max_features_ : int\n",
      "    The inferred value of max_features.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "HistGradientBoostingRegressor : Histogram-based Gradient Boosting\n",
      "    Classification Tree.\n",
      "sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n",
      "sklearn.ensemble.RandomForestRegressor : A random forest regressor.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The features are always randomly permuted at each split. Therefore,\n",
      "the best found split may vary, even with the same training data and\n",
      "``max_features=n_features``, if the improvement of the criterion is\n",
      "identical for several splits enumerated during the search of the best\n",
      "split. To obtain a deterministic behaviour during fitting,\n",
      "``random_state`` has to be fixed.\n",
      "\n",
      "References\n",
      "----------\n",
      "J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
      "Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
      "\n",
      "J. Friedman, Stochastic Gradient Boosting, 1999\n",
      "\n",
      "T. Hastie, R. Tibshirani and J. Friedman.\n",
      "Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.datasets import make_regression\n",
      ">>> from sklearn.ensemble import GradientBoostingRegressor\n",
      ">>> from sklearn.model_selection import train_test_split\n",
      ">>> X, y = make_regression(random_state=0)\n",
      ">>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "...     X, y, random_state=0)\n",
      ">>> reg = GradientBoostingRegressor(random_state=0)\n",
      ">>> reg.fit(X_train, y_train)\n",
      "GradientBoostingRegressor(random_state=0)\n",
      ">>> reg.predict(X_test[1:2])\n",
      "array([-61...])\n",
      ">>> reg.score(X_test, y_test)\n",
      "0.4...\n",
      "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/spells/lib/python3.11/site-packages/sklearn/ensemble/_gb.py\n",
      "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "GradientBoostingRegressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./pickles/norm_gb.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dump(foil_gs,'./pickles/foil_gb.pkl')\n",
    "# dump(norm_gs,'./pickles/norm_gb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_gs = load('./pickles/norm_gb.pkl')\n",
    "foil_gs = load('./pickles/foil_gb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.380695178665317\n",
      "19.65957104881276\n"
     ]
    }
   ],
   "source": [
    "X = train_foil.drop(columns=['median_foil','id']).to_numpy()\n",
    "y = train_foil['median_foil'].to_numpy()\n",
    "print(mean_squared_error(foil_gs.predict(\n",
    "        X\n",
    "    ),y,squared=False))\n",
    "\n",
    "X = train_norm.drop(columns=['median_normal','id']).to_numpy()\n",
    "y = train_norm['median_normal'].to_numpy()\n",
    "print(mean_squared_error(norm_gs.predict(\n",
    "        X\n",
    "    ),y,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_diff = pd.DataFrame(\n",
    "    {\n",
    "        'true': train_norm['median_normal'].to_numpy(),\n",
    "        'preds':norm_gs.predict(train_norm.drop(columns=['median_normal','id']).to_numpy())\n",
    "    }\n",
    ")\n",
    "norm_diff['diff'] = abs(norm_diff['true'] - norm_diff['preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "foil_diff = pd.DataFrame(\n",
    "    {\n",
    "        'true': train_foil['median_foil'].to_numpy(),\n",
    "        'preds':foil_gs.predict(train_foil.drop(columns=['median_foil','id']).to_numpy())\n",
    "    }\n",
    ")\n",
    "foil_diff['diff'] = abs(foil_diff['true'] - foil_diff['preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8474606186294842"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(norm_diff[norm_diff['diff']<1])/len(norm_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11999.99\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.scatter(norm_diff['true'],norm_diff['preds'])\n",
    "_mx = max(norm_diff['true'])\n",
    "print(_mx)\n",
    "ax.plot((0,_mx),(0,_mx))\n",
    "'';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.scatter(foil_diff['true'],foil_diff['preds'])\n",
    "_mx = max(foil_diff['true'])\n",
    "print(_mx)\n",
    "ax.plot((0,_mx),(0,_mx))\n",
    "'';"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spells",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
