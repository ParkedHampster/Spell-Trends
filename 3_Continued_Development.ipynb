{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continued Development\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook is intended to provide a continuation on\n",
    "the project thus far. Rather than deleting or\n",
    "overwriting sections in the first two notebooks, this\n",
    "notebook is to act as a way to cover \"Next Steps\" as\n",
    "outlined at the end of the second notebook.\n",
    "\n",
    "Because this notebook exists outside of the scope of\n",
    "the original project, this notebook may be messier and\n",
    "won't provide as much extensive detail.\n",
    "\n",
    "Once this discovery reaches a satisfactory point, the\n",
    "project will undergo the same restructuring that was\n",
    "taken during its creation, including a recreation of\n",
    "the README and presentation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Because work has been done to clean data that exists as\n",
    "a jumping-off point, this notebook won't recreate the\n",
    "initial data - though it is important to note that\n",
    "newer and more accurate data will become available over\n",
    "time and this may not immediately be taken into account\n",
    "during the processes outlined within."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, \\\n",
    "            GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, \\\n",
    "            cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from joblib import parallel_backend, Parallel, \\\n",
    "            dump, load\n",
    "\n",
    "from _code.cleaner import preprocess\n",
    "\n",
    "from IPython.display import Image, Markdown\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreating Processes\n",
    "\n",
    "Many of the processes used in 2_Modeling.ipynb are\n",
    "still valid that lead up to the actual model process,\n",
    "so these steps will be combined here without explicit\n",
    "explanation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import initial card data set\n",
    "cards = pd.read_parquet('./data/simplified_cards.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform pre-processing of abilities\n",
    "processed_abilities = preprocess(cards['oracle_text'])\n",
    "cards['abilities_list'] = [\n",
    "    abilities.split('\\n') \n",
    "    for abilities in processed_abilities\n",
    "    ]\n",
    "\n",
    "# create an ability count feature\n",
    "cards['n_abilities'] = cards['abilities_list'].map(len)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test sets that are divided based on\n",
    "# the set that a card is a part of \n",
    "X = cards.drop(columns=['prices_normal','prices_foil'])\n",
    "y = cards['set']\n",
    "train, test, _, _ = \\\n",
    "    train_test_split(\n",
    "        X,y,stratify=y,\n",
    "        random_state=13,\n",
    "        test_size=0.2\n",
    "    )\n",
    "\n",
    "# we'll reset the indices of both sets to more easily\n",
    "# translate between polars and pandas in an upcoming\n",
    "# step\n",
    "\n",
    "train.reset_index(drop=True,inplace=True)\n",
    "test.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization Process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize Abilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our token pattern needs to be able to account for\n",
    "# several non-standard things for it to be effective\n",
    "# for our needs. As normal, it needs to be able to\n",
    "# match words that are contiguous letters of an\n",
    "# arbitrary length. However, we also need to be able to\n",
    "# account for numbers in a few formats. These can be\n",
    "# wrapped in curly brackets, e.g. {2}, representing 2\n",
    "# colorless mana.\n",
    "# It also needs to be able to account for letters\n",
    "# inside of curly brackets and return them as such.\n",
    "# It must also match something like +1/+1, -1/-1, or\n",
    "# several other variations thereof.\n",
    "# Lastly, it should also ignore any text that is inside\n",
    "# of parentheses, as this text is reminder text - which\n",
    "# is text that explains what an ability does but this\n",
    "# text doesn't actually contribute to explaining what\n",
    "# a card does in a meaningful way. \n",
    "\n",
    "token_pattern = \\\n",
    "    r\"([a-zA-Z]+(?:’[a-z]+)?|[+-]?\\d\\/[+-]?\\d|\\{\\d\\d?\\}|\\{.\\s?.?\\}|\\n)|\\(.+?\\)\"\n",
    "\n",
    "cvec = CountVectorizer(\n",
    "        token_pattern=token_pattern,\n",
    "        # min_df=0.0005, # <= this will mean that the\n",
    "                    # minimum number of cards that it\n",
    "                    # takes for an ability to show up\n",
    "                    # on this list will be 46 after the\n",
    "                    # explode is run, since it will be\n",
    "                    # 83,000 entries long. We'll just\n",
    "                    # limit our overall features since\n",
    "                    # this is such a small percentage. \n",
    "        max_df=0.4,\n",
    "        ngram_range=(1,5),\n",
    "        max_features=1500\n",
    "    )\n",
    "\n",
    "# Exploding abilities to create a vectorized set\n",
    "explode_train = train.explode('abilities_list')\n",
    "\n",
    "explode_vec = cvec.fit_transform(\n",
    "        explode_train['abilities_list']\n",
    "    )\n",
    "explode_vec = pd.DataFrame.sparse.from_spmatrix(\n",
    "    explode_vec\n",
    ")\n",
    "\n",
    "# we save the vocab here to export for later. This is\n",
    "# so we can bring in new data and make sure it's only\n",
    "# being segmented into vocab we can \"understand\"\n",
    "explode_vec.columns = sorted((vocab := cvec.vocabulary_))\n",
    "explode_vec['id'] = explode_train['id'].values\n",
    "explode_vec.head()\n",
    "\n",
    "# convert pandas vectorized dataframe to polars\n",
    "pl_vec = pl.from_pandas(explode_vec.astype(np.int32,errors='ignore'))\n",
    "# perform group by and sum aggregation and convert back\n",
    "# to pandas \n",
    "agged_vec = pl_vec.groupby('id').sum().to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_type_cvec = CountVectorizer()\n",
    "\n",
    "type_frame = pl.from_pandas(train['type_line']).apply(lambda x: x.split('—')[0])\n",
    "type_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "    card_type_cvec.fit_transform(type_frame)\n",
    ")\n",
    "# we save the type vocab for later to export\n",
    "type_df.columns = sorted((type_vocab:=card_type_cvec.vocabulary_))\n",
    "type_df['id'] = train['id']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing Color Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['str_color_identity'] = \\\n",
    "    train['color_identity'].map(\n",
    "        lambda x: ' '.join(x)\n",
    "        )\n",
    "\n",
    "color_match = CountVectorizer(\n",
    "    token_pattern=r\"[wubrg]\"\n",
    ")\n",
    "color_id_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "    color_match.fit_transform(train['str_color_identity'])\n",
    ")\n",
    "color_id_df.columns = sorted(\n",
    "        (color_vocab:=color_match.vocabulary_)\n",
    "    )\n",
    "color_id_df['c'] = color_id_df.T.apply(lambda x: 1 if sum(x)==0 else 0)\n",
    "color_id_df['id'] = train['id']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing Pseudo-Numbers et al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_vectorizer = CountVectorizer(\n",
    "    token_pattern = r\".*\",\n",
    "    stop_words=[''],\n",
    "    lowercase=False\n",
    "    )\n",
    "dummy_dict = {}\n",
    "dummy_vocab = {}\n",
    "dummy_columns = ['rarity','power','toughness','loyalty']\n",
    "for _col in dummy_columns:\n",
    "    dummy_column = train[_col].T.apply(\n",
    "        lambda x: '' if x == None else f'{_col}_{x}'\n",
    "        )\n",
    "    dummy_dict[_col] = pd.DataFrame.sparse.from_spmatrix(\n",
    "        dummy_vectorizer.fit_transform(dummy_column)\n",
    "    )\n",
    "    dummy_vocab[_col] = dummy_vectorizer.vocabulary_\n",
    "    dummy_dict[_col].columns = sorted(dummy_vocab[_col])\n",
    "    dummy_dict[_col]['id'] = train['id']\n",
    "\n",
    "dummies = train[['id']]\n",
    "for _col in dummy_columns:\n",
    "    dummies = dummies.merge(\n",
    "        dummy_dict[_col],\n",
    "        on='id'\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date-to-Age Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_now = pd.Timestamp.today().floor('D')\n",
    "train['card_age'] = train['released_at'].apply(lambda x: (_now - x).days)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# purpose for each column is explained in notebook 2\n",
    "# for simplicity, this won't be repeated here \n",
    "used_columns = [\n",
    "    'id','cmc','promo','reprint','full_art','textless',\n",
    "    'n_abilities','median_normal','median_foil','card_age'\n",
    "]\n",
    "train_reduced = train[used_columns].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Merging and Subsetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined = train_reduced.merge(\n",
    "    agged_vec,\n",
    "    on='id'\n",
    ").merge(\n",
    "    type_df,\n",
    "    on='id'\n",
    ").merge(\n",
    "    color_id_df,\n",
    "    on='id'\n",
    ").merge(\n",
    "    dummies,\n",
    "    on='id'\n",
    ")\n",
    "\n",
    "train_combined = train_combined[\n",
    "    (train_combined[ 'stickers' ] == 0) &\n",
    "    (train_combined['conspiracy'] == 0)\n",
    "].drop(columns=['stickers','conspiracy']).copy()\n",
    "\n",
    "# creating a normal and foil subset\n",
    "train_norm = train_combined.dropna(\n",
    "    subset=['median_normal']\n",
    "    ).drop(columns=['median_foil']\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "train_foil = train_combined.dropna(\n",
    "    subset=['median_foil']\n",
    "    ).drop(columns=['median_normal']\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "norm_prices = train_norm['median_normal']\n",
    "foil_prices = train_foil['median_foil']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling (Again)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Model\n",
    "\n",
    "We'll recreate the dummy model here to have a baseline\n",
    "for comparison later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyRegressor(strategy='median')\n",
    "norm_guess = dummy.fit(\n",
    "    train_norm,norm_prices\n",
    "    ).predict(train_norm)\n",
    "foil_guess = dummy.fit(\n",
    "    train_foil,foil_prices\n",
    "    ).predict(train_foil)\n",
    "\n",
    "norm_base_rmse = mean_squared_error(\n",
    "        norm_prices,norm_guess,\n",
    "        squared=False\n",
    "    )\n",
    "foil_base_rmse = mean_squared_error(\n",
    "        foil_prices,foil_guess,\n",
    "        squared=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spells",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
